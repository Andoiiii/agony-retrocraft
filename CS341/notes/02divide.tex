\chapter{Divide and Conquer}
In general, we want to:
\begin{itemize}[nosep]
  \item divide: split a problem into subproblems;
  \item conquer: solve the subproblems recursively; and
  \item combine: use subproblem results to derive problem result
\end{itemize}
This is possible when:
\begin{itemize}[nosep]
  \item the original problem is easily decomposable into subproblems;
  \item combining solutions is not costly; and
  \item subproblems are not overly unbalanced
\end{itemize}

\section{Examples}
\begin{problem}[counting inversions]
  Given an unsorted array $A[1..n]$, find the number of \term[inversion]{inversions} in it,
  i.e., pairs $(i,j)$ such that $A[i] > A[j]$.
\end{problem}
\begin{example}
  Given $A = [1,5,2,6,3,8,7,4]$,
  we get (2,3), (2,5), (2,8), (4,5), (4,8), (6,7), (6,8), and (7,8).
\end{example}
The naive algorithm checks all pairs and takes $\Theta(n^2)$ time.
We can do better.

Let $c_\ell$ be the number of inversions in $A[1..n/2]$,
$c_r$ be the number of inversions in $A[n/2+1..n]$,
and $c_t$ be the number of transverse inversions, i.e.,
inversions where $i$ is on the left and $j$ is on the right.

We can find $c_\ell$ and $c_r$ by recursion.

To find $c_t$, we must count the number of left indices
greater than each right index.
This can be done by sorting and then binary searching,
since the binary search result index gives exactly what we want.
The sort takes $O(n\log n)$ and each of the $n$ binary searches takes $O(\log n)$.

This gives us $T(n) \leq 2T(n/2) + O(n\log n) = O(n\log^2 n)$.

We can instead modify \Call{MergeSort}{} and find $c_t$ using a modified \Call{Merge}{}:
\begin{algorithm}[H]
  \caption{Modified \Call{Merge}{$A[1..n]$} (additions in \textcolor{Green}{green})}
  \begin{algorithmic}[1]
    \Require{both halves of $A$ are sorted}
    \State \textcolor{Green}{copy $A$ into a new array $S$; $c = 0$}
    \State $i \gets 1$; $j \gets n/2+1$
    \For{$k \gets 1,\dotsc,n$}
      \If{$i > n/2$} $A[k] \gets S[j\pp]$
      \ElsIf{$j > n$}
        \State $A[k] \gets S[i\pp]$
        \State \textcolor{Green}{$c \gets c + \frac{n}{2}$}
      \ElsIf{$S[i] < S[j]$}
        \State $A[k] \gets S[i\pp]$
        \State \textcolor{Green}{$c \gets c + j - (\frac{n}{2}+1)$}
      \Else{} $A[k] \gets S[j\pp]$
      \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}
Here, every time we merge in an element from the left,
we add to $c$ the number of elements on the right which are greater than it.
This will run in $\Theta(n\log n)$ time because the modified \Call{Merge}{} is still $\Theta(n)$.

\begin{problem}[polynomial multiplication]
  Given $F = f_0 + \dotsb + f_{n-1}x^{n-1}$
  and $G = g_0 + \dotsb + g_{n-1}x^{n-1}$,
  calculate $H = FG$.
\end{problem}

The naive algorithm takes $\Theta(n^2)$ time to expand.

Notice that we can split $F = F_0 + F_1x^{n/2}$ and $G = G_0 + G_1x^{n/2}$.
Then, we have $H = F_0G_0 + (F_0G_1+F_1G_0)x^{n/2} + F_1G_1x^n$.
If we divide and conquer, we make 4 recursive calls with size $n/2$
and $\Theta(n)$ extra work for the additions.

However, $T(n) = 4T(n/2)+\Theta(n) \in \Theta(n^2)$ which is not an improvement.

\begin{lemma}[Karatsuba's identity]
  $(x+y)(a+b) - xa - yb = xb + ya$
\end{lemma}

\lecture{(05/18)}
But if we already have $F_0G_0$ and $F_1G_1$,
we can use Karatsuba's identity to instead calculate
$(F_0 + F_1)(G_0 + G_1) - F_0G_0 - F_1G_1 = F_0G_1 + F_1G_0$.
That is, we will calculate:
\begin{align*}
  H & = (F_0 + F_1x^{n/2})(G_0 + G_1x^{n/2})                           \\
    & = F_0G_0 + ((F_0+F_1)(G_0+G_1)-F_0G_0-F_1G_1)x^{n/2} + F_1G_1x^n
\end{align*}
This means we only need to make 3 recursive calls instead of 4.

Then, $T(n) = 3T(n/2) + \Theta(n) \in \Theta(n^{\lg 3})$ which is an improvement.

Based on this observation, Toom--Cook created a family of algorithms
that for $k \geq 2$ make $2k-1$ recursive calls in size $n/k$,
i.e., $T(n) \in \Theta(n^{\log_k(2k-1)})$ which gets arbitrarily close to linear
(but with increasingly massive constants).

If $F,G \in \C[x]$, then we can use FFT to get $T(n) = 2T(n/2)+\Theta(n) \in \Theta(n\log n)$ time.

\begin{problem}[matrix multiplication]
  Given $A = [a_{i,j}] \in M_{n\times n}$ and $B = [b_{j,k}] \in M_{n \times n}$,
  calculate $C = AB$.
\end{problem}

The naive algorithm takes inputs of size $\Theta(n^2)$ in $\Theta(n^3)$ time.

Consider instead breaking into block matrices:
$A = \pmqty[c|c]{A_{1,1}&A_{2,2}\\\hline A_{2,1}&A_{2,2}}$ and
$B = \pmqty[c|c]{B_{1,1}&B_{2,2}\\\hline B_{2,1}&B_{2,2}}$.

Then, $C = \begin{pmatrix}[c|c]
    A_{1,1}B_{1,1} + A_{1,2}B_{2,1} & A_{1,1}B_{1,2} + A_{1,2}B_{2,2} \\ \hline
    A_{2,1}B_{1,1} + A_{2,2}B_{2,1} & A_{2,1}B_{1,2} + A_{2,2}B_{2,2}
  \end{pmatrix}$

This makes 8 recursive calls of size $n/2$ and $\Theta(n^2)$ additions,
which resolves to $\Theta(n^3)$ (no improvement).
However, due to Strassen, we can reduce this to 7, giving $\Theta(n^{\lg 7})$ time.

We can generalize to do $k$ multiplications of $\ell\times\ell$ matrices in $\Theta(n^{\log_\ell k})$ time
and $k$ multiplications of $\ell\times m$ by $m\times p$ in $\Theta(n^{3\log_{\ell m p}k})$ time.

\begin{problem}[closest pairs]
  Given $n$ distinct points $(x_i, y_i)$,
  find a pair $(i,j)$ that minimizes the distance
  \[ d_{i,j} = \sqrt{(x_i-x_j)^2+(y_i-y_j)^2} \]
  Equivalently, minimize $d^2_{i,j} = (x_i-x_j)^2+(y_i-y_j)^2$.
\end{problem}

Separate the space of points into $L$ and $R$ halfspaces based on the median $x$ value.
The closest pair is either entirely in $L$, entirely in $R$, or transverse.

We can recursively find minimum distances $\delta_L$ and $\delta_R$.
Then, if we let $\delta = \min\{\delta_L,\delta_R\}$,
any closer transverse points must be within $\delta$ units of the median $x$ value.

Now, if we start from the bottom point $P \in L$ by $y$-value in that band,
we only have to compare $P$ with points $Q \in R$ with $y_P \leq y_Q < y_P + \delta$.

We can only have a maximum of 8 points inside the $2\delta \times \delta$
rectangle of possible $Q$ points, because the points must be at least $\delta$ apart.

Therefore, we are doing $\Theta(1)$ work for each $P$, so we do $\Theta(n)$
work to find transverse pairs.

For this to work, we must first sort the points by $x$ and by $y$ (in $O(n\log n)$ time).
We can find the median in $O(1)$ time.
We split the sorted points in $O(n)$ time for the two recursive calls
and find the $\delta$ band in $O(n)$ time.
Again, it takes $O(n)$ time to find transverse pairs.
Therefore, $T(n) = 2T(n/2) + O(n) = O(n\log n)$.

\begin{problem}[selection]
  Given $A[0..n-1]$, find the entry that would be at index $k$ if $A$ were sorted.
\end{problem}

Recall from CS 240 that selection by sorting takes $O(n\log n)$ time
or $O(n)$ randomized expected time using \Call{QuickSelect}{$A$, $k$}:

\begin{algorithm}[H]
  \caption{\Call{QuickSelect}{$A$, $k$}}
  \begin{algorithmic}[1]
    \State $p \gets \Call{ChoosePivot}{A}$
    \State $i \gets \Call{Partition}{A, p}$ \Comment{$i$ is the correct index of $p$}
    \If{$i = k$} \Return $A[i]$
    \ElsIf{$i > k$} \Return \Call{QuickSelect}{$A[0..i-1]$, $k$}
    \Else{} \Return \Call{QuickSelect}{$A[i+1..n-1]$, $k-i-1$}
    \EndIf
  \end{algorithmic}
\end{algorithm}

Consider splitting $A$ into groups $G_i$ of size 5.
Then, find the medians $m_i$ of each group.
We can choose the pivot $p$ as the median of medians:
\begin{center}
  \begin{tikzpicture}
    \newcommand{\Geq}{\rotatebox{270}{$\geq$}}
    \matrix(M)[matrix of math nodes]{
           &      &      &      &        &      & n/2  &      &        &      &           &      & n       \\
      *    &      & *    &      & \dotsb &      & *    &      & \dotsb &      & *         &      & *       \\
      \Geq &      & \Geq &      & \Geq   &      & \Geq &      & \Geq   &      & \Geq      &      & \Geq    \\
      *    &      & *    &      & \dotsb &      & *    &      & \dotsb &      & *         &      & *       \\
      \Geq &      & \Geq &      & \Geq   &      & \Geq &      & \Geq   &      & \Geq      &      & \Geq    \\
      m_1  & \geq & m_2  & \geq & \dotsb & \geq & p    & \geq & \dotsb & \geq & m_{n/5-1} & \geq & m_{n/5} \\
      \Geq &      & \Geq &      & \Geq   &      & \Geq &      & \Geq   &      & \Geq      &      & \Geq    \\
      *    &      & *    &      & \dotsb &      & *    &      & \dotsb &      & *         &      & *       \\
      \Geq &      & \Geq &      & \Geq   &      & \Geq &      & \Geq   &      & \Geq      &      & \Geq    \\
      *    &      & *    &      & \dotsb &      & *    &      & \dotsb &      & *         &      & *       \\
    };
    \draw[red,ultra thick,rounded corners] (M-2-1.north west) -- (M-2-7.north east) -- (M-5-7.south east) -- (M-5-7.south west) -- (M-6-6.north east) -- (M-6-6.south east) -- (M-6-1.south west) -- cycle;
    \draw[Green,ultra thick,rounded corners] (M-10-13.south east) -- (M-10-7.south west) -- (M-7-7.north west) -- (M-7-7.north east) -- (M-6-8.south west) -- (M-6-8.north west) -- (M-6-13.north east) -- cycle;
  \end{tikzpicture}
\end{center}
Then, we are guaranteed to have $3n/10$ elements
\textcolor{Red}{above} and \textcolor{Green}{below} $p = A[i]$,
so the recursive calls to $A[0..i-1]$ and $A[i+1 .. n-1]$ have size at most $7n/10$
(with equality when $i$ is exactly $3n/10$ or $7n/10$).

Therefore, $T(n) \leq T(n/5) + T(7n/10) + O(n)$.

\lecture{(05/25)}

\begin{claim}
  $T(n/5) + T(7n/10) + O(n) \in O(n)$
\end{claim}
\begin{prf}
  Proceed by induction. Note that $T(n) \leq \begin{cases}
      O(1)                                       & n < 120    \\
      T(\frac{n}{5}) + T(\frac{7}{10}n+6) + O(n) & n \geq 120
    \end{cases}$

  We will show that $T(n) \leq cn$ for large enough $c$ and all $n > 0$.
  We know that there exists a sufficiently large $c$ such that
  $T(n) \leq cn$ for $n < 120$ because $T(n)$ is just $O(1) \subsetneq O(n)$.

  Choose a constant $a$ to write $O(n)$ as $an$.

  Suppose $T(m) \in O(m)$ for all $0 < m < n$. Then,
  \begin{align*}
    T(n) & \leq \frac{cn}{5} + c\qty(\frac{7n}{10} + 6) + an \\
         & \leq c\frac{n}{5} + c\frac{7n}{10} + 6c + an      \\
         & = 9c\frac{n}{10} + 6c + an                        \\
         & = cn + \qty(-c\frac{n}{10} + 6c + an)
  \end{align*}
  We can show that the latter term is non-positive:
  \begin{align*}
    -c\frac{n}{10} + 6c + an \leq 0
     & \iff c\qty(6-\frac{n}{10}) + an \leq 0 \\
     & \iff c\qty(6-\frac{n}{10}) \leq -an    \\
     & \iff c\qty(\frac{n}{10}-6) \geq an     \\
     & \iff c \geq 10a\frac{n}{n-60}
  \end{align*}
  Now, if $\frac{n}{n-60} \leq 2$, i.e., $n \geq 120$,
  then we can say that $c \geq 20a$.

  Therefore, we can say that $T(n) \leq cn$, i.e., $T(n) \in O(n)$.
\end{prf}

\begin{example}
  What does $T(n) = T(\frac23n) + T(\frac{n}{3}) + n$ resolve to?
\end{example}
\begin{sol}
  Notice that if we draw a tree, each layer sums to $n$
  (this makes sense inductively since we pass $\frac23$ of $n$ to the left and $\frac13$ of $n$ to the right).
  There will be $O(\log_{3/2}n)$ layers in the tree, so it should resolve to $O(n\log n)$.
\end{sol}
