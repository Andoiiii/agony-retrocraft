\chapter{Greedy Algorithms}

\section{Introduction}
\lecture{(06/15)}

Suppose we are solving a \term{combinatorial optimization} problem,
i.e., a problem with a large (but finite) domain $\mathcal D$
such that we are trying to find an optimal solution $E \in \mathcal D$
that maximizes/minimizes some sort of cost function.

We will build $E$ step-by-step by taking the locally best solution.
Usually, it is very hard to prove correctness/optimality
but easy to find a counterexample.

For example, recall the Huffman encoding from CS 240.
We build the binary code tree by joining trees with the least frequencies.
This actually minimizes the length of the encoding.

\section{Basic Greedy Examples}

\begin{problem}[interval scheduling]\label{prob:g:intsched}
  Suppose we have $n$ intervals $[s_i, f_i]$.
  What is the subset of disjoint intervals with maximum length?
\end{problem}

We can show that a few naive greedy algorithms are wrong by drawing counterexamples:
\begin{itemize}
  \item Choose $\min_i s_i$: \tikz[yscale=0.25]{\draw (1,1) -- (10,1); \draw (2,2) -- (3,2); \draw (4,2) -- (5,2); \draw (6,2) -- (7,2); \draw (8,2)--(9,2);}
  \item Choose $\min_i\{f_i - s_i\}$: \tikz[yscale=0.25]{\draw (1,1) -- (4,1); \draw (3.5,2) -- (5.5,2); \draw (5,1) -- (8,1);}
  \item Choose minimum conflicts:
        \begin{tikzpicture}[yscale=-0.25]
          \draw (2.75,1) -- (4.25,1); \draw (2.75,0) -- (4.25,0); \draw (2.75,-1) -- (4.25,-1);
          \draw (6.75,1) -- (8.25,1); \draw (6.75,0) -- (8.25,0); \draw (6.75,-1) -- (8.25,-1);
          \draw (4.75,1) -- (6.25,1);
          \draw (2,2) -- (3,2); \draw (4,2) -- (5,2); \draw (6,2) -- (7,2); \draw (8,2)--(9,2);
        \end{tikzpicture}
\end{itemize}

However, we can prove that the greedy algorithm taking the earliest finish time is optimal.

\begin{algorithm}[H]
  \caption{\Call{IntervalScheduling}{$I = [[s_1,f_1],\dotsc,[s_n,f_n]]$}\label{alg:g:is}}
  \begin{algorithmic}[1]
    \State $S \gets \varnothing$
    \State $I \gets$ sort $I$ by finish time
    \For{$[s_i,f_i] \in I$}
      \If{$[s_i,f_i]$ has no conflicts in $S$}
        \State $S \gets S \cup \{[s_i,f_i]\}$
      \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{prop}
  Suppose $O$ is optimal. Then, $\abs{S} = \abs{O}$ where $S$ is generated by \Cref{alg:g:is}.
\end{prop}
\begin{prf}
  Let $i_1,\dotsc,i_k$ be the intervals in $S$ ordered by their addition
  and likewise $j_1,\dotsc,j_m$ be the intervals in $O$ ordered by increasing finish time.

  We prove the claim that for all $r \leq k$, $f_{i_r} \leq f_{j_r}$.
  Proceed by induction on $r$.

  For $r=1$ this is true since $i_1$ is the interval with the earliest finish time.

  Suppose $r > 1$ and it is true for $r-1$.
  Then, $f_{i_{r-1}} \leq f_{j_{r-1}}$ by assumption and $f_{j_{r-1}} < s_{j_r}$
  by the order we set on $O$.
  Therefore, $f_{i_{r-1}} < s_{j_r}$.

  That is, at the time the greedy algorithm chose $i_{r-1}$, $j_r$ was an option.
  Since the greedy algorithm picks the earliest finish time, $f_{i_r} \leq f_{j_r}$.

  Now, suppose for a contradiction that $S$ is not optimal, i.e., $\abs{S} < \abs{O}$.
  Then, there must be a $j_{k+1}$.
  But by the above claim, $f_{i_k} \leq f_{j_k} < s_{j_{k+1}}$.
  This means $j_{k+1}$ was an option for the greedy algorithm,
  so it would not have stopped at $i_k$ and instead added $j_{k+1}$.

  Therefore, $S$ must be optimal.
\end{prf}

We call proofs of this kind, i.e., contradicting that greedy could not have
chosen an optimal solution, \term{greedy stays ahead}.

\lecture{(06/20)}
We can also greedily solve a similar problem:

\begin{problem}[interval colouring]
  Suppose we have $n$ intervals $[s_i, f_i]$.
  Use the minimum number of colours to colour the intervals,
  so that each interval gets one colour
  and any overlapping intervals get different colours.
\end{problem}

Consider the algorithm:

\begin{algorithm}[H]
  \caption{\Call{IntervalColouring}{$I = [[s_1,f_1],\dotsc,[s_n,f_n]]$}\label{alg:g:ic}}
  \begin{algorithmic}[1]
    \State $c \gets$ empty colouring
    \State $I \gets$ sort $I$ by start time
    \For{$[s_i,f_i] \in I$}
      \State $c(i) \gets$ minimum $c$ such that there are no conflicts
    \EndFor
  \end{algorithmic}
\end{algorithm}

which we do not bother analyzing the time complexity of.
We show correctness:

\begin{prop}
  Suppose that \Cref{alg:g:ic} uses $k$ colours.
  There is no way to colour $I$ with $k-1$ colours.
\end{prop}
\begin{prf}
  Suppose interval $\ell$ is the first to use $k$.
  Then, the algorithm must have found $k-1$ overlapping intervals with colours $1,\dotsc,k-1$.
  Let these be intervals $i_1,\dotsc,i_{k-1}$.
  By the initial sorting, we have $s_{i_j} < s_\ell$ for $j = 1,\dotsc,k-1$.
  Also, since they overlap, we have $f_{i_j} > f_\ell$.
  Therefore, $s_\ell$ is a point with $k$ intervals,
  meaning that it is impossible to colour with $k-1$ colours.
\end{prf}

\begin{problem}[minimize total completion time]
  Suppose we have $n$ jobs each requiring processing time $p_i$,
  and we are adding one job each step (e.g., the first step runs just one job,
  the fifth step runs five jobs, the last step runs all jobs).
  Order the jobs such that the total processing time is minimized.
\end{problem}

The setup is a bit weird, so we can construct an example.

\begin{example}
  For $n=5$ and $\vb p = [2,8,1,10,5]$,
  we can construct tables to find the total processing time
  if we do not order the jobs and if we order the jobs by increasing processing time:
  \begin{center}
    \begin{tabular}{CCCCC|C}
      2 & 8  & 1  & 10 & 5  & \Sigma \\ \hline
      2 & 2  & 2  & 2  & 2  & 10     \\
        & 8  & 8  & 8  & 8  & 24     \\
        &    & 1  & 1  & 1  & 3      \\
        &    &    & 10 & 10 & 20     \\
        &    &    &    & 5  & 5      \\ \hline
      2 & 10 & 11 & 21 & 26 & 70
    \end{tabular}
    \quad
    \begin{tabular}{CCCCC|C}
      1 & 2 & 5 & 8  & 10 & \Sigma \\ \hline
      1 & 1 & 1 & 1  & 1  & 5      \\
        & 2 & 2 & 2  & 2  & 8      \\
        &   & 5 & 5  & 5  & 15     \\
        &   &   & 8  & 8  & 16     \\
        &   &   &    & 10 & 10     \\ \hline
      1 & 3 & 8 & 16 & 26 & 54
    \end{tabular}
  \end{center}
  to find that the total processing times are 70 and 54, respectively.
\end{example}

\begin{prop}
  The total processing time is minimized when
  $e(i)$ is a permutation of $[n]$ such that $(p_{e(i)})$ is non-decreasing.
\end{prop}
\begin{prf}
  Suppose there is an optimal permutation $e$ that is not non-decreasing.
  That is, there exists an $i$ such that $p_{e(i)} > p_{e(i+1)}$.

  The total processing time is:
  \begin{center}
    \begin{tabular}{CCCCCCC|C}
      e(1)     & e(2)     & \dotsc & e(i)     & e(i+1)     & \dotsc & e(n)       & \Sigma           \\ \hline
      p_{e(1)} & p_{e(1)} &        & p_{e(1)} & p_{e(1)}   &        & p_{e(1)}   & n p_{e(1)}       \\
               & p_{e(2)} &        & p_{e(2)} & p_{e(2)}   &        & p_{e(2)}   & (n-1) p_{e(2)}   \\
               &          &        & \vdots   & \vdots     &        & \vdots     & \vdots           \\
               &          &        & p_{e(i)} & p_{e(i)}   &        & p_{e(i)}   & (n-i+1) p_{e(i)} \\
               &          &        &          & p_{e(i+1)} &        & p_{e(i+1)} & (n-i) p_{e(i+1)} \\
               &          &        &          &            &        & \vdots     & \vdots           \\
               &          &        &          &            &        & p_{e(n)}   & p_{e(n)}         \\
    \end{tabular}
  \end{center}
  Suppose we swap $e(i)$ and $e(i+1)$.
  Then, we have removed one copy of $p_{e(i)}$ and added one copy of $p_{e(i+1)}$.
  But by assumption, $p_{e(i+1)} - p_{e(i)} < 0$, so this swap decreases the total processing time,
  and the solution was not optimal.
\end{prf}

\section{Shortest Paths: Dijkstra's Algorithm}

Recall that we can define a weight function on a graph.
\lecture{(06/22)}

\begin{defn}[weight function]
  Given a graph $G = (V, E)$, a function $w : E \to \R$.
  We call $(G, w)$ a \term{weighted graph}.

  Then, we define the weight for a path $P = v_0\cdots v_k$
  by $w(P) = \sum_{i=1}^k w(v_{i-1}v_i)$.
\end{defn}

\begin{remark}
  A shortest path exists in any directed weighted graph
  with no negative-weight cycles.
\end{remark}

\begin{problem}[single-source shortest path]\label{prob:g:sp}
  Given $G = (V, E)$ with weight $w : E \to \R_{\geq 0}$ and a source $s \in V$,
  find a shortest path from $s$ to each $v \in V$.
\end{problem}

We denote the length of the shortest path $s \leadsto v$ by $\delta(s,v)$.

\begin{remark}\label{rem:greedy:path}
  If $v_0\cdots v_k$ is a shortest path from $v_0$ to $v_k$,
  then $v_0\cdots v_i$ is a shortest path from $v_0$ to $v_i$
  for all $0 \leq i \leq k$.
\end{remark}
\begin{prf}
  Suppose not.
  Then, use the shorter path to get to $v_i$ and continue onward to $v_k$
  to get a shorter path to $v_k$.
\end{prf}

Dijkstra's algorithm is built on this observation.

For each vertex, we maintain an estimate of the distance $d[v]$
and a predecessor in that path estimate $\pi[v]$.
We start with all vertices in a set $Q$
and pop vertices one at a time in order of $d$,
adding them to a set $C$.
When a vertex moves from $Q$ to $C$,
update the distance estimates and predecessors of its neighbours.

\begin{algorithm}[H]
  \caption{\Call{Dijkstra}{$G,w,s$}}
  \begin{algorithmic}[1]
    \For{each vertex $v \in V$}
      \State $d[v] \gets \infty$
      \State $\pi[v] \gets \bot$
    \EndFor
    \State $d[s] \gets 0$
    \State $C \gets \varnothing$
    \State $Q \gets V$
    \While{$Q \neq \varnothing$}
      \State $u \gets \Call{ExtractMin}{Q}$
      \State $C \gets C \cup \{u\}$
      \For{neighbours $v \in \vv{Adj}[u]$}
        \If{$d[v] > d[u] + w(uv)$}
          \State $d[v] = d[u] + w(uv)$
          \State $\pi[v] = u$
        \EndIf
      \EndFor
    \EndWhile
  \end{algorithmic}
\end{algorithm}

Notice that we implement $Q$ and $C$ as heaps since then
we get time complexity $O(\abs{V}\log\abs{V} + \abs{E}\log\abs{V})$
(for the extractions and the updates, respectively)
instead of $O(\abs{V}^2+\abs{E})$ with arrays.

\lecture{(06/27)}
Claim now that Dijkstra's is correct.
Proceed by a greedy stays ahead proof.

\begin{prop}
  For each vertex $v \in V$, $d[v] = \delta(s,v)$ at the time $v$ is added to $C$.
\end{prop}
\begin{prf}
  Suppose for a contradiction that $u \in V$
  is the first vertex for which $d[u] \neq \delta(s,u)$ when it is added to $C$.
  Denote the iteration when $u$ is added to $C$ as time $t$.

  Let $P$ be a shortest path $s \leadsto u$.
  Since $s \in C$ and $P \in V-C$,
  there exists a pair of vertices $x \in C$ and $y \in V-C$ with $xy \in P$.
  We claim that at time $t$, $d[y] = \delta(s,y)$.

  Since $u$ is the first vertex with $d[u] \neq \delta(s,u)$,
  for all vertices $w$ in $C$ at time $t$, $d[w] = \delta(s,w)$.
  In particular, $d[x] = \delta(s,x)$.
  When $x$ was added to $C$, the edge $xy$ was considered.
  By \Cref{rem:greedy:path}, since $P$ is a shortest path,
  the parts from $s$ to $x$ and $y$ are also shortest paths.
  Therefore, $\delta(s,y) = d[x] + w(x,y)$ which is exactly what
  $d[y]$ is set to during that iteration.

  However, we can also say that $d[y] = \delta(s,y) \leq \delta(s,u) \leq d[u]$.
  But at time $t$, Dijkstra's chose $u$ from the min-heap,
  so we must have $d[u] \leq d[y]$.

  Therefore, $d[y] = d[u]$ so we must have tight $d[y] = \delta(s,y) = \delta(s,u) = d[u]$,
  which is our contradiction.
\end{prf}

\section{Minimum Spanning Trees: Kruskal's Algorithm}

Recall again some MATH 239 content:
\lecture{(06/29)}

\begin{defn}[spanning tree in $G$]
  Given a connected graph $G = (V,E)$,
  a tree $T = (V,A)$ where $A \subseteq E$.
\end{defn}

Suppose we have a weighted graph. Then,

\begin{problem}[minimal spanning tree]
  Given a connected graph $G$ with edge weights $w$,
  what is the spanning tree with minimal weight $w(T) = \sum_{e \in A}w(e)$?
\end{problem}

Recall that a tree is a graph with no cycles.
Kruskal's algorithm greedily selects the lowest-weight edge available
that does not create a cycle:

\begin{algorithm}[H]
  \caption{\Call{GreedyMST}{$G$}\label{alg:g:mst1}}
  \begin{algorithmic}[1]
    \State $A \gets $ empty graph
    \State sort edges by increasing weight
    \For{$e \in E$}
      \If{$e$ does not create a cycle in $A$}
        \State append $e$ to $A$
      \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}

Now, we prove that this indeed works.

\begin{claim}
  Let $G = (V,E)$ be a connected graph and $A \subseteq E$.
  If $(V,A)$ has no cycles and $\abs{A} < n-1$,
  then there is always an edge $e \in E-A$ such that $(V,A \cup \{e\})$
  still has no cycles.
\end{claim}
\begin{prf}
  In any graph, $\abs{V} - c \leq \abs{E}$
  where $c$ is the number of components $C_i = (V_i,E_i)$ in $(V,E)$.
  Since each component is connected, $\abs{E_i} \geq \abs{V_i} - 1$. Then,
  \[
    \abs{V} - c
    = \sum_{i=1}^c \abs{V_i} - \sum_{i=1}^c 1
    = \sum_{i=1}^c (\abs{V_i} - 1)
    \leq \sum_{i=1}^c \abs{E_i} = \abs{E}
  \]
  In particular, for $(V,A)$, we have $n - c < n-1 \implies c > 1$.
  This means there exists an edge $e$ that connects two components of $(V,A)$.

  Since $e$ is a bridge of $(V, A\cup \{e\})$, it does not add a cycle.
\end{prf}

\begin{claim}\label{claim:g:2}
  If the output of \Cref{alg:g:mst1} is $A = [e_1,\dotsc,e_r]$,
  then $(V,A)$ is a spanning tree and $r = n-1$.
\end{claim}
\begin{prf}
  By construction, $(V,A)$ has no cycles (i.e., it is a tree).
  Suppose $(V,A)$ is not spanning.
  Then, it has at least two components and there exists an edge $e \not\in A$
  such that $(V,A\cup\{e\})$ has no cycles because $G$ is connected.
  Now, either:
  \begin{itemize}
    \item $w(e) < w(e_1)$. This cannot happen because $e_1$ is the edge with the smallest weight.
    \item $w(e_i) < w(e) < w(e_{i+1})$ for some $i$.
          This also does not work because if $e$ was considered after inserting $e_i$
          and rejected, then that means that $e$ creates a cycle in $(V,\{e_1,\dotsc,e_i\})$.
          But by construction, $e$ does not create a cycle in $V(,A)$.
    \item $w(e) > w(e_r)$. This also does not make sense, because the algorithm
          would have included it in $A$ after selecting $e_r$.
  \end{itemize}
  Therefore, no such $e$ exists, which means that $(V,A)$ is spanning.
  Since $(V,A)$ is a spanning tree, we also get that $r = n-1$.
\end{prf}

\begin{claim}\label{claim:g:3}
  Let $(V, A)$ and $(V, T)$ be two spanning trees,
  and let $e$ be an edge in $T$ but not in $A$.

  Then, there exists an edge $e'$ in $A - T$ such that
  $(V, T + e' - e)$ is still a spanning tree.
  Also, $e'$ is on the cycle that $e$ creates in $A$.

  (This is Theorem 5.2.4 from MATH 239)
\end{claim}
\begin{prf}
  Let $e = uv$. Then, $(V,A+e)$ contains a cycle $C = vw\cdots v$.

  Since $T$ is a tree, $(V,T-e)$ has two connected components $T_1$ and $T_2$.
  \WLOG, suppose $v \in T_1$.
  Then, $C$ starts in $V(T_1)$, immediately crosses to $V(T_2)$,
  and at some point later crosses back into $V(T_1)$.
  Therefore, there exists an edge $e' \in E(C) \subseteq A+e$ connecting $T_1$ and $T_2$
  that is in $A$ but not in $T$.

  It follows that $(V,T+e'-e)$ is a spanning tree by reconnecting those components.
\end{prf}

Now, proceed by an exchange argument.

\begin{prop}
  Let $A$ be the output of \Cref{alg:g:mst1} and $(V,T)$ be any spanning tree.
  Then, $(V,A)$ is a spanning tree with $w(A) \leq w(T)$.
\end{prop}
\begin{prf}
  We know that $(V,A)$ is a spanning tree from \Cref{claim:g:2}.

  Claim that $w(A) \leq w(T)$. We induct on the size of $T - A$.
  If $T - A = \varnothing$, then $A = T$ and naturally $w(A) = w(T)$.

  Suppose $A \neq T$. Then, let $e \in T - A$.
  By \Cref{claim:g:3}, there exists an edge $e' \in A - T$
  such that $(V, T+e'-e)$ is a spanning tree and $e'$ is on the cycle that $e$ creates in $A$.

  Since we rejected $e$, it must have created a cycle in $A$.
  Therefore, it appeared in the sorted list after all elements in its induced cycle,
  in particular, $w(e') \leq w(e)$.

  Then, if we let $T' = T + e' - e$,
  we have $w(T') = w(T) + w(e) - w(e') \leq w(T)$.

  Also, since $T'$ has one more edge in common with $A$,
  $\abs{T' - A} < \abs{T - A}$.
  By the inductive hypothesis, $w(A) \leq w(T') \leq w(T)$.

  Therefore, $A$ is optimal.
\end{prf}

Now, consider how we can implement this optimally.
Every vertex is initially in a component of just itself.
Then, as edges are added, the components grow.
To check if a cycle is formed, check if both ends of an edge are in the same component.

We need a data structure that allows us to quickly identify
a set given an element belongs to (\Call{Find}{$e$}) and also join two sets (\Call{Union}{$S_1,S_2$}).
With these operations, we can rewrite Kruskal's as:

\begin{algorithm}[H]
  \caption{\Call{GreedyMSTUnionFind}{$G$}}
  \begin{algorithmic}[1]
    \State $T \gets [\ ]$
    \State $U \gets \{\{v_1\},\dotsc,\{v_n\}\}$
    \State sort edges by increasing weight
    \For{$e \in E(G)$ sorted}
      \If{$U.\Call{Find}{e_k.1} \neq U.\Call{Find}{e_k.2}$}
        \State $U.\Call{Union}{U.\Call{Find}{e_k.1}, U.\Call{Find}{e_k.2}}$
        \State $T.\Call{Append}{e_k}$
      \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}

We can implement this with $U$ as an array of linked lists
paired with an array of indices $X$ such that $e \in U[X[e]]$.
Then, we can run \Call{Find}{} in $O(1)$ time and \Call{Union}{} in $O(n)$ time.

There will be $O(m)$ \Call{Find}{}'s and $O(n)$ \Call{Union}{}'s.
The total time complexity is then $O(m \log m + n^2)$.

We can optimize this slightly.
First, only traverse the smaller list when unioning.
Also, have each linked list in $U$ keep track of its size
and have an optional pointer to a continuation.
Then, by setting the pointer, we can get concatenation in $O(1)$
even though the overall union is still $O(n)$ (because of the updates to $X$).

However, for any vertex $v$, the size of the list containing $v$
at least doubles every time we update $X[v]$.
That means that $X[v]$ is updated $O(\log n)$ times,
meaning the overall cost per union per vertex is $O(\log n)$ for $O(n \log n)$ total.

Therefore, we can do Kruskal's in $O(m\log m)$.
