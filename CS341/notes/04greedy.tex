\chapter{Greedy Algorithms}

\section{Introduction}
\lecture{(06/15)}

Suppose we are solving a \term{combinatorial optimization} problem,
i.e., a problem with a large (but finite) domain $\mathcal D$
such that we are trying to find an optimal solution $E \in \mathcal D$
that maximizes/minimizes some sort of cost function.

We will build $E$ step-by-step by taking the locally best solution.
Usually, it is very hard to prove correctness/optimality
but easy to find a counterexample.

For example, recall the Huffman encoding from CS 240.
We build the binary code tree by joining trees with the least frequencies.
This actually minimizes the length of the encoding.

\section{Basic Greedy Examples}

\begin{problem}[interval scheduling]\label{prob:g:intsched}
  Suppose we have $n$ intervals $[s_i, f_i]$.
  What is the subset of disjoint intervals with maximum length?
\end{problem}

We can show that a few naive greedy algorithms are wrong by drawing counterexamples:
\begin{itemize}
  \item Choose $\min_i s_i$: \tikz[yscale=0.25]{\draw (1,1) -- (10,1); \draw (2,2) -- (3,2); \draw (4,2) -- (5,2); \draw (6,2) -- (7,2); \draw (8,2)--(9,2);}
  \item Choose $\min_i\{f_i - s_i\}$: \tikz[yscale=0.25]{\draw (1,1) -- (4,1); \draw (3.5,2) -- (5.5,2); \draw (5,1) -- (8,1);}
  \item Choose minimum conflicts:
        \begin{tikzpicture}[yscale=-0.25]
          \draw (2.75,1) -- (4.25,1); \draw (2.75,0) -- (4.25,0); \draw (2.75,-1) -- (4.25,-1);
          \draw (6.75,1) -- (8.25,1); \draw (6.75,0) -- (8.25,0); \draw (6.75,-1) -- (8.25,-1);
          \draw (4.75,1) -- (6.25,1);
          \draw (2,2) -- (3,2); \draw (4,2) -- (5,2); \draw (6,2) -- (7,2); \draw (8,2)--(9,2);
        \end{tikzpicture}
\end{itemize}

However, we can prove that the greedy algorithm taking the earliest finish time is optimal.

\begin{algorithm}[H]
  \caption{\Call{IntervalScheduling}{$I = [[s_1,f_1],\dotsc,[s_n,f_n]]$}\label{alg:g:is}}
  \begin{algorithmic}[1]
    \State $S \gets \varnothing$
    \State $I \gets$ sort $I$ by finish time
    \For{$[s_i,f_i] \in I$}
    \If{$[s_i,f_i]$ has no conflicts in $S$}
    \State $S \gets S \cup \{[s_i,f_i]\}$
    \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}

\begin{prop}
  Suppose $O$ is optimal. Then, $\abs{S} = \abs{O}$ where $S$ is generated by \Cref{alg:g:is}.
\end{prop}
\begin{prf}
  Let $i_1,\dotsc,i_k$ be the intervals in $S$ ordered by their addition
  and likewise $j_1,\dotsc,j_m$ be the intervals in $O$ ordered by increasing finish time.

  We prove the claim that for all $r \leq k$, $f_{i_r} \leq f_{j_r}$.
  Proceed by induction on $r$.

  For $r=1$ this is true since $i_1$ is the interval with the earliest finish time.

  Suppose $r > 1$ and it is true for $r-1$.
  Then, $f_{i_{r-1}} \leq f_{j_{r-1}}$ by assumption and $f_{j_{r-1}} < s_{j_r}$
  by the order we set on $O$.
  Therefore, $f_{i_{r-1}} < s_{j_r}$.

  That is, at the time the greedy algorithm chose $i_{r-1}$, $j_r$ was an option.
  Since the greedy algorithm picks the earliest finish time, $f_{i_r} \leq f_{j_r}$.

  Now, suppose for a contradiction that $S$ is not optimal, i.e., $\abs{S} < \abs{O}$.
  Then, there must be a $j_{k+1}$.
  But by the above claim, $f_{i_k} \leq f_{j_k} < s_{j_{k+1}}$.
  This means $j_{k+1}$ was an option for the greedy algorithm,
  so it would not have stopped at $i_k$ and instead added $j_{k+1}$.

  Therefore, $S$ must be optimal.
\end{prf}

We call proofs of this kind, i.e., contradicting that greedy could not have
chosen an optimal solution, \term{greedy stays ahead}.

\lecture{(06/20)}
We can also greedily solve a similar problem:

\begin{problem}[interval colouring]
  Suppose we have $n$ intervals $[s_i, f_i]$.
  Use the minimum number of colours to colour the intervals,
  so that each interval gets one colour
  and any overlapping intervals get different colours.
\end{problem}

Consider the algorithm:

\begin{algorithm}[H]
  \caption{\Call{IntervalColouring}{$I = [[s_1,f_1],\dotsc,[s_n,f_n]]$}\label{alg:g:ic}}
  \begin{algorithmic}[1]
    \State $c \gets$ empty colouring
    \State $I \gets$ sort $I$ by start time
    \For{$[s_i,f_i] \in I$}
    \State $c(i) \gets$ minimum $c$ such that there are no conflicts
    \EndFor
  \end{algorithmic}
\end{algorithm}

which we do not bother analyzing the time complexity of.
We show correctness:

\begin{prop}
  Suppose that \Cref{alg:g:ic} uses $k$ colours.
  There is no way to colour $I$ with $k-1$ colours.
\end{prop}
\begin{prf}
  Suppose interval $\ell$ is the first to use $k$.
  Then, the algorithm must have found $k-1$ overlapping intervals with colours $1,\dotsc,k-1$.
  Let these be intervals $i_1,\dotsc,i_{k-1}$.
  By the initial sorting, we have $s_{i_j} < s_\ell$ for $j = 1,\dotsc,k-1$.
  Also, since they overlap, we have $f_{i_j} > f_\ell$.
  Therefore, $s_\ell$ is a point with $k$ intervals,
  meaning that it is impossible to colour with $k-1$ colours.
\end{prf}

\begin{problem}[minimize total completion time]
  Suppose we have $n$ jobs each requiring processing time $p_i$,
  and we are adding one job each step (e.g., the first step runs just one job,
  the fifth step runs five jobs, the last step runs all jobs).
  Order the jobs such that the total processing time is minimized.
\end{problem}

The setup is a bit weird, so we can construct an example.

\begin{example}
  For $n=5$ and $\vb p = [2,8,1,10,5]$,
  what is the total processing time if we do not order the jobs
  and if we order the jobs by increasing processing time?
\end{example}
\begin{sol}
  Construct tables:
  \begin{center}
    \begin{tabular}{CCCCC|C}
      2 & 8  & 1  & 10 & 5  & \Sigma \\ \hline
      2 & 2  & 2  & 2  & 2  & 10     \\
        & 8  & 8  & 8  & 8  & 24     \\
        &    & 1  & 1  & 1  & 3      \\
        &    &    & 10 & 10 & 20     \\
        &    &    &    & 5  & 5      \\ \hline
      2 & 10 & 11 & 21 & 26 & 70
    \end{tabular}
    \quad
    \begin{tabular}{CCCCC|C}
      1 & 2 & 5 & 8  & 10 & \Sigma \\ \hline
      1 & 1 & 1 & 1  & 1  & 5      \\
        & 2 & 2 & 2  & 2  & 8      \\
        &   & 5 & 5  & 5  & 15     \\
        &   &   & 8  & 8  & 16     \\
        &   &   &    & 10 & 10     \\ \hline
      1 & 3 & 8 & 16 & 26 & 54
    \end{tabular}
  \end{center}
  to find that the total processing times are 70 and 54, respectively.
\end{sol}

\begin{prop}
  The total processing time is minimized when
  $e(i)$ is a permutation of $[n]$ such that $(p_{e(i)})$ is non-decreasing.
\end{prop}
\begin{prf}
  Suppose there is an optimal permutation $e$ that is not non-decreasing.
  That is, there exists an $i$ such that $p_{e(i)} > p_{e(i+1)}$.

  The total processing time is:
  \begin{center}
    \begin{tabular}{CCCCCCC|C}
      e(1)     & e(2)     & \dotsc & e(i)     & e(i+1)     & \dotsc & e(n)       & \Sigma           \\ \hline
      p_{e(1)} & p_{e(1)} &        & p_{e(1)} & p_{e(1)}   &        & p_{e(1)}   & n p_{e(1)}       \\
               & p_{e(2)} &        & p_{e(2)} & p_{e(2)}   &        & p_{e(2)}   & (n-1) p_{e(2)}   \\
               &          &        & \vdots   & \vdots     &        & \vdots     & \vdots           \\
               &          &        & p_{e(i)} & p_{e(i)}   &        & p_{e(i)}   & (n-i+1) p_{e(i)} \\
               &          &        &          & p_{e(i+1)} &        & p_{e(i+1)} & (n-i) p_{e(i+1)} \\
               &          &        &          &            &        & \vdots     & \vdots           \\
               &          &        &          &            &        & p_{e(n)}   & p_{e(n)}         \\
    \end{tabular}
  \end{center}
  Suppose we swap $e(i)$ and $e(i+1)$.
  Then, we have removed one copy of $p_{e(i)}$ and added one copy of $p_{e(i+1)}$.
  But by assumption, $p_{e(i+1)} - p_{e(i)} < 0$, so this swap decreases the total processing time,
  and the solution was not optimal.
\end{prf}

\section{Shortest Paths: Dijkstra's Algorithm}
\lecture{(06/22)}

\lecture{(06/27)}

\section{Minimum Spanning Trees: Kruskal's Algorithm}
\lecture{(06/29)}
