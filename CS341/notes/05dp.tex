\chapter{Dynamic Programming}
\section{Introduction}
\lecture{(07/04)}

Recall the \term{Fibonacci numbers} $F_n$ defined by
$F_0 = 0$, $F_1 = 1$, $F_n = F_{n-1} + F_{n-2}$ with the naive algorithm
\begin{algorithm}[H]
  \caption{\Call{Fib}{$n$}}
  \begin{algorithmic}[1]
    \If{$n=0$}
      \Return{0}
    \ElsIf{$n=1$}
      \Return{1}
    \Else
      \Return{$\Call{Fib}{n-1} + \Call{Fib}{n-2}$}
    \EndIf
  \end{algorithmic}
\end{algorithm}
Assuming we count additions as unit cost,
the runtime is $T(n) = F_{n+1} + 1 \in \Theta(\varphi^n)$ which is bad.

Notice that we are recomputing small $F_n$ a bunch of times,
but we actually only need each one once.
We can instead cache:
\begin{algorithm}[H]
  \caption{\Call{FibCached}{$n$}}
  \begin{algorithmic}[1]
    \Require $T \gets [0,1,\bot,\dotsc,\bot]$ global array of size $n$
    \If{$T[n] = \bot$} \State $T[n] \gets \Call{FibCached}{n-1} + \Call{FibCached}{n-2}$ \EndIf
    \State \Return{$T[n]$}
  \end{algorithmic}
\end{algorithm}
Also, notice that the dependency graph of which subproblems require each other is a DAG.
Therefore, we can take an order on the dependencies and iterate:
\begin{algorithm}
  \caption{\Call{FibIterative}{$n$}}
  \begin{algorithmic}[1]
    \State $T \gets [0,1,\bot,\dotsc,\bot]$ 0-indexed array of size $n$
    \For{$i = 2,\dotsc,n$}
      \State $T[i] \gets T[i-1] + T[i-2]$
    \EndFor
    \State \Return{$T[n]$}
  \end{algorithmic}
\end{algorithm}
This is our dynamic programming algorithm.
In fact, we can optimize even more by noticing that we can discard
all but the last two elements of the array,
giving a constant-space algorithm:
\begin{algorithm}[H]
  \caption{\Call{FibOptimal}{$n$}}
  \begin{algorithmic}[1]
    \State $(u,v) \gets (0,1)$
    \For{$i = 2,\dotsc,n$}
      \State $(u,v) \gets (v,u+v)$
    \EndFor
    \Return{$v$}
  \end{algorithmic}
\end{algorithm}
All these improved algorithms run in $O(n)$ time, a significant improvement.

We can give a general recipe for dynamic programming algorithms:
\begin{enumerate}[1.]
  \item \textbf{Identify the subproblem:}
        We are retaining solutions in an array.
        What are the dimensions of the array?
        What does each entry represent?
        Where will the final answer be in the array?
  \item \textbf{Establish DP-recurrence:}
        How does a subproblem contribute to a larger subproblem?
        What is the dependency between cells in the array?
  \item \textbf{Set base cases:}
        Initialize the array with some non-recursively defined base cases.
  \item \textbf{Specify the order of computation:}
        Clarify the DAG of subproblem dependencies.
        How does the algorithm maintain this order?
  \item \textbf{Recover the solution (if needed):}
        What subproblem answers provide the problem solution?
        How, if necessary, do we traceback the solution from the subproblems?
\end{enumerate}

We can often convert a DP algorithm into iterative loop(s).
Distinguish divide and conquer algorithms which do not always solve subproblems
and are not easily rewritten iteratively.

\section{Interval Scheduling}

\begin{problem}[weighted interval scheduling]
  Recall \cref{prob:g:intsched}.
  Now, add a weight $w_i$ to each interval.
  We choose a subset $T \subseteq [n]$ which maximizes $W = \sum_{i \in T} w_i$.
\end{problem}

\begin{example}
  Let $I = [[2,8], [2,4], [5,6], [7,9]]$ with weights $[6,2,1,2]$.
\end{example}
\begin{sol}
  By inspection, since the weight $w_1 = 6 > 5 = w_2 + w_3 + w_4$,
  the solution is $T = [1]$ with $W = 6$.
\end{sol}

Notice that we can split on whether we accept the last interval $I_n$
and write for example that the optimal weight
\[
  W(I_1,\dotsc,I_n) = \begin{cases*}
    w_n + W(I_{m_1},\dotsc,I_{m_s}) & if we choose $I_n$ \\
    W(I_1,\dotsc,I_{n-1})           & if we do not
  \end{cases*}
\]
where $I_{m_1},\dotsc,I_{m_s}$ are the $s < n$ intervals not intersecting $I_n$.

Suppose we sort the intervals by finish time, i.e., $f_i \leq f_{i+1}$ for all $i$.
Then, we have $m_1,\dotsc,m_s = 1,\dotsc,j$ where $j = \max\{ i : f_i < s_n \}$
because $I_n$ is the last interval with the latest finish time,
so we only need to compare its start time with earlier intervals' finish times.
(If $j$ does not exist just return $w_n + 0$.)

We need to calculate the $j$-values for every $i$:
\begin{algorithm}[H]
  \caption{\Call{FindJs}{$A,s_1,\dotsc,s_n,f_1,\dotsc,f_n$}}
  \begin{algorithmic}[1]
    \State $j \gets$ array of size $n$
    \State $f_0 \gets \infty$
    \State $i \gets 1$
    \For{$k=0,\dotsc,n$}
      \While{$i \leq n$ and $f_k \leq s_{A[i]} < f_{k+1}$}
        \State $j[i] \gets k$
        \State $i\pp$
      \EndWhile
    \EndFor
    \State \Return{$j$}
  \end{algorithmic}
\end{algorithm}
where $A$ is a sorting permutation such that $(s_{A[i]})$ is non-decreasing.
This runs in $O(n\log n) + O(n) = O(n\log n)$ time.

Now, for the main procedure, we define $W[i]$ as the maximal weight
possible with the intervals $I_1,\dotsc,I_i$.

Then, for $W[0] = 0$ and $i \geq 1$, $W[i] = \max\{W[i-1], w_i + W[j[i]]\}$.

Since $W[i]$ depends only on entries in $W$ before it,
we can just iterate on $i = 1,\dotsc,n$ in $O(n)$ time.

\begin{algorithm}[H]
  \caption{\Call{IntervalScheduling}{$s_1,\dotsc,s_n,f_1,\dotsc,f_n,w_1,\dotsc,w_n$}}
  \begin{algorithmic}[1]
    \Require intervals are sorted by finish time
    \State $A \gets$ sorting permutation of $s_1,\dotsc,s_n$
    \State $j \gets \Call{FindJs}{A,s_1,\dotsc,s_n,f_1,\dotsc,f_n}$
    \State $W \gets$ 0-indexed array of size $n$
    \State $W[0] \gets 0$
    \For{$i = 1,\dotsc,n$}
      \State $W[i] \gets \max\{W[i-1], w_i + W[j[i]]\}$
    \EndFor
    \State \Return{$W[n]$}
  \end{algorithmic}
\end{algorithm}

This gives a total time for the algorithm of $O(n\log n) + O(n) = O(n\log n)$.

\section{Knapsack Problem}

\begin{problem}[0/1 knapsack]\label{prob:dp:k}
  Suppose we have items with weights $w_1,\dotsc,w_n$
  and values $v_1,\dotsc,v_n$ but our knapsack has capacity $W$.
  We want to select items $S \subseteq \{1,\dotsc,n\}$
  satisfying $\sum_{i \in S} w_i \leq W$
  and maximizes $\sum_{i \in S} v_i$.
\end{problem}

\begin{example}
  $\vb w = [3,4,6,5]$, $\vb v = [2,3,1,5]$, $W = 8$.
\end{example}
\begin{sol}
  The optimal $S = \{1,4\}$ with weight $3+5 = 8$ and value $2+5 = 7$.
\end{sol}

\lecture{(07/06)}
For each item $n$, we can either choose it or we can not.
Let $O[W,n]$ be best value for a knapsack of capacity $W$
and considering only the items $1,\dotsc,n$.
Then, $O[W,n]$ is either $v_n + O[W - w_n, n - 1]$ or $O[W, n - 1]$.

We can initialize $O[0, i] = 0$ for all $i$ and $O[w, 0] = 0$ for all $w$.
To be able to calculate $O[W,n]$, we must have already calculated
$O[W - w_n .. W, n-1]$.
In particular, if we iterate on $n$ first, we can guarantee that
the entire row $O[,n-1]$ exists before considering $O[W,n]$:

\begin{algorithm}[H]
  \caption{\Call{01KnapSack}{$v_1,\dotsc,v_n,w_1,\dotsc,w_n,W$}}
  \begin{algorithmic}[1]
    \State $O \gets$ 0-indexed array of size $(n+1) \times (W+1)$
    \State $O[0,] \gets \vb{0}$; $O[,0] \gets \vb{0}\trans$
    \For{$i = 1,\dotsc,n$}
      \For{$w = 1,\dotsc,W$}
        \If{$w_i > w$}
          \State $O[w,i] \gets O[w,i-1]$
        \Else
          \State $O[w,i] \gets \max\{v_n + O[W - w_n, n - 1], O[W, n - 1]\}$
        \EndIf
      \EndFor
    \EndFor
    \State \Return{$O[W,n]$}
  \end{algorithmic}
\end{algorithm}

The runtime here is obviously $\Theta(nW)$.
We call this \term{pseudo-polynomial} because it is polynomial in
$n$ (the size of the input) but also in $W$ (the \emph{value} of an input).
It is not polynomial because the size parameters are $n$ and $\lg W$,
but we have $n2^{\lg W}$.

\section{Subsequence Problems}

\begin{problem}[longest increasing subsequence]
  Find the longest (potentially discontinuous)
  increasing subsequence of an array $A[1..n]$ of integers.
\end{problem}
\begin{example}
  Given $A = [7,1,3,10,11,5,19]$, the longest increasing subsequence is
  $[1,3,10,11,19]$.
\end{example}

Notice that there are $\Theta(2^n)$ subsequences, so brute force is very bad here.

Suppose we try doing DP and storing $\ell[i]$ as the longest increasing subsequence of $A[1..i]$.
This doesn't work, since we can't immediately deduce $\ell[i+1]$ from just $\ell[i]$ and $A$.

We could instead store into $L[i]$ a pair of the length and the last entry $(\ell, c)$.
Then, we can add on the next element $L[i] \gets (\ell + 1, A[i])$,
but what is $L[i]$ if we do not select $A[i]$?

Alternatively, let $L[i]$ be the length of the longest increasing subsequence
of $A[1..i]$ that ends with $A[i]$. Then, $L[1] = 1$.
The longest increasing subsequence $S_i$ ending at $A[i]$
either looks like $[\dotsc,A[j],A[i]] = [\dots S_j, A[i]]$ for some $j$
or just $[A[i]]$.

\begin{algorithm}[H]
  \caption{\Call{LongestIncreasingSubsequence}{$A[1..n]$}}
  \begin{algorithmic}[1]
    \State $L \gets$ array of size $n$
    \State $L[1] \gets 1$
    \For{$i = 2,\dotsc,n$}
      \State $L[i] \gets 1$ \Comment{$S_i = [A[i]]$}
      \For{$j = 1,\dotsc,i-1$} \Comment{$S_i = [\dots S_j, A[i]]$}
        \If{$A[j] < A[i]$}
          \State $L[i] \gets \max\{L[i], L[j] + 1\}$
        \EndIf
      \EndFor
    \EndFor
    \State \Return{$\max L$}
  \end{algorithmic}
\end{algorithm}

This algorithm runs in $\Theta(n^2)$ time which is much faster than $\Theta(2^n)$.
Note that we don't return the actual sequence here, only its length,
but it is trivial to find the sequence from the array $L$.

\begin{problem}[longest common subsequence]
  Given two arrays of characters (strings) $A[1..n]$ and $B[1..m]$,
  find the maximum length of a (potentially discontinuous) subsequence
  common to both $A$ and $B$.
\end{problem}
\begin{example}
  For $A = \vv{blurry}$ and $B = \vv{burger}$, we should return $\vv{burr}$ for $k=4$.
\end{example}

As with \cref{prob:dp:k}, we have to work in a 2D problem space.
Let $M[i,j]$ be the longest subsequence length between $A[1..i]$ and $B[1..j]$.
Zero out $M[0,]$ and $M[,0]$.
Then, $M[i,j]$ will be the greatest of either
(1) ignoring $B[j]$, (2) ignoring $A[i]$, or (3) adding $A[i] = B[j]$:

\begin{algorithm}
  \caption{\Call{LongestCommonSubsequence}{$A[1..n],B[1..m]$}}
  \begin{algorithmic}[1]
    \State $M \gets$ 0-indexed array of size $n+1 \times m+1$
    \State $M[0,] \gets \vb{0}$; $M[,0] \gets \vb{0}\trans$
    \For{$i = 1,\dotsc,n$}\For{$j = 1,\dotsc,m$}
        \State $M[i,j] \gets \max\{M[i,j-1], M[i-1,j]\}$
        \If{$A[i] = B[j]$}
          \State $M[i,j] \gets \max\{M[i,j], 1 + M[i-1,j-1]\}$
        \EndIf
      \EndFor
    \EndFor
    \State \Return{$M[n,m]$}
  \end{algorithmic}
\end{algorithm}

Notice that because we iterate by $i$ first, $M[i-1,0..m]$ will have values.
Also, since we are iterating by increasing $j$, $M[i,1..j-1]$ will be calculated.
Therefore, this algorithm works and runs in $\Theta(nm)$ time.

\lecture{(07/11)}
\begin{problem}[edit distance]
  Given two arrays of characters $A[1..n]$ and $B[1..m]$,
  what is the minimum number of
  \textcolor{Green}{add}, \textcolor{Red}{delete}, or \textcolor{Orange}{change}
  operations are required to turn $A$ into $B$?
\end{problem}

\begin{example}
  For $A = \vv{snowy}$, $B = \vv{sunny}$, the edit distance is 3.

  Both $\vv{s{\color{Orange}now}y} \to \vv{s{\color{Orange}unn}y}$ and
  $\vv{sn{\color{Orange}o}{\color{Red}w}y} \to \vv{s{\color{Green}u}n{\color{Orange}n}y}$
  each take 3 operations.
\end{example}

Let $D[i,j]$ be the edit distance between $A[1..i]$ and $B[1..j]$.
Then, $D[0,j] = j$ for all $j$ (add $j$ characters to the empty string)
and $D[i,0] = i$ for all $i$ (add $i$ characters to the empty string).
Otherwise, the value of $D[i,j]$ is the minimum of:
\begin{itemize}[nosep]
  \item Changing the last character: $D[i-1,j-1] + \delta_{A[i]B[j]}$
  \item Deleting $A[i]$ and matching $A[1..i-1]$ with $B[1..j]$: $D[i-1,j]+1$
  \item Adding $B[j]$ and matching $A[1..i]$ with $B[1..j-1]$: $D[i,j-1]+1$
\end{itemize}
Computing all values of $D$ takes $\Theta(mn)$ time.

\section{Graph Algorithms}

\begin{problem}[optimal BST]
  Given objects indexed by $1,\dotsc,n$
  with probabilities of access $p_1,\dotsc,p_n$ (with $\sum p_i = 1$),
  what is the optimal binary search tree that minimizes the expected access time?
\end{problem}

This problem is similar to the greedy Huffman tree problem (where frequencies are given instead of probabilities)
and finding an optimal ordering for a linked list (greedily sort by probability).

The expected access time is $\sum_{i=1}^n p_i(\vv{depth}(i)+1)$.

\begin{example}\label{ex:dp:bst}
  A possible BST with $n=6$ with the cost of each node:
  \tikz[vertalign]\graph[math nodes,grow down,tree layout]{
  p_4 -> {
  2p_2 -> {3p_1,3p_3},
  2p_5 -> {,3p_6}
  }
  };
\end{example}

We will split into subproblems based on where we place the root of a subtree.
Let $M[i,j]$ be the minimal cost for a tree of the items $\{i,\dotsc,j\}$
and $M[i,j] = 0$ when $j < i$.
If we select $k$ as the root of the $i,j$-subtree,
then the cost of the tree is the cost of the $i,k-1$ and $k+1,j$ subtrees plus $p_k$.

When we place a subtree beneath $k$, the depth of every node increases by 1.
That is, we can take the weight from the $M$ array and add on one more copy of $p_\ell$
for each node $\ell$ in the subtree:
\begin{center}
  \begin{tikzpicture}[vertalign]
    \graph[math nodes,grow down,tree layout] {
      p_4[Violet] --[white] {
      p_2[Red] -> {2p_1[Red],2p_3[Red]},
      p_5[ForestGreen] -> {,2p_6[ForestGreen]}
      }
      };
  \end{tikzpicture}
  becomes
  \begin{tikzpicture}[vertalign]
    \graph[grow down,tree layout] {
      "$p_4$"[Violet] -> {
      "${\color{Red}p_2}+{\color{Violet}p_2}$" -> {"${\color{Red}2p_1}+{\color{Violet}p_1}$","${\color{Red}2p_3}+{\color{Violet}p_3}$"},
      "${\color{ForestGreen}p_5}+{\color{Violet}p_5}$" -> {,"${\color{ForestGreen}2p_6}+{\color{Violet}p_6}$"}
      }
      };
  \end{tikzpicture}
  to get \cref{ex:dp:bst}.
\end{center}
Then,
\begin{align*}
  M[i,j]
   & = \min_{i \leq k \leq j}\qty({\color{Red}M[i,k-1]} + {\color{Violet}\sum_{\ell=i}^{k-1}p_\ell} + {\color{Violet}p_k} + {\color{ForestGreen}M[k+1,j]} + {\color{Violet}\sum_{\ell=k+1}^j p_\ell}) \\
   & = \min_{i \leq k \leq j}({\color{Red}M[i,k-1]} + {\color{ForestGreen}M[k+1,j]}) + \color{Violet}\sum_{\ell=i}^j p_\ell
\end{align*}
Notice that $\sum_{\ell=i}^j p_\ell = \sum_{\ell=1}^j p_\ell - \sum_{\ell=1}^{i-1} p_\ell$.
We can cache these sums in $O(n)$ time.
We now have our algorithm:

\begin{algorithm}[H]
  \caption{\Call{OptimalBST}{$p_1,\dotsc,p_n$}}
  \begin{algorithmic}[1]
    \State $S[0] \gets 0$
    \For{$i = 1,\dotsc,n$}
      \State $S[i] \gets S[i-1] + p_i$
    \EndFor
    \For{$i=1,\dotsc,n+1$}
      \State $M[i,i-1] \gets 0$
    \EndFor
    \For{$d = 0,\dotsc,n-1$} \Comment{$d = j-i$}
      \For{$i = 1,\dotsc,n-d$}
        \State $j \gets d + i$
        \State $M[i,j] \gets \min_{i \leq k \leq j}(M[i,k-1]+M[k+1,j] + S[j] - S[i-1])$
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}
which runs in $O(n^3)$ time.

\begin{problem}[largest independent set of a tree]
  Given a tree $T = (V,E)$, what is the largest \term{independent set}
  (i.e., a set $S \subseteq V$ such that $S^2 \cap E = \varnothing$)?
\end{problem}

\begin{example}
  If $G = \tikz[vertalign]\graph[clockwise]{subgraph C_n[n=4]; 2--4};$,
  then $S = \{1,3\}$ and we return 2.
\end{example}

Let $I[v]$ be the size of the largest independent set of the subtree rooted at $v$.
We can either include $v$ in the independent set or exclude it.
If we include it, take the sum of the $I[u]$ for each grandchild $u$.
Otherwise, we take the sum of the $I[u]$ for the children of $u$.

We will have to calculate $I$ bottom-up, but otherwise this gives us an $O(n)$ algorithm.

\section{Shortest Path Revisited: Bellman--Ford}

\lecture{(07/18)}
Recall \cref{prob:g:sp} (\nameref{prob:g:sp}) that we solved with Dijkstra's.

Let $\delta_i(s,v)$ be the length of the shortest path $s \leadsto v$ with at most $i$ edges
and $\infty$ if no such path exists.

Then, we get a base case $\delta_0(s,s) = 0$ and $\delta_0(s,v) = \infty$ for $v \neq s$.
By construction, $\delta(s,v) \leq \delta_i(s,v)$ for all $i$ and $v$.
In particular, if there are no negative cycles, $\delta_{n-1}(s,v) = \delta(s,v)$.\footnote{
  Since a path with no cycles on a graph with $n$ vertices
  can only have at most $n-1$ edges (if it is Hamiltonian).}
Therefore, we only need to calculate for $i = 0,\dotsc,n-1$.

Now, consider $\delta_i(s,v)$.
The size of the shortest path is either exactly $i$ or at most $i-1$.
If it is at most $i-1$, then $\delta_i(s,v) = \delta_{i-1}(s,v)$.
Otherwise, there is an $i\xth$ edge $uv$ and the length is
exactly $\delta_{i-1}(s,u) + w(uv)$.
That is, we can calculate all of $\delta_{i-1}$ by:
\begin{algorithm}[H]
  \caption{\Call{BellmanFord}{$G,s$}}
  \begin{algorithmic}[1]
    \State $d_0 \gets [\infty,\dotsc,\infty]$; $d_0[s] \gets 0$
    \State $\vv{parent} \gets [\bot,\dotsc,\bot]$; $\vv{parent}[s] \gets s$
    \For{$i = 1,\dotsc,n-1$}
      \For{$v \in V$}
        \State $d_i[v] \gets d_{i-1}[v]$
        \For{$u$ neighbour of $v$}
          \If{$d_{i-1}[u] + w(u,v) < d_i[v]$}
            \State $d_i[v] \gets d_{i-1}[u] + w(uv)$
            \State $\vv{parent}[v] = u$
          \EndIf
        \EndFor
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}
The runtime here is not great ($O(n^2m)$).
We can improve by noticing that we can use only one $d$ array.
Instead of iterating over edges incident to vertices, iterate over every edge $uv$.
If $u$ and $v$ are both undiscovered,
we get $d[u] + w(uv) < d[v] = \infty + w(uv) < \infty$ which is false,
so the effect is the same:
\begin{algorithm}[H]
  \caption{\Call{BellmanFord2.0}{$G,s$}}
  \begin{algorithmic}[1]
    \State $d \gets [\infty,\dotsc,\infty]$; $d[s] \gets 0$
    \State $\vv{parent} \gets [\bot,\dotsc,\bot]$; $\vv{parent}[s] \gets s$
    \For{$i = 1,\dotsc,n-1$}
      \For{$uv \in E$}
        \If{$d[u] + w(u,v) < d[v]$}
          \State $d[v] \gets d[u] + w(uv)$
          \State $\vv{parent}[v] = u$
        \EndIf
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}
but we only use $O(nm)$ time, which is better (compare with Dijkstra's $O((n+m)\log m)$).

\lecture{(07/18)}
We now prove that this simplification (and the original) works.
Call the operation of considering a new edge and setting
$d[v] \gets \min\{d[v],d[u] + w(u,v)\}$ a \term{relaxation}.
\begin{claim}\label{claim:dp:bf1}
  After iteration $i$, we have $d \leq d_i$.
  Also, $d[v]$ only decreases through relaxations.
\end{claim}
\begin{prf}
  Induct on $i$.
  At $i = 0$, $d_0 = [0,\infty,\dotsc,\infty] \leq d$.

  Suppose $d \leq d_{i-1}$.
  Since we are assigning $d[v] \gets \min\{d[v], \dotsc\}$,
  obviously $d[v]$ can only decrease with each relaxation.
  Then, $d \leq d_{i-1}$ holds throughout the iteration.
  For each vertex:

  \vspace*{1em}
  \begin{equation*}
    d[v] \gets \min\{\eqnmark{dvl}{d[v]}, \min_{uv \in E}(\eqnmark{sql}{d[u]} + w(u,v))\}
    \leq \min\{\eqnmark{dvr}{d_{i-1}[v]}, \min_{uv \in E}(\eqnmark{sqr}{d_{i-1}[u]} + w(u,v))\}
    = d_i[v]
  \end{equation*}
  \annotatetwo[yshift=1em]{above}{dvl}{dvr}{$d[v] \leq d_{i-1}[v]$ by inductive hypothesis}
  \annotatetwo[yshift=-1em]{below, label below}{sql}{sqr}{since $d[u]$ (now) $\leq d[u]$ (at the start) $\leq d_{i-1}[u]$}
  \vspace*{1em}

  so at the end of the iteration, $d \leq d_i$.
\end{prf}

\begin{claim}\label{claim:dp:bf2}
  When there are no negative cycles:
  if $\delta(s,u) \leq d[u]$ and $d(s,v) \leq d[v]$ before a relaxation,
  then $\delta(s,v) \leq d[v]$ afterwards.
\end{claim}
\begin{prf}
  The triangle inequality $\delta(s,v) \leq \delta(s,u) + w(u,v)$
  holds for all $uv \in E$ if there is no negative cycle.
  Then, if $\delta(s,u) \leq d[u]$, we get $\delta(s,v) \leq d[u] + w(u,v)$.
  We are also given $\delta(s,v) \leq d[v]$.
  Therefore, $\delta(s,v) \leq \min\{d[v],d[u] + w(u,v)\}$,
  so $\delta(s,v) \leq d[v]$ afterwards.
\end{prf}

\begin{prop}
  If there are no negative cycles, for all $i$, after iteration $i$,
  $\delta(s,v) \leq d[v] \leq \delta_i(s,v)$ for all $v$.
  After $n-1$ iterations, $\delta(s,v) = d[v]$.
\end{prop}
\begin{prf}
  We get $\delta(s,v) \leq d[v]$ from \Cref{claim:dp:bf2}
  and $d[v] \leq \delta_i(s,v)$ from \Cref{claim:dp:bf1}.

  Recall that $\delta(s,v) = \delta_{n-1}(s,v)$ if there are no negative cycles.
  Then, at iteration $i=n-1$,
  we have $\delta(s,v) \leq d[v] \leq \delta_{n-1}(s,v) = \delta(s,v)$,
  i.e., $\delta(s,v) = d[v]$.
\end{prf}

\begin{prop}
  If there is a negative cycle, then there must be an edge $uv$
  with $d[u] > d[v] + w(u,v)$ (i.e., violates the triangle inequality).
\end{prop}
\begin{prf}
  Let $C = v_1,\dotsc,v_k = v_1$ be a negative cycle with $\sum w(v_i,v_{i+1}) < 0$.
  Suppose for a contradiction that all $d[v_{i+1}] \leq d[v_i] + w(v_i,v_{i+1})$.
  Sum the inequality around the cycle:
  \begin{align*}
    \sum_{i=1}^{k-1} d[v_{i+1}] & \leq \sum_{i=1}^{k-1} (d[v_i] + w(v_i,v_{i+1}))                \\
    \sum_{i=1}^{k-1} d[v_{i+1}] & \leq \sum_{i=1}^{k-1} d[v_i] + \sum_{i=1}^{k-1} w(v_i,v_{i+1}) \\
    0                           & \leq \sum_{i=1}^{k-1} w(v_i,v_{i+1})
  \end{align*}
  which means this is a positive cycle. Contradiction.
\end{prf}

Therefore, we can test for a negative cycle after calculating $d$
by analyzing all edges and finding one where the triangle inequality does not hold.

\section{All Shortest Paths: Floyd--Warshall}

We now solve a variation of the single-source shortest-path problem.

\begin{problem}[all shortest paths]
  Given a graph $G = (V, E)$ with weights $w : E \mapsto \R$,
  find every value of $\delta(u,v)$ for all $u, v \in V$.
\end{problem}

Floyd--Warshall is very simple, but slower than other algorithms.
Notice that we are not restricting weights to be non-negative,
but still disallow negative cycles.

Instead of setting the path length as the subproblem parameter like Bellman--Ford,
we instead restrict which vertices can be used.

Let $D_i(v_j,v_k)$ be the length of the shortest path $v_j \leadsto v_k$
with all intermediate vertices in $\{v_1,\dotsc,v_k\}$.

Initially, $D_0(v_j,v_k) = \begin{cases}
    0          & j = k            \\
    w(v_j,v_k) & v_jv_k \in E     \\
    \infty     & \text{otherwise}
  \end{cases}$

Then, by construction, $D_n(v_j,v_k) = \delta(v_j,v_k)$

\begin{claim}
  $D_i(v_j,v_k) = \min\{D_{i-1}(v_j,v_k), D_{i-1}(v_j,v_i) + D_{i-1}(v_i,v_k)\}$
\end{claim}
\begin{prf}
  The shortest path $P : v_j \leadsto v_k$ either goes through $v_i$ or does not.
  Since there are no negative cycles, it can only go through it exactly once.
\end{prf}

This gives the algorithm:
\begin{algorithm}[H]
  \caption{\Call{FloydWarshall}{$G$}}
  \begin{algorithmic}[1]
    \State initialize $D_0$ as above
    \For{$i = 1,\dotsc,n$}
      \For{$j = 1,\dotsc,n$}
        \For{$k = 1,\dotsc,n$}
          \State $D_i[v_j,v_k] \gets \min\{D_{i-1}[v_j,v_k],D_{i-1}[v_j,v_i] + D_{i-1}[v_i,v_k]\}$
        \EndFor
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}
which has runtime $O(n^3)$ (similar to running Bellman--Ford on every source in $O(mn^2)$).

