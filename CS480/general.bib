@misc{carlini_poisoning_2023,
 title = {Poisoning Web-Scale Training Datasets is Practical},
 url = {http://arxiv.org/abs/2302.10149},
 doi = {10.48550/arXiv.2302.10149},
 abstract = {Deep learning models are often trained on distributed, webscale datasets crawled from the internet. In this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious examples to a model's performance. Our attacks are immediately practical and could, today, poison 10 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet content to ensure a dataset annotator's initial view of the dataset differs from the view downloaded by subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have poisoned 0.01\% of the {LAION}-400M or {COYO}-700M datasets for just \$60 {USD}. Our second attack, frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content -- such as Wikipedia -- where an attacker only needs a time-limited window to inject malicious examples. In light of both attacks, we notify the maintainers of each affected dataset and recommended several low-overhead defenses.},
 number = {{arXiv}:2302.10149},
 publisher = {{arXiv}},
 author = {Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A. and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tram√®r, Florian},
 urldate = {2024-03-26},
 date = {2023-02-20},
 eprinttype = {arxiv},
 eprint = {2302.10149 [cs]},
}
