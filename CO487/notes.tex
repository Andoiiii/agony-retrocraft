\documentclass[class=co487,tikz,minted,notes]{agony}
\declaretheoremstyle[
	headfont=\bfseries\color{MidnightBlue},
	mdframed={style=mdround,linecolor=RoyalBlue,backgroundcolor=CornflowerBlue!5},
	headpunct={\\[3pt]},
	postheadspace={0pt}
]{thmroundblue}
\declaretheorem[name=Cryptoscheme,refname={scheme,schemes},style=thmroundblue,parent=chapter]{scheme}
\declaretheorem[name=Attack,refname={attack,attacks},style=thmroundpink,sibling=scheme]{attack}

\title{CO 487 Winter 2024: Lecture Notes}
\begin{document}
\renewcommand{\contentsname}{CO 487 Winter 2024:\\{\huge Lecture Notes}}
\thispagestyle{firstpage}
\tableofcontents

Lecture notes taken, unless otherwise specified,
by myself during the Winter 2024 offering of CO 487,
taught by Alfred Menezes.

\begin{multicols}{2}
  \listoflecture
\end{multicols}

\chapter{Introduction}
\lecture{Jan 8}

Cryptography is securing communications in the presence of malicious adversaries.
To simplify, consider Alice and Bob communicating with the eavesdropper Eve.
Communications should be:
\begin{itemize}[noitemsep]
  \item Confidential: Only authorized people can read it
  \item Integral: Ensured that it is unmodified
  \item Origin authenticated: Ensured that the source is in fact Alice
  \item Non-repudiated: Unable to gaslight the message existing
\end{itemize}
Examples: TLS for intenet browsing, GSM for cell phone communications,
Bluetooth for other wireless devices.

\paragraph{Overview: Transport Layer Security} The protocol used by browsers
to visit websites.
TLS assures an individual user (a \term{client})
of the authenticity of the website (a \term{server})
and to establish a secure communications \term{session}.

TLS uses \term{symmetric-key cryptography}.
Both the client and server have a shared secret $k$ called a \term{key}.
They can then use AES for encryption and HMAC for authentication.

To establish the shared secret, use \term{public-key cryptography}.
Alice can encrypt the session key $k$ can be encrypted with Bob's RSA public key.
Then, Bob can decrypt it with his private key.

To ensure Alice is getting an authentic copy of Bob's public key,
a \term{certification authority} (CA) signs it using the CA's private key.
The CA public key comes with Alice's device preinstalled.

Potential vulnerabilities when using TLS:
\begin{itemize}[noitemsep]
  \item Weak cryptography scheme or vulnerable to quantum computing
  \item Weak random number generation for the session key
  \item Fraudulent certificates
  \item Implementation bugs
  \item Phishing attacks
  \item Transmission is secured, but the endpoints are not
\end{itemize}
These are mostly the purview of cybersecurity,
of which cryptography is a part.
Cryptography is not typically the weakest link in the cybersecurity chain.

\chapter{Symmetric key encryption}
\lecture{Jan 10}

\section{Basic concepts}

\begin{defn}[symmetric-key encryption scheme]
  A \term*{symmetric-key encryption scheme} (SKES) consists of:
  \begin{itemize}[nosep]
    \item plaintext space $M$,
    \item ciphertext space $C$,
    \item key space $K$,
    \item family of encryption functions $E_k : M \to C$ for all keys $k \in K$, and
    \item family of decryption functions $D_k : C \to M$ for all keys $k \in K$
  \end{itemize}
  such that $D_k(E_k(m)) = m$ for all $m$ and $k$.
\end{defn}

For Alice to send a message to Bob:
\begin{enumerate}[1.,nosep]
  \item Alice and Bob agree on a secret key $k$ \emph{somehow} (assume a secured channel)
  \item Alice computes $c = E_k(m)$ and sends $c$ to Bob
  \item Bob recovers the plaintext by computing $m = D_k(c)$
\end{enumerate}

Examples include the Enigma and Lorenz machines.

\begin{scheme}[simple substitution cipher]
  Let:
  \begin{itemize}[nosep]
    \item $M$ be English messages
    \item $C$ be encrypted messages
    \item $K$ be permutations of the English alphabet
    \item $E_k(m)$ apply the permutation $k$ to $m$, one letter at a time
    \item $D_k(c)$ apply the inverse permutation $k^{-1}$ to $c$, one letter at a time
  \end{itemize}
\end{scheme}

We want a system to have:
\begin{enumerate}[nosep]
  \item Efficient algorithms should be known for computing (encryption and decryption)
  \item Small keys but large enough to render exhaustive key search infeasible
  \item Security
  \item Security against its designer
\end{enumerate}

To determine how secure the protocol is, we have to define security.

\begin{defn}[security model]
  Some parameters which define the strength of the adversary,
  specific interaction with the ``secure'' channel,
  and the goal of the adversary.
\end{defn}

Some options for strength:
\begin{itemize}[nosep]
  \item \term[security!information-theoretic]{Information-theoretic security}: Eve has infinite resources.
  \item \term[security!complexity-theoretic]{Complexity-theoretic security}: Eve is a polynomimal-time Turing machine.
  \item \term[security!computational-theoretic]{Computational-theoretic security}: Eve has a specific amount of computing power.
        In this course, Eve is \term{computationally bounded} by
        6,768 Intel E5-2683 V4 cores running at 2.1 GHz at her disposal.
\end{itemize}
For the interaction:
\begin{itemize}[nosep]
  \item \term[attack!ciphertext-only]{Ciphertext-only attack}: Eve only knows the ciphertext.
  \item \term[attack!known-plaintext]{Known-plaintext attack}: Eve knows some plaintext and the corresponding ciphertext.
  \item \term[attack!chosen-plaintext]{Chosen-plaintext attack}: Eve picks some plaintext and knows the corresponding ciphertext.
  \item \term[attack!clandestine]{Clanedestine attack}: Eve resorts to bribery, blackmail, etc.
  \item \term[attack!side-channel]{Side-channel attack}: Eve has physical access to hardware and has some monitoring data.
\end{itemize}
And for the goal:
\begin{itemize}[nosep]
  \item Recovering the secret key $k$
  \item Systematically decrypt arbitrary ciphertexts without knowing $k$ (\term[security!total]{total security})
  \item Learn partial information about the plaintext (other than the length) (\term[security!semantic]{semantic security})
\end{itemize}

\begin{defn*}[security]
  An SKES is \term[symmetric-key encryption scheme!security]{secure}
  if it is semantically secure against a chosen-plaintext attack
  by a computationally bounded adversary.
\end{defn*}

Equivalently, an SKES is \term{broken} if:
\begin{enumerate}[nosep]
  \item Given a challenge ciphertext $c$ for $m$ generated by Alice,
  \item \dots and access to an encryption oracle for Alice,
  \item \dots Eve can obtain some information about $m$ other than its length,
  \item \dots using only a feasible amount of computation.
\end{enumerate}
Note: this is IND--CPA from CO 485.

\begin{example}
  Is the simple substitution cipher secure?
  What about under a ciphertext-only attack?
\end{example}
\begin{sol}
  Under CPA, encrypt the entire alphabet.
  Then, the entire key $k$ is recovered.

  With a ciphertext-only attack, an exhaustive key search would take
  $26! \approx 2^{88}$ attempts.
  This would take over 1,000 years, which is pretty infeasible,
  so it is secure.
\end{sol}

Can we quantify how feasible something is?

\begin{defn}[security level]
  A scheme has a \term{security level} of $\ell$ bits if
  the fastest known attack on the scheme takes approximately $2^\ell$ operations.
\end{defn}

\begin{convention}
  In this course:
  \begin{itemize}[nosep]
    \item 40 bits is very easy to break
    \item 56 bits is easy to break
    \item 64 bits is feasible to break
    \item 80 bits is barely feasible to break
    \item 128 bits is infeasible to break
  \end{itemize}
\end{convention}

\lecture{Jan 12}
The simple substitution cipher can be attacked by frequency analysis,
since, for example, if ``e'' is the most common English letter,
we check the ciphertext for the most common letter and identify it with ``e''.

\begin{scheme}[Vigen√®re cipher]
  Let the key $K$ be an English word with no repeated letters, e.g., $K = \symrm{CRYPTO}$.

  To encrypt, add letter-wise the key modulo 26, where $k$ is $K$ repeated until
  it matches the length of the message:
  \begin{center}
    \begin{tabular}{rcccccccccccccc}
      $m =$        & t & h & i & s & i & s & a & m & e & s & s & a & g & e \\
      $+\quad k =$ & C & R & Y & P & T & O & C & R & Y & P & T & O & C & R \\ \hline
      $c =$        & V & Y & G & H & B & G & C & D & C & H & L & O & I & V
    \end{tabular}
  \end{center}
  To decrypt, just take $c - k$.
\end{scheme}

This solves our frequency analysis problem.
However, the Vigenere cipher is still totally insecure.

\begin{xca}
  Show that the Vigenere cipher is totally insecure under a chosen-plaintext attack
  and a ciphertext-only attack.
\end{xca}

\begin{scheme}[one-time pad]
  The key is a random string of letters with the same length as the message.

  Repeat the process for Vigenere.
  To encode, add each letter.
  To decode, subtract each letter.
\end{scheme}

\begin{example}
  We can encrypt as follows:
  \begin{center}
    \begin{tabular}{rcccccccccccccc}
      $m =$        & t & h & i & s & i & s & a & m & e & s & s & a & g & e \\
      $+\quad k =$ & Z & F & K & W & O & G & P & S & M & F & J & D & L & G \\ \hline
      $c =$        & S & M & S & P & W & Y & P & F & Q & X & C & D & R & K \\
    \end{tabular}
  \end{center}
\end{example}

This is semantically secure as long as the key is never reused.
Formally, there exist keys that can decrypt the ciphertext into \emph{anything},
so there is no way for an attacker to know the plaintext.
If it is reused, i.e., if $c_1 = m_1 + k$ and $c_2 = m_2 + k$,
then $c_1 - c_2 = (m_1 + k) - (m_2 + k) = m_1 - m_2$.
Since this is a function only of messages, it can leak frequency information etc.

Also, since the key is never reused, this is secure against a chosen plaintext attack,
since one would only recover the already used key.

\begin{convention}
  From now on, messages and keys are assumed to be binary strings.
\end{convention}

\begin{defn}[bitwise exclusive or]
  For two bitstrings $x,y \in \bits{n} \cong \Z/2\Z^n$,
  the bitwise XOR $x \xor y$ is just addition mod 2.
\end{defn}

Unfortunately, due to Shannon, we have this theorem:

\begin{theorem}
  A perfectly secure symmetric-key scheme must have
  at least as many keys as there are messages.
\end{theorem}

\section{Stream ciphers}

Instead of using a random key in the OTP, use a pseudorandom key.

\begin{defn}[pseudorandomness]
  A \term*{pseudorandom bit generator} (PBRG) is a deterministic algorithm
  that takes as input a \term{seed} and outputs a \term*{pseudorandom} sequence called
  the \term{keystream}.
\end{defn}

Then, we can construct a stream cipher by defining the key as the seed
and the ciphertext as the keystream XOR'd with the plaintext.
To decrypt, use the seed to generate the same keystream and XOR with the ciphertext.

For a stream cipher to be secure, we need:
\begin{itemize}
  \item Indistinguishability: the keystream is indistinguishable from a
        truly random sequence; and
  \item Unpredictability: given a partial keystream, it is infeasible to learn
        any information from the remainder of the keystream.
\end{itemize}

\begin{remark}
  Do not use built-in UNIX \texttt{rand} or \texttt{srand} for cryptography!
\end{remark}

Now, we introduce ChaCha20, a stream cipher actually uesd in the real world.
The algorithm works entirely on words (32-bit numbers).
It has no known flaws (other than people bungling the implementation).

\begin{scheme}[ChaCha20]\label{s:chacha20}
  First, define a helper function $QR(a,b,c,d)$ on 32-bit words:
  \begin{enumerate}[nosep]
    \item $a \gets a \modplus b$, $d \gets d \xor a$, $d \gets d \lll 16$
    \item $c \gets c \modplus d$, $b \gets b \xor c$, $b \gets b \lll 12$
    \item $a \gets a \modplus b$, $d \gets d \xor a$, $d \gets d \lll 8$
    \item $c \gets c \modplus d$, $b \gets b \xor c$, $b \gets b \lll 7$
  \end{enumerate}
  where $\xor$ is bitwise XOR, $\modplus$ is addition mod $2^{32}$, and $\lll$ is left bit-rotation.

  Given a 256-bit key $k = (k_1,\dotsc,k_8)$,
  a selected 96-bit nonce $n = (n_1,n_2,n_3)$,
  a 128-bit given constant $f = (f_1,\dotsc,f_4)$,
  and 32-bit counter $c \gets 0$, construct an initial state:
  \[
    S := \begin{bmatrix}
      f_1 & f_2 & f_3 & f_4 \\
      k_1 & k_2 & k_3 & k_4 \\
      k_5 & k_6 & k_7 & k_8 \\
      c   & n_1 & n_2 & n_3
    \end{bmatrix} = \begin{bmatrix}
      S_1    & S_2    & S_3    & S_4    \\
      S_5    & S_6    & S_7    & S_8    \\
      S_9    & S_{10} & S_{11} & S_{12} \\
      S_{13} & S_{14} & S_{15} & S_{16}
    \end{bmatrix}
  \]
  Keep a copy $S' \gets S$, then apply:
  \begin{gather*}
    QR(S_{1}, S_{5}, S_{9}, S_{13}),\quad QR(S_{2}, S_{6}, S_{10}, S_{14}),\quad QR(S_{3}, S_{7}, S_{11}, S_{15}),\quad QR(S_{4}, S_{8}, S_{12}, S_{16}) \\
    QR(S_{1}, S_{6}, S_{11}, S_{16}),\quad QR(S_{2}, S_{7}, S_{12}, S_{13}),\quad QR(S_{3}, S_{8}, S_{9}, S_{14}),\quad QR(S_{4}, S_{5}, S_{10}, S_{15})
  \end{gather*}
  ten times (for 80 total calls to $QR$) and output $S \xor S'$.
  This gives us 64 keystream bytes.

  Increment $c \gets c+1$ and repeat as necessary to generate more keystream bytes.

  To encrypt, XOR the keystream with the plaintext, then append the nonce.

  To decrypt, pop off the nonce, then XOR the keystream with the ciphertext.
\end{scheme}

\lecture{Jan 15}
One must be careful never to reuse nonces, since this results in the same keystream,
leading to recoverable messages.
In practice, this is hard (e.g., two devices with the same key).

Miscellaneous remarks:
\begin{itemize}[nosep]
  \item Why is ChaCha20 so good? The $QR$ function is very fast at the hardware level
        and there is wide adoption/standardization by experts.
  \item Why 10 rounds? If you do 1 or 2 rounds, there is a trivial attack.
        The latest theoretical attacks can attack 7 rounds
        (currently infeasible, but still better than exhaustive key search).
        So 8 rounds is secure and we do 10 to be safe.
  \item Is this secure forever (i.e., can we always just increase rounds)?
        No. Nothing in this course is.
        Someone could find a super crazy PMATH theorem that shows
        predictability of the $QR$ scramble.
\end{itemize}

\section{Block ciphers}

\begin{defn}[block cipher]
  Like a stream cipher, but instead of processing one character at a time,
  we break up the plaintext into \term[block]{blocks} of equal length
  and encrypt block-wise.
\end{defn}

\begin{example}
  The Data Encryption Standard (DES) is a standard
  56-bit key and 64-bit blocks.
\end{example}

\paragraph{Aside: History and the NSA doing ratfuckery}
In 1972, the National Institute of Standards and Technology (NIST)\footnote{of standardized peanut butter fame}
puts out an RfP for encryption algorithms.

IBM developed and proposed 64-bit DES, but then the NSA reduced it in 1975
to 56-bit so they can do some spying.
This made DES feasible to break by nation-states but not smaller organizations.

The National Security Agency (NSA)
is the US' signals intelligence (SIGINT; hacking foreign intelligence)
and information insurance (IA; defending domestic intelligence) agency.
They have a history of regulating how strong cryptoraphic products can be
by banning the export of strong cryptography.

Canada has an NSA equivalent: the Communications Security Establishment (CSE).
Along with the Kiwi CCSA, British GCHQ, and Australian ASD,
these are the Five Eyes who spy on just about everyone.


We only really know stuff about the NSA/Five Eyes due to the Snowden leaks.
For example, the SIGINT Enabling Project attempts to influence/blackmail
companies to weaken their security with backdoors.

Throughout the course, we will use the NSA to mean ``generic nation-state level adversary'',
since if you can defeat the NSA, you can defeat basically anyone.

Anyways, weakened DES was adopted by NIST in 1977 as FIPS 46 in 1977,
then as a banking standard as ANSI X3.92 in 1982 (replaced by Triple-DES in 1988).
From 1997--2001, a new contest developed the Advanced Encryption Standard (AES),
which is the current standard block cipher.

\paragraph{Desired properties of block ciphers} (Shannon, 1949):
\begin{enumerate}[nosep]
  \item Diffusion: Each ciphertext bit should depend on all plaintext bits.
  \item Confusion: The key--ciphertext relationship should be complicated.
  \item Key length: Keys should be small but not too small to be searchable.
  \item Simplicity: Ease of implementation and analysis.
  \item Speed: Runs quickly on all reasonable hardware.
  \item Platform: Can be implemented in hardware and software.
\end{enumerate}

\begin{scheme}[DES]
  The design principles of DES are still classified,
  so we just treat it as a black box for this course.
  We only need to know that there is a 56-bit key and 64-bit blocks.
\end{scheme}

The DES key space is not very big.
Exhaustive search on DES takes $2^{56}$ operations.
In 1997, this took three months.
In 2012, it takes 11.5 hours.

The blocks are also not very large.
By the birthday paradox, there is a collision every $2^{32}$ blocks.
This is an information leak, breaking semantic security.

These are the only (known) weaknesses in DES.

\begin{defn}[multiple encryption]
  Re-encrypt the ciphertext more times with different keys.
\end{defn}

This is not always more secure. For example, in the simple substitution cipher,
permutations can be composed and do not introduce more security.

\begin{scheme}[Double-DES]
  Pick a secret key $k = (k_1,k_2) \randin \bits{112}$.

  Then, encrypt $E_{k_2}(E_{k_1}(m))$ where $E$ is DES encryption

  Likewise, decrypt $E^{-1}_{k_2}(E^{-1}_{k_1}(m))$ where $E^{-1}$ is DES decryption.
\end{scheme}

We now have an exhaustive key search of $2^{112}$ operations, which is better.
However, there is an attack which reduces this to breaking DES.

\textrule{$\downarrow$ Lectures 5, 6, and 7 taken directly from slides $\downarrow$}
\lecture{Jan 17}
\begin{attack}[Meet-in-the-middle attack on Double-DES]\label{atk:mitm}
  The main idea is that $c = E_{k_2}(E_{k_1}(m))$ if and only if
  $E_{k_2}^{-1}(c) = E_{k_1}(m)$.

  Given three plaintext/ciphertext pairs $(m_1,c_1)$, $(m_2,c_2)$ and $(m_3,c_3)$:
  \begin{algorithmic}[1]
    \State Create a table $T$ of pairs sorted by first entry
    \For{$h_2 \in \bits{56}$} \Comment{$h_2$ is a guess for $k_2$}
      \State $T.\vv{insert}(E_{h_2}^{-1}(m_1),h_2)$
    \EndFor
    \For{$h_1 \in \bits{56}$} \Comment{$h_1$ is a guess for $k_1$}
      \State Compute $E_{h_1}(m_1)$
      \State Search for entries in $T$ matching $(E_{h_1}(m_1),-)$
      \For{each match $(-,h_2)$}
        \If{$E_{h_2}(E_{h_1}(m_2)) = c_2$}
          \If{$E_{h_2}(E_{h_1}(m_3)) = c_3$}
            \State \Return{$(h_1,h_2)$}
          \EndIf
        \EndIf
      \EndFor
    \EndFor
  \end{algorithmic}
\end{attack}

In \cref{atk:mitm}, we use three pairs. Why do we need that many?

Since the key space is smaller than the message space,
there will be multiple keys that encrypt a message to the same ciphertext.

\begin{lemma}[number of plaintext-ciphertext pairs needed]
  Let $E$ be a block cipher with $\ell$-bit key space
  and $L$-bit plaintext/ciphertext space.

  If $E$ is a random bijection, the expected number of
  \term{false keys} matching $t$ pairs is $\frac{2^\ell-1}{2^{Lt}}$.
\end{lemma}
\begin{prf}
  We assume that $E$ is a random bijection, so we can calculate probabilities.

  Fix the true key $k'$.
  Let $(m_i,c_i)$ for $i = 1,\dotsc,t$ be known plaintext-ciphertext pairs
  where each plaintext is distinct.

  For some $k \in K$, $k \neq k'$, the probability that $E_k(m_i) = c_i$
  for all $i$ is $\underbrace{\frac{1}{2^L}\cdot\frac{1}{2^L}\cdots\frac{1}{2^L}}_{\text{$t$ times}} = \frac{1}{2^{Lt}}$.

  Therefore, across all of $K \setminus \{k'\}$,
  the expected number of false keys is $\frac{2^\ell-1}{2^{Lt}}$.
\end{prf}

For Double-DES, $\ell = 112$ and $L = 64$:
\begin{itemize}
  \item For $t=1$, $FK \approx 2^{48}$.
        That is, given $(m,c)$ the number of Double-DES keys $(h_1,h_2)$
        for which $E_{h_2}(E_{h_1}(m)) = c$ is $\approx 2^{48}$.
  \item For $t=2$, $FK \approx 2^{-16}$.
        That is, the number of Double-DES keys $(h_1,h_2)$
        for which $E_{h_2}(E_{h_1}(m_1)) = c_1$ is $\approx 2^{48}$.
\end{itemize}

Therefore, we use three plaintext-ciphertext pairs in \cref{atk:mitm}.

The time requirement of the attack is $2^{56} + 2^{57} + 2 \cdot 2^{48} \approx 2^{57}$
DES encryptions/decryptions.
The size of the table is $2^{56}(64+56)$ bits or about 1 million TB.

\begin{xca}
  Modify \cref{atk:mitm} to decrease storage requirements at the expense of time.
  We can get down to $2^{56+s}$ operations and $2^{56-s}$ rows
  for $1 \leq s \leq 55$.
\end{xca}

We can now conclude that the security level of Double-DES is 57 bits,
not much better than normal DES' 56 bits.

\begin{scheme}[Triple-DES]
  Pick a secret key $k = (k_1,k_2,k_3) \randin \bits{168}$.

  Then, encrypt $E_{k_3}(E_{k_2}(E_{k_1}(m)))$ where $E$ is DES encryption

  Likewise, decrypt $E^{-1}_{k_3}(E^{-1}_{k_2}(E^{-1}_{k_1}(m)))$
  where $E^{-1}$ is DES decryption.
\end{scheme}

As for Double-DES, the 168-bit keys are infeasible to search.

\begin{xca}
  Show that \cref{atk:mitm} on Triple-DES takes $2^{112}$ operations.
\end{xca}

This means the security level of Triple-DES is 112 bits.
We cannot \emph{prove} Triple-DES is more secure than DES,
just that it empirically feels better.

\section{Substitution-permutation networks}
\lecture{Jan 19}

\begin{defn}[substitution-permutation network]
  A \term*{substitution-permutation network} (SPN) is an iterated block cipher
  where each iteration (\term{round}) is a substitution followed by a permutation.

  Formally, we have:
  \begin{itemize}[nosep]
    \item a block length $n$, key length $\ell$
    \item number of rounds $h$,
    \item \term{substitution} $S : \bits{b} \to \bits{b}$,
          an invertible function where $b \mid n$,
    \item \term{permutation} $P$,
          an invertible function $\{1,\dotsc,n\} \to \{1,\dotsc,n\}$, and
    \item \term{key scheduling algorithm} $k_i$ that determines a
          \term{round key} for each round $i = 1,\dotsc,h+1$ given a key $k$.
  \end{itemize}
  Note that $n$, $\ell$, $h$, $S$, $P$, and the key scheduling algorithm are public.

  Then, we can write encryption as
  \begin{algorithmic}
    \State $A \gets m$
    \For{$i = 1,\dotsc,h$}
      \State $A \gets A \xor k_i$
      \State $A \gets S(A[1 : b]) \concat S(A[b+1 : 2b]) \concat \dotsb \concat S(A[n-b+1 : n])$
      \Comment{Apply $S$ to each $b$ bits}
      \State $A \gets P(A)$
    \EndFor
    \State $A \gets A \xor k_{h+1}$
    \Return{$A$}
  \end{algorithmic}
  and decryption is the reverse (since $S$ and $P$ are invertible).
\end{defn}

The most notable SPN is the Advanced Encryption Standard (AES)
which was adopted in 2001 as FIPS 197, a U.S. government standard.
It uses 128-bit blocks and either 128, 192, or 256-bit keys.

As of 2024, there are no known AES attacks
that are significantly faster than exhaustive key search.

\begin{scheme}[AES]
  Given a key $k$ and block of plaintext,
  initialize a $4 \times 4$ byte array $\vv{State}$
  containing the plaintext.

  Depending on the key size (128, 192, 256), let $h$ be (10, 12, 14).
  Using the key schedule, generate $h+1$ round keys $k_0,\dotsc,k_h$.
  We will need three helper functions
  \Call{SubBytes}{}, \Call{ShiftRows}{}, and \Call{MixColumns}{}.

  Then, encryption is
  \begin{algorithmic}
    \State $\vv{State} \gets$ block of plaintext
    \State $(k_0,\dotsc,k_h) \gets$ round keys from the key schedule
    \State $\vv{State} \gets \vv{State} \xor k_0$
    \For{$i = 1,\dotsc,h-1$}
      \State $\vv{State} \gets \Call{SubBytes}{\vv{State}}$
      \State $\vv{State} \gets \Call{ShiftRows}{\vv{State}}$
      \State $\vv{State} \gets \Call{MixColumns}{\vv{State}}$
      \State $\vv{State} \gets \vv{State} \xor k_i$
    \EndFor
    \State $\vv{State} \gets \Call{SubBytes}{\vv{State}}$
    \Comment{Note: we skip \Call{MixColumns}{} in the last round}
    \State $\vv{State} \gets \Call{ShiftRows}{\vv{State}}$
    \State $\vv{State} \gets \vv{State} \xor k_h$
    \State \Return{$\vv{State}$}
  \end{algorithmic}
  To decrypt, do everything backwards
  (making calls to \Call{InvSubBytes}{}, \Call{InvShiftRows}{}, and \Call{InvMixColumns}{}).
\end{scheme}

AES does a lot of math over the Galois field $\GF(2^8)$.

\begin{defn*}[$\GF(2^8)$]\label{def:gf28}
  Consider the field $\Z/2\Z[y]$ of polynomials with coefficients in $\Z$ mod 2.

  The finite field $\GF(2^8) = (\Z/2\Z[y])/(y^8+y^4+y^3+y+1)$
  contains those polynomials with degree at most 7.

  Addition and multiplication are defined normally (mod $y^8+y^4+y^3+y+1$).
\end{defn*}

We notate elements $a(y) \in \GF(2^8)$ as the binary string of their coefficients.

\begin{example}
  The string $a = 11101100 = \x{ec}$
  is identified with $a(y) = y^7 + y^6 + y^5 + y^3 + y^2$.
\end{example}

Since polynomial addition is coefficient-wise and $\Z/2\Z$ is isomorphic with XOR,
we can treat $\GF(2^8)$ addition as binary string XOR.

\begin{example}
  Let $b = 00111011 = \x{3b}$.
  Then, $c(y) = a(y) + b(y) = y^7 + y^6 + y^4 + y^2 + y + 1$.
  We could have instead found $c = 11010111 = \x{d7}$
  by noticing that $a \xor b = \x{ec} \xor \x{3b} = \x{d7}$.
\end{example}

Multiplication requires a long division to find the answer mod $y^8 + y^4 + y^3 + y + 1$.

\begin{example}\label{exa:mult}
  Let $d(y) := a(y) \cdot b(y)$. Calculate:
  \begin{align*}
    d(y) & = (y^7 + y^6 + y^5 + y^3 + y^2)(y^5 + y^4 + y^3 + y + 1)                           \\
         & = y^{12} + 2y^{11} + 3y^{10} + 2y^9 + 3y^8 + 4y^7 + 4y^6 + 2y^5 + y^4 + 2y^3 + y^2 \\
         & = y^{12} + y^{10} + y^8 + y^4 + y^2
  \end{align*}
  Then, do polynomial long division:
  \[
    \setlength\arraycolsep{0pt}
    \setlength\extrarowheight{2pt}
    \newcolumntype{q}{>{{}}c<{{}}}
    \begin{array}[t]{rqrqrqrqrqrqrqrqrqrqrqrqrqrqr}
                                     &   &        &   &        &   &     &   &     &   &     &   &     &   & y^4 &   &     & + & y^2 \\
      \cline{2-19}
      y^8 + y^4 + y^3 + y + 1 \bigl) &   & y^{12} & + & y^{10} & + & y^8 & + &     &   &     &   &     &   & y^4 &   &     & + & y^2 \\
                                     & - & y^{12} &   &        & + & y^8 & + & y^7 &   &     & + & y^5 & + & y^4                     \\
      \cline{2-15}
                                     &   &        &   & y^{10} &   &     & + & y^7 &   &     & + & y^5                               \\
                                     &   &        & - & y^{10} &   &     &   &     & + & y^6 & + & y^5 &   &     & + & y^3 & + & y^2 \\
      \cline{4-19}
                                     &   &        &   &        &   &     &   & y^7 & + & y^6 &   &     &   &     & + & y^3
    \end{array}
  \]
  to conclude that the remainder is $y^7 + y^6 + y^3$.
  Therefore, $\x{ec} \cdot \x{3b} = 11001000 = \x{c8}$.
\end{example}

But by the XOR trick from addition, we can do this faster using XOR.

\begin{example}
  First, long multiply $11101100$ by $00111011$ using XOR to reduce:
  \[
    \setlength\arraycolsep{0pt}
    \setlength\extrarowheight{2pt}
    \begin{array}[t]{rrl}
               &       & 11101100 \\
               & 1     & 1101100  \\
               & 111   & 01100    \\
               & 1110  & 1100     \\
      {}\xor{} & 11101 & 100      \\ \hline
               & 10101 & 00010100
    \end{array}
  \]
  Then, do long division by $f = 100011011$ using XOR to subtract:
  \[
    \begin{array}[t]{rrrrr}
      \setlength\arraycolsep{0pt}
      \setlength\extrarowheight{2pt}
                       &    & 1       & 01 & 00 \\ \cline{2-5}
      100011011 \bigl) & 10 & 1010001 & 01 & 00 \\
                       & 10 & 0011011 &         \\ \cline{2-3}
                       & 00 & 1001010 &         \\
                       &    & 1000110 & 11      \\ \cline{3-4}
                       &    & 0001100 & 10 & 00
    \end{array}
  \]
  Therefore, $\x{ec} \cdot \x{3b} = 11001000 = \x{c8}$.
\end{example}

Now, we can define the helper functions.
The substitution in AES is based on the inverse in $\GF(2^8)$.

\begin{defn*}[AES S-box]
  Let $p \in \bits8$. We define $S : \bits8 \to \bits8$.

  Considering $p$ as an element of $\GF(2^8)$,
  let $q = p^{-1}$ (which always exists except if $p = 0$, in which case let $q=0$).
  Treating $q$ as a bit vector, compute
  \[
    S(p) = r = \begin{pmatrix}
      1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\
      1 & 1 & 0 & 0 & 0 & 1 & 1 & 1 \\
      1 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\
      1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 \\
      1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\
      0 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\
      0 & 0 & 1 & 1 & 1 & 1 & 1 & 0 \\
      0 & 0 & 0 & 1 & 1 & 1 & 1 & 1
    \end{pmatrix}q
    + \mqty(1\\1\\0\\0\\0\\1\\1\\0)
  \]
  with scalar arithmetic in $\Z/2\Z$.
\end{defn*}

Then, \Call{SubBytes}{} just applies $S$ to each byte of $\vv{State}$.
The decryption call \Call{InvSubBytes}{} just multiplies by the inverse of the matrix.

The permutation takes two steps:
first, \Call{ShiftRows}{} shifts the $i$\xth row left by $i$ bits.
Then, \Call{MixColumns}{} treats each column as a polynomial in $\GF(2^8)[x]/(x^4-1)$
and multiplies it by $c(x) = \x{02} + \x{01}x + \x{01}x^2 + \x{03}x^3$.

\begin{example}\label{exa:mixcolumn}
  Let $a = \x{d0f112bb}$ be a column.
  Multiply
  \begin{align*}
    a(x) \cdot c(x)
     & = (\x{d0} + \x{f1}x + \x{12}x^2 + \x{bb}x^3)(\x{02} + \x{01}x + \x{01}x^2 + \x{03}x^3)                                                            \\
     & = (\x{d0}\cdot\x{02}) + (\x{d0}\cdot\x{01} + \x{f1}\cdot{02})x + (\x{d0}\cdot\x{01} + \x{f1}\cdot\x{01} + \x{12}\cdot\x{02})x^2                   \\
     & \quad\quad + (\x{d0}\cdot\x{03} + \x{f1}\cdot\x{01} + \x{12}\cdot\x{01} + \x{bb}\cdot\x{02})x^3                                                   \\
     & \quad\quad + (\x{f1}\cdot\x{03} + \x{12}\cdot\x{01} + \x{bb}\cdot\x{01})x^4 + (\x{12}\cdot\x{03} + \x{bb}\cdot\x{01})x^5 + (\x{bb}\cdot\x{03})x^6 \\
     & = \x{bb} + \x{29}x + \x{05}x^2 + \x{e5}x^3 + \x{a1}x^4 + \x{8d}x^5 + \x{d6}x^6
  \end{align*}
  where coefficient arithmetic is in $\GF(2^8)$.
  Find the remainder modulo $\x{01}x^4-\x{01}$
  by replacing $x^4 \mapsto 1$:
  \begin{align*}
    r(x) & = \x{bb} + \x{29}x + \x{05}x^2 + \x{e5}x^3 + \x{a1} + \x{8d}x^2 + \x{d6}x^3 \\
         & = \x{1a} + \x{a4}x + \x{d3}x^2 + \x{e5}x^3
  \end{align*}
  Therefore, $\Call{MixColumn}{\x{d0f112bb}} = \x{1aa4d3e5}$.
\end{example}

Naturally, \Call{InvShiftRows}{} shifts the $i$\xth row right by $i$ bits
and \Call{InvMixColumns}{} multiplies each column by $c^{-1} = \x{0e09d00b}$.

Finally, we can define the key schedule.
For 128-bit keys, we need 11 round keys.
The first round key $k_0 = (r_0,r_1,r_2,r_3)$ is the actual AES key.
Then, each subsequent round key
\begin{align*}
  k_i & = (r_{4i},r_{4i+1},r_{4i+2},r_{4i+3}) = (f(r_{4i-1})\xor r_{4i-4}, r_{4i} \xor r_{4i-3}, r_{4i+1} \xor r_{4i-2}, r_{4i+2} \xor r_{4i-1})
\end{align*}
where $f_i$ maps the four bytes $(a,b,c,d)$ to $(S(b) \xor \ell_i, S(c), S(d), s(a))$
for some round constants $\ell_i$.

\paragraph{Aside: Implementation}
This section is just me doing nerd shit trying to make Assignment 2 easier.
The finite fields used in AES can be replicated in Sage or Mathematica.
In Sage:
\begin{minted}[autogobble,mathescape]{python}
  aes.<y> = GF(2^8, modulus=x^8+x^4+x^3+x+1) # define AES field $(\Z/2\Z)[y]/(f(y))$
  aes_int = aes._cache.fetch_int             # byte to $\GF(2^8)$ element
  mcf.<x> = aes[]                            # MixColumns field $\GF(2^8)[x]$
  hex_string = lambda x: bytes(u.integer_representation() for u in x.list()).hex()

  # $\text{\Cref{exa:mult}}$: multiply ec * 3b
  a, b = aes_int(0xec), aes_int(0x3b)
  r = a * b
  print('r(x):', r)
  print('a*b:', hex(r.integer_representation()))
  # r(x): $y^7 + y^6 + y^3$
  # a*b:  0xc8

  # $\text{\Cref{exa:mixcolumn}}$: MixColumn(d0f112bb)
  a = [0xd0,0xf1,0x12,0xbb]
  ax = mcf([aes_int(u) for u in a])
  cx = (y + x + x^2 + (y+1) * x^3)
  bx = (ax * cx).mod(x^4+1)
  print('b(x):', bx)
  print('b:', hex_string(bx))
  # b(x): $(y^7 + y^6 + y^5 + y^2 + 1)x^3 + (y^7 + y^6 + y^4 + y + 1)x^2 + (y^7 + y^5 + y^2)x + y^4 + y^3 + y$
  # b:    1aa4d3e5
\end{minted}

In Mathematica (version 13.3 or later):
\begin{minted}[autogobble,mathescape]{mathematica}
  F = FiniteField[2, #^8 + #^4 + #^3 + # + 1 &];
  GF[hex_] := F[FromDigits[hex, 16]];
  poly[f_] := Expand@FromDigits[Reverse@f["Coefficients"], y];
  hex[f_] := IntegerString[f["Index"], 16, 2];

  (* $\text{\Cref{exa:mult}}$: multiply ec * 3b *)
  a = GF["ec"];
  b = GF["3b"];
  poly[a*b]      (* $y^3+y^6+y^7$ *)
  hex[a*b]       (* c8 *)

  (* $\text{\Cref{exa:mixcolumn}}$: MixColumn(d0f112bb) *)
  a = GF["d0"] + GF["f1"] x + GF["12"] x^2 + GF["bb"] x^3;
  c = F[2] + F[1] x + F[1] x^2 + F[3] x^3;
  b = PolynomialRemainder[a*c, x^4 - 1, x];
  StringJoin[hex /@ CoefficientList[b, x]] (* 1aa4d3e5 *)
\end{minted}


\section{Block cipher modes of operation}

TODO

\chapter{Hash functions}

\section{Definitions}
\lecture{Jan 22}

\begin{defn}[hash function]
  A mapping $H$ such that
  \begin{enumerate}
    \item $H : \bits{\leq L} \to \bits{n}$ maps binary messages
          of arbitrary lengths $\leq L$ to outputs of a fixed length $n$.
    \item $H(x)$ can be efficiently computed for all $x \in \bits{\leq L}$
  \end{enumerate}
  is an \term*{$n$-bit hash function}.
  We call $H(x)$ the \term{hash} or \term{digest} of $x$.

  We usually suppose that $L$ is large and just write $H : \hash{n}$.
\end{defn}

In a more general context, a hash function is an efficiently computable function.

\begin{example}
  Let $H : \bits{\leq4} \to \bits{2}$
  be a hash function mapping a bitstring to its last two digits.
  For example, $H(1101) = 01$.

  We call 1001 a \term{preimage} of 01.

  The pair $(01, 1001)$ is a \term{collision}
  where 01 is a \term{second preimage} of 1001.
\end{example}

Generically, we can create a hash function given a block cipher.

\begin{scheme}[Davies--Meyer hash function]
  Let $E_k$ be an $m$-bit block cipher with $n$-bit key $k$.
  Let $\vv{IV}$ be a fixed $m$-bit initializing value.

  Then, to compute $H(x)$,
  \begin{enumerate}[noitemsep]
    \item Break up $x \concat 1$ into $n$-bit blocks $\bar x = x_1,\dotsc,x_t$
          (padding $x_t$ with 0s if necessary)
    \item $H_0 \gets \vv{IV}$
    \item $H_i \gets E_{x_i}(H_{i-1}) \xor H_{i-1}$ for all $i=1,\dotsc,t$
    \item $H(x) \gets H_t$
  \end{enumerate}
\end{scheme}

Hash functions are used basically everywhere in cryptography,
mostly just because they are stupidly fast and introduce ``scrambling'' that,
given a good enough hash function, cannot be reversed.

\begin{defn}[preimage resistance]
  A hash function $H = \hash{n}$ is \term*{preimage resistant} (PR) if,
  given a hash value $y \randin \bits{n}$, it is computationally infeasible
  to find any $x \in \bits{*}$ with $H(x) = y$ with non-negligible success probability.
\end{defn}

Note that we include disclaimers like ``non-negligible success probability''
since otherwise we could just use an attack like ``guess! it might just work!''

This is helpful for implementing passwords.
If we store $(\vv{password}, H(\vv{password}))$ with a PR hash $H$,
then stealing the system password file does not actually reveal the passwords.

\begin{defn}[2\nd preimage resistance]
  A hash function $H = \hash{n}$ is \term*{2\nd preimage resistant} (2PR) if,
  given $x \randin \bits{*}$, it is computationally infeasible to find
  any $x' \in \bits{*}$ with $x' \neq x$ and $H(x') = H(x)$ with non-negligible success probability.
\end{defn}

This is helpful for ensuring that a message is unchanged
(Modification Detection Codes; MDCs).
To ensure a message $m$ is unmodified, publicize $H(m)$.
Then, as long as $H$ is 2PR and we can verify the hash,
we can safely assume $m$ is unmodified.

\begin{defn}[collision resistance]
  A hash function $H = \hash{n}$ is \term*{collision resistant} (CR)
  if it is computationally infeasible to find distinct $x,x' \in \bits{*}$
  where $H(x') = H(x)$.
\end{defn}

This allows us to optimize message signing.
Instead of signing a large file $x$, Alice can sign $H(x)$ instead.
Keeping all the desired properties of a signing scheme
requires PR, 2PR, and CR.

\textrule{$\uparrow$ Lectures 5, 6, and 7 taken directly from slides $\uparrow$}
\lecture{Jan 24}

\begin{prop}
  If $H$ is CR, then $H$ is 2PR.
\end{prop}
\begin{prf}
  Take the contrapositive: $H$ is not 2PR $\implies$ $H$ is not CR.

  Suppose $H$ is not 2PR, i.e., we have an efficient algorithm
  to find a collision $x'$ given $x$.

  Select a random $x$. Get the collision $x'$ from our algorithm.
  Then, we have a collision $(x,x')$ that we found efficiently,
  so $H$ is not CR.
\end{prf}

It will \emph{always} be easier to do the contrapositive in this course,
especially because most definitions use ``it is not possible''.

\begin{prop}
  CR does not guarantee PR.
\end{prop}
\begin{prf}
  We give a counterexample.

  Suppose that $H : \bits{*} \to \bits{n}$ is CR.

  Consider the hash function $\bar H = \bits{*} \to \bits{n+1}$ defined by
  \[
    \bar H(x) = \begin{cases}
      0 \concat H(x) & x \not\in \bits{n} \\
      1 \concat x    & x \in \bits{n}
    \end{cases}
  \]
  where $\concat$ denotes the concatenation operation. Then $\bar H$ is CR because $H$ is.

  However, $\bar H$ is not PR for at least half of all $y \in \bits{n+1}$
  we can efficiently find the preimage (i.e., for all the hash values beginning with 1,
  we can just lop off the 1 to get the original).
\end{prf}

Note: if we disallow pathological hash functions like this, i.e.,
we have some constraint on uniformity in the size of preimages,
CR does guarantee PR.

\begin{prop}
  Suppose $H$ is \term{somewhat uniform}, i.e., preimages are all around the same size.
  If $H$ is CR, then $H$ is PR.
\end{prop}
\begin{prf}
  Suppose that $H : \bits{*} \to \bits{n}$ is not PR.
  We must show $H$ is not CR.

  Select $x \randin \bits{*}$ and compute $y = H(x)$.
  Since $H$ is not PR, we can efficiently find $x' \in \bits{*}$ with $H(x') = y$.
  Since $H$ is somewhat uniform, we expect that $y$ has many preimages,
  so $x' \neq x$ with very high probability.

  Therefore, $(x,x')$ is a collision and $H$ is not CR.
\end{prf}

\begin{prop}
  PR does not guarantee 2PR.
\end{prop}
\begin{prf}
  Suppose that $H : \bits{*} \to \bits{n}$ is PR.

  Define $\bar H : \bits{*} \to \bits{n}$ by $\bar H(x_1x_2\dots x_t) = H(0x_2\dots x_t)$.

  Then, $\bar H$ is PR but not 2PR.
\end{prf}

\begin{prop}
  Suppose $H$ is somewhat uniform. If $H$ is 2PR, then $H$ is PR.
\end{prop}
\begin{prf}
  Suppose that $H : \bits{*} \to \bits{n}$ is not PR and show it is not 2PR.

  Suppose we are given $x \bits{*}$.
  Compute $y = H(x)$ and then find with our PR-breaking algorithm $x'$ with $H(x') = y$.
  Since $H$ is somewhat uniform, we expect $x \neq x'$.

  Then, we have a collision $x'$ for $x$ and that breaks PR.
\end{prf}

\begin{prop}
  2PR does not guarantee CR.
\end{prop}
\begin{prf}
  Suppose that $H : \bits{*} \to \bits{n}$ is 2PR.

  Consider $\bar H : \bits{*} \to \bits{n}$ defined by $\bar H(x) = H(x)$
  except $\bar H(1) = H(0)$.

  Then, $\bar H$ is not CR because $(0,1)$ is a collision.

  However, we can show $\bar H$ is 2PR. Suppose $\bar H$ is \emph{not} 2PR.
  We show $H$ would also not be 2PR.

  Suppose we are given some $x$.
  Since $\bar H$ is not 2PR, we can find $x' \neq x$ with $\bar H(x') = \bar H(x)$.
  With almost certain probability, we can assume $x \neq 0,1$.
  Then, $\bar H(x) = H(x)$.
  If $x' \neq 1$, we have $\bar H(x') = H(x') = H(x)$.
  Otherwise, $\bar H(x') = \bar H(1) = H(0) = H(x)$
  We found a second preimage $x'$ or 0 for $x$, so $H$ is not 2PR.

  Therefore, by contradiction, $\bar H$ is 2PR.
\end{prf}

\begin{theorem}[relation between PR, 2PR, CR]\label{thm:prcr}
  Summarize:
  \begin{center}
    \begin{tabular}{r|ccc}
      If $H$ is $\downarrow$, then it is $\rightarrow$ & PR        & 2PR         & CR          \\ \hline
      PR                                               & --        & $\not\Rarr$ & $\not\Rarr$ \\
      2PR                                              & $\Rarr^*$ & --          & $\not\Rarr$ \\
      CR                                               & $\Rarr$   & $\Rarr^*$   & --
    \end{tabular}
  \end{center}
  where $\Rarr^*$ means ``implies under somewhat uniformity''
\end{theorem}

\section{Attacks}
\lecture{Jan 26}

\begin{defn}[generic attack]
  An attack which does not exploit any specific properties of a hash function.
  That is, it works on any generic hash function $H : \bits{*} \to \bits{n}$.
\end{defn}

To analyze a generic attack, we assume $H$ is a random function
in the sense that $y = H(x)$ can be treated as $y \randin \bits{n}$.

\begin{attack}[generic attack for preimages]
  Given $y \randin\bits{n}$, repeatedly select arbitrary $x$ until $H(x) = y$.
\end{attack}

This will take $2^n$ attempts, so as long as $n \geq 128$ we are safe.

\begin{attack}[generic attack for collisions]
  Select arbitrary $x \in \bits{*}$ and store $(H(x),x)$ in a table
  sorted by the first entry. Repeat until a collision is found.
\end{attack}

By the birthday paradox, the expected number of hash operations is
$\sqrt{\pi 2^n/2} \approx \sqrt{2^n}$.
Therefore, the attack is infeasible for $n \geq 256$.

The space complexity is also $\order{\sqrt{2^n}}$.
This is important since, for example, $n=128$ has a feasible runtime $2^{64}$
but an infeasible space requirement of 500 million TB.

We can prove that this is the optimal generic collision attack, i.e.,
no faster generic attack exists.

However, we can improve the space complexity.

Let $N = 2^n$.
Define a sequence $(x_i)_{i\geq0}$ by $x_0 \randin \bits{n}$
and $x_i = H(x_{i-1})$.

Since $(x_i) \subseteq \bits{n}$, we will eventually get repetitions.
Therefore, $(x_i)$ is eventually periodic, i.e.,
we will eventually get $x_a = x_b$ for $a \neq b$.
Then, we found a collision $(x_{a-1}, x_{b-1})$.

More formally, let $j$ be the smallest index for which $x_j = x_i$
for some $i < j$, which must exist.
Then, $x_{j+\ell} = x_{i+\ell}$ for all $\ell \geq 1$.

By the birthday paradox, $\E[j] \approx \sqrt{\pi N/2} \approx \sqrt{N}$.
In fact, since $i$ is a random element from before $j$,
we can say $\E[i] \approx \frac12\sqrt{N}$ and $\E[j-i] \approx \frac12\sqrt{N}$.

We will store only some \term*{distinguished points},
for example, elements where the top 32 bits are all 0.
Let $\theta$ be the proportion of distinguished points.
Here, $\theta = 2^{-32}$.

We can still tell detect a cycle as long as there is a distinguished point in the cycle.
Once we detect a collision, we work through the sequence near it.

\begin{attack}[van Oorschot--Wiener parallel collision search]
  We write this attack as two stages:
  \begin{algorithmic}[1]
    \State Create a table $T$
    \Procedure{DetectCollision}{$H$}
      \State Select $x_0 \randin \bits{n}$
      \State $T[x_0] \gets (0,-)$ \Comment{store (index, last distinguished point)}
      \State $c \gets 0$ \Comment{last distinguished point}
      \For{$d = 1,2,3\dotsc$}
        \State $x_d \gets H(x_{d-1})$
        \If{$x_d$ is distinguished}
          \If{$T[x_d]$ exists as $(b,x_a)$}
            \State $(a,-) \gets T[x_a]$ \Comment{need the index of $x_a$}
            \State \Return{\Call{FindCollision}{$x_a,x_c,a,b,c,d$}}
          \EndIf
          \State $T[x_d] \gets (d,c)$
          \State $c \gets d$
        \EndIf
      \EndFor
    \EndProcedure
    \Procedure{FindCollision}{$x_a,x_c,a,b,c,d$}
      \State $\ell_1 \gets b-a$, $\ell_2 \gets d-c$
      \State Suppose $\ell_1 \geq \ell_2$ so $k \gets \ell_1 - \ell_2$
      \State Compute $x_{a+1},\dotsc,x_{a+k}$
      \State $m \gets 1$
      \Repeat
        \State Compute $x_{a+k+m}, x_{c+m}$
        \State $m \gets m + 1$
      \Until{$x_{a+k+m} = x_{c+m}$}
      \State \Return{$(x_{a+k+m-1}, x_{c+m-1})$} \Comment{the collision is $H(x_{a+k+m-1}) = H(x_{c+m-1})$}
    \EndProcedure
  \end{algorithmic}
\end{attack}

In \Call{DetectCollision}{}, we will call the hash function
$\sqrt{\pi N/2} + \frac{1}{\theta}$ times.

In \Call{FindCollision}{}, we perform at most $\frac{3}{\theta}$ hashes.

In total, we expect to take $\sqrt{N}+\frac{4}{\theta}$ time.
But this time we only need $3n\theta\sqrt{N}$ space.

So for our $n=128$ case with $\theta=2^{-32}$,
the expected runtime is $2^{64}$ hashes (feasible)
and the expected storage is 192 GB (negigible).

\lecture{Jan 29}
We can parallelize VW collision search by having each processor
start on a random point and report discovered distinguished points to a central server.

For $m$ processors, we get an expected time of $\frac{1}{m}\sqrt{N}+\frac{4}{\theta}$ hashes
and space of $3n\theta\sqrt{N}$ bits.
That is, we get a speedup of $m$ times.

This is also nice because there is no communication between processors
and only occasional communication with the central server
(reducing the chance of race conditions and other parallelism problems).

\section{Iterated hash functions}

\begin{scheme}[Merkle's meta method]
  Fix an initializing value $\vv{IV} \in \bits{n}$
  and pick a compression function $f : \bits{n+r} \to \bits{n}$.

  Given a $b$-bit message $x$, to compute $H(x)$:
  \begin{enumerate}[nosep]
    \item Break up $x$ into $r$-bit blocks $\bar x = x_1,\dotsc,x_t$ (padding the last block with 0s if necessary)
    \item Define $x_{t+1}$ to hold the binary representation of $b$ (left-padding with 0s as necessary)
    \item Define $H_0 = \vv{IV}$
    \item Compute $H_i = f(H_{i-1} \concat x_i)$ for $i = 1,\dotsc,t+1$
    \item Return $H(x) = H_{t+1}$
  \end{enumerate}
\end{scheme}

Merkle also proved that collision resistance depends on $f$.

\begin{theorem}[Merkle]
  If the compression function $f$ is collision resistant,
  then the iterated hash function $H$ is also collision resistant.
\end{theorem}

Note that by \cref{thm:prcr}, we get PR and 2PR as well.

This feels very circular, but it can be helpful to give a proof of security
given certain very precise definitions.
However, the assumptions in the definitions might not be realistic.

\begin{prf}
  Suppose that $H$ is not CR. We will show that $f$ is not CR.

  Since $H$ is ont CR, we can efficiently find messages $x,x' \in \bits*$
  with $x \neq x'$ and $H(x) = H(x')$.

  Define $\bar x = x_1,\dotsc,x_t$, $b = \abs{x}$, length block $x_{t+1}$,
  and $\bar x' = x'_1,\dotsc,x'_{t'}$, $b' = \abs{x'}$, length block $x'_{t'+1}$.

  Then, we can efficiently compute
  \begin{align*}
    H_0        & = \vv{IV}            & H_0             & = \vv{IV}                \\
    H_1        & = f(H_0,x_1)         & H'_1            & = f(H_0,x'_1)            \\
    H_2        & = f(H_1,x_2)         & H'_2            & = f(H'_1,x'_2)           \\
    H_3        & = f(H_2,x_1)         & H'_3            & = f(H'_2,x'_3)           \\
               & \vdotswithin{=}      &                 & \vdotswithin{=}          \\
    H_{t-1}    & = f(H_{t-2},x_{t-1}) & H'_{t'-1}       & = f(H'_{t'-2},x'_{t'-1}) \\
    H(x) = H_t & = f(H_{t-1},x_t)     & H(x') = H'_{t'} & = f(H'_{t'-1},x'_{t'})
  \end{align*}
  Since $H(x) = H(x')$, we have $H_t = H'_{t'}$.

  Now, if $b \neq b'$, then $x_{t+1} \neq x'_{t'+1}$.
  Then, $(H_t \concat x_{t+1}, H'_{t'} \concat x'_{t'+1})$ is a collision for $f$.

  Otherwise, if $b = b'$, then $t = t'$ and $x_{t+1} = x'_{t+1}$.
  Let $i$ be the largest index for which $(H_i \concat x_{i+1}) \neq (H'_i \concat x'_{i+1})$
  which must exist because $x \neq x'$.

  Then, $H_{i+1} = f(H_i,x_{i+1}) = f(H'_i,x'_{i+1}) = H'_{i+1}$
  and we have a collision $(H_i\concat x_{i+1}, H'_i \concat x'_{i+1})$.

  Since we found a collision, $f$ is not CR.
\end{prf}

\paragraph{Aside: MD5 is bad}
MDx is a family of iterated hash functions.
MD4 was designed by Rivest in 1990 with a security level against VW of 64 bits
but broken \emph{by hand} by Wang in 2004 with an attack reducing the security level to 4 bits.
MD4 preimages can also be found in $2^{102}$ operations, which is infeasible but also still bad.

In 1991, Rivest designed MD5, a strengthened version of MD4.
The Wang attack reduced the security level of 39 bits,
but modern attacks can find collisions in $2^{24}$ operations.

In summary, MD5 should not be used if collision resistance is required
but it's \emph{probably} preimage resistant.
In fact, the Flame malware used a forged MD5-based Microsoft certificate
created using an improved version of Wang's attack.

\lecture{Jan 31}
For another example of why this is a problem,
consider the fact that Crowdmark uses MD5 hashes to verify
that a submitted file is the same (in case of an upload error).
A student could change their answer and submit the ``same''
file after looking at the solutions after the deadline.

The SHA family of functions are designed by the NSA.
Wang (our recurring character) attacked SHA (1993) to 39 bits and SHA-1 (1994) to 63 bits.
The SHA-2 family, a variable-length output version of SHA-1, has no known weaknesses.
The 224-, 256-, 384-, and 512-bit lengths are chosen so that the security levels
(against VW collision finding) line up with the security levels of
Triple-DES, AES-128, AES-192, and AES-256.

\begin{scheme}[SHA-256]
  SHA-256 is an iterated hash function with block length $r=512$,
  hash length $n = 256$, and compression function $f : \bits{256+512}\to\bits{256}$.

  The design principles are classified, so we can treat it as a black box.

  Define $h_1,\dotsc,h_8$ as the fractional parts of the
  square roots of the first eight primes
  and $y_0,\dotsc,y_{63}$ as the fractional parts of the
  cube roots of the first 64 primes.
  Finally, let
  \begin{align*}
    f(A,B,C) & = AB \xor \overline{BC}                                                         & g(A,B,C) & = AB \xor AC \xor BC                                                            \\
    r_1(A)   & = (A \hookrightarrow 2) \xor (A \hookrightarrow 13) \xor (A \hookrightarrow 22) & r_2(A)   & = (A \hookrightarrow 6) \xor (A \hookrightarrow 11) \xor (A \hookrightarrow 25) \\
    r_3(A)   & = (A \hookrightarrow 7) \xor (A \hookrightarrow 18) \xor (A \gg 3)              & r_4(A)   & = (A \hookrightarrow 17) \xor (A \hookrightarrow 19) \xor (A \gg 10)
  \end{align*}
  To find the hash of a $b$-bit message $x$ made of 32-bit words $x_0,x_1\dotsc$:
  \begin{algorithmic}[1]
    \State pad $x$ with 1 followed by 0s until the bitlength is $-64 \pmod{512}$.
    \State append a 64-bit representation of $b \pmod{2^{64}}$
    \State initialize $(H_1,\dotsc,H_8) \gets (h_1,\dotsc,h_8)$
    \For{$i = 0,\dotsc,m-1$}
      \State $X_j \gets x_{16i+j}$ \textbf{for} $0 \leq j \leq 15$
      \Comment{copy $i$\xth 16-word block into temp storage}
      \State $X_j \gets r_4(X_{j-2}) + X_{j-7} + r_3(X_{j-15}) + X_{j-16}$ \textbf{for} $16 \leq j \leq 63$
      \Comment{expand $X$ to 64 words}
      \State $(A,B,\dotsc,G,H) \gets (H_1,H_2,\dotsc,H_7,H_8)$
      \Comment{initialize working variables}
      \For{$j = 0,\dotsc,63$}
        \Comment{weird random shuffling}
        \State $T_1 \gets H + r_2(E) + f(E,F,G) + y_j + X_j$
        \State $T_2 \gets r_1(A) + g(A,B,C)$
        \State $(H,G,F,E,D,C,B,A) \gets (F,G,E,D+T_1,C,B,A,T_1+T_2)$
      \EndFor
      \State $(H_1,\dotsc,H_8) \gets (H_1 + A,\dotsc,H_8 + H)$
      \Comment{update working variables}
    \EndFor
    \State \Return{$H_1 \concat H_2 \concat \dotsb \concat H_8$}
  \end{algorithmic}
\end{scheme}

It would be very profitable to crack SHA-256 since it is used to
verify proof of work for Bitcoin mining.
Finding messages with arbitrary numbers of 0s at the starts of hashes would print money.

Just to be sure Wang can't come back and break SHA-2 (since it is still a Merkle design).
SHA-3 (Keecak, based on ``sponge construction'') was selected in 2012,
but nobody really uses it because SHA-256 is still better.

\chapter{Message authentication codes}
\lecture{Feb 2}

\section{Definitions}

\begin{defn}[message authentication code]
  A family of functions $\MAC_k : \hash{n}$ paramaterized by an $\ell$-bit key $k$
  where each function $\MAC_k$ can be efficiently computed.

  The \term{MAC} or \term{tag} of a message $x$ is denoted $t = \MAC_k(x)$.
\end{defn}

We use MAC schemes to provide data integrity and origin verification.
To do this:
\begin{enumerate}[nosep]
  \item Alice and Bob establish a secret key $k \in \bits{\ell}$.
  \item Alice computes the tag $t = \MAC_k(x)$ of a message $x$ and sends $(x,t)$ to Bob.
  \item Bob verifies that $t = \MAC_k(x)$.
\end{enumerate}
To avoid a \term[attack!replay]{replay attack}
(Eve saves a copy of a message and resends it later),
add a timestamp or sequence number.

Like with encryption, we have to formulate a security definition.

\begin{defn*}[MAC security]
  A MAC scheme is \term[message authentication code!security]{secure}
  if it is \term{existentially unforgeable} under a chosen-message attack.

  That is, for chosen messages $x_i$ and their MACs $t_i$,
  it is computationally infeasible to find with non-negligible success probability
  a valid message-MAC pair $(x,t)$ for a new message $x$.
\end{defn*}

Realistically, the messages $x_i$ will have to be ``harmless'' messages
that Alice is ordinarily willing to tag
and the forgery $x$ is a ``harmful'' message that Alice would ordinarily
be unwilling to tag.

\begin{defn}
  An \term[message authentication code!ideal]{ideal} MAC scheme
  is one where for each key $k \in \bits{\ell}$,
  the function $\MAC_k : \hash{n}$ is a random function.
\end{defn}

The naive generic attack is to just guess.

\begin{attack}[generic attack for tags]
  Select $y \randin \bits{n}$ and guess that $\MAC_k(x) = y$.
  Keep guessing.
\end{attack}

Assuming an ideal scheme, the success probability is $\frac{1}{2^n}$.

\begin{attack}[generic attack for keys]
  Perform the same attack as on an SKES.
\end{attack}

Assuming an ideal scheme, the expected number of keys for which $r$ messages
verify is $1+\vv{FK} = 1+(2^\ell -1)/2^r$.
If $\vv{FK}$ is negligible, the expected number of operations is $2^{\ell-1}$.

\section{Specific MACs}

\begin{scheme}[CBC-MAC]
  Let $E$ be an $n$-bit block cipher with key space $\bits{\ell}$.
  We assume that plaintext messages all have lengths that are multiples of $n$.
  To compute $\CBCMAC_k(x)$:
  \begin{algorithmic}[1]
    \State Divide $x$ into $n$-bit blocks $x_1,\dotsc,x_r$.
    \State $H_1 \gets E_k(x_1)$
    \For{$i = 2,\dotsc,r$}
      \State $H_i \gets E_k(H_{i-1} \xor x_i)$
    \EndFor
    \State \Return{$H_r$}
  \end{algorithmic}
\end{scheme}

It was proven in 1994 that CBC-MAC with fixed-length messages is secure
if $E$ is ideal (i.e., $E_k : \bits{n} \to \bits{n}$ is random).

However, it is totally broken for variable-length messages.
\begin{prf}
  Select an arbitrary 3-block message $x = (x_1,x_2,x_3)$.
  Obtain $t_1 = \CBCMAC_k(x_1) = E_k(x_1)$.
  Obtain $t_2 = \CBCMAC_k((t_1 \xor t_2) \concat x_3) = E_k(E_k(t_1 \xor x_2) \xor x_3)$.
  Then, $(x,t_2)$ is a forgery.
\end{prf}

One way to fix this is to add one more encryption round.

\begin{scheme}[Encrypted CBC-MAC (EMAC)]
  Given a second key $s$ for $E$,
  let $\operatorname{EMAC}_{k,s}(x) = E_s(\CBCMAC_k(x))$.
\end{scheme}

Again, it has been proven that EMAC is secure if $E$ is ideal.

Now, consider creating a MAC based on a hash function.
Let $H$ be an iterated $n$-bit hash function
with compression function $f : \bits{n+r} \to \bits{n}$.
Pick $k \randin \bits{n}$ and let $K \in \bits{r}$ be $k$ padded with 0s.

We might propose $\MAC_k(x) = H(K \concat x)$.
However, this is insecure under a \term[attack!length extension]{length extension attack}
since if we know $(x,\MAC_k(x))$,
we can calculate $\MAC_k(x \concat y)$ for arbitrary $y$
by resuming the hash process with the last block of $x$
(i.e., computing $f(t \concat y_1)$, etc.).

\begin{xca}
  Show that this is also insecure if messages are of arbitrary length
  and a length block is added to $K \concat x$.
\end{xca}

Securing against this attack is as simple as adding the key to each step,
giving us a secure hash-based MAC (``HMAC'')

\begin{scheme}[HMAC]
  Let $H$ be a hash function.
  Define $r$-bit constants $\vv{opad} = \x{3636\cdots 36}$ and $\vv{ipad} = \x{5c5c\cdots 5c}$.

  Then, $\HMAC_k(x) = H(K \oplus \vv{opad}, H(K \oplus \vv{ipad}, x))$.
\end{scheme}

\lecture{Feb 5}
The security analysis of HMAC is hard, but we can prove a theorem.

\begin{theorem}
  Suppose that the $f$ compression function used in $H$ is a secure MAC
  with fixed-length messages and a secret IV as the key.
  Then, HMAC is a secure MAC scheme.
\end{theorem}

Usually, HMAC uses SHA-256. It is widely used for internet security.

HMAC is also widely used as a \term{key derivation function}.
If Alice has $k$ and needs multiple session keys $sk_i$,
she can compute $sk_1 = \HMAC_k(1)$, $sk_2 = \HMAC_k(2)$, etc.
Then, since HMAC is secure, Eve can intercept individual session keys
and still know nothing about either the actual key $k$ or the other session keys.

\section{GSM}

TODO

\chapter{Authenticated encryption}

\section{Generic schemes}
We have established ways to encrypt data (e.g., AES-CBC)
and ways to authenticate data (i.e., MACs)
but what if we need both?

\begin{scheme}[Encrypt-and-MAC]
  Alice sends $(c,t) = (E_{k_1}(m), \MAC_{k_2}(m))$ to Bob,
  where $m$ is the plaintext and $(k_1,k_2)$ is a secret key shared with Bob.

  Then, Bob first decrypts $c$ to obtain $m = E_{k_1}^{-1}(c)$
  and then verifies $t = \MAC_{k_2}(m)$.
\end{scheme}

This is insecure because $\MAC_{k_2}(m)$ might leak information about $m$
(MACs are generally not semantically secure).

\begin{scheme}[Encrypt-then-MAC]
  Alice sends $(c,t) = (E_{k_1}(m), \MAC_{k_2}(c))$ to Bob,
  where $m$ is the plaintext and $(k_1,k_2)$ is a secret key shared with Bob.

  Then, Bob first verifies that $t = \MAC_{k_2}(c)$ and decrypts $c$ to obtain
  $m = E_{k_1}^{-1}(c)$.
\end{scheme}

This has been proven to be secure given that $E_{k_1}$ and $\MAC_{k_2}$
are secure.

\begin{defn*}[authentication encryption security]
  An authentication encryption scheme is
  \term[authentication encryption scheme!security]{secure} if
  \begin{enumerate}[nosep]
    \item it is semantically secure against chosen-plaintext attacks, and
    \item has \term{ciphertext integrity}, i.e., given $(m_1,c_1,t_1),\dotsc,(m_\ell,c_\ell,t_\ell)$,
          an attacker cannot forge a valid $(m,c,t)$.
  \end{enumerate}
\end{defn*}

\lecture{Feb 7}
\section{AES-GCM}
The most popular AE scheme is AES-GCM which uses AES-CTR and GMAC.

Recall how ChaCha20 (\cref{s:chacha20})
used a key, nonce, and counter to generate a keystream.
AES-CTR uses AES in a similar stream cipher paradigm.

\begin{scheme}[AES-CTR]
  Let $k \randin \bits{128}$ be a shared secret
  and $M = (M_1,\dotsc,M_u)$ be a message of 128-bit blocks.
  To encrypt:
  \begin{algorithmic}[1]
    \State Select a nonce $\vv{IV} \in \bits{96}$,
    \State $J_0 \gets \vv{IV} \concat 0^{31} \concat 1$,
    \For{$i = 1,\dotsc,u$}
      \State $J_i \gets J_{i-1} + 1$
      \State $C_i \gets \AES_k(J_i) \xor M_i$
    \EndFor
    \State \Return{$(\vv{IV}, C_1, \dotsc, C_u)$}
  \end{algorithmic}
  To decrypt, generate the keystream and XOR.
\end{scheme}

Since it is a counter, CTR encryption is parallelizable.
We also require, as with other IV-containing schemes, that the IV be unique
and never reused.

To define GMAC, recall \cref{def:gf28}.

\begin{defn*}[$\GF(2^{128})$]
  The field $\Z/2\Z[x]$ modulo $f(x) = 1 + x + x^2 + x^7 + x^{128}$.

  We associate a 128-bit block $a = a_0\cdots a_{127}$
  with the polynomial $a(x) = a_0 + a_1x + \dotsb + a_{127}x^{127}$.

  As in $\GF(2^8)$, we define $a \vdot b = a(x)\cdot b(x) \pmod{f(x)}$.
\end{defn*}

Then, we can describe GMAC:

\begin{scheme}[Galois Message Authentication Code (GMAC)]
  Let $A = (A_1,\dotsc,A_v)$ be the message in 128-bit blocks,
  $L$ be the bitlength of $A$ as a 128-bit block,
  and $k$ be a secret key.
  \begin{algorithmic}[1]
    \State $J_0 \gets \vv{IV} \concat 0^{31} \concat 1$
    \State $H \gets \AES_k(0^{128})$
    \State Define $f_A(y) = A_1 y^{v+1} + A_2y^v + \dotsb + A_{v-1}y^3 + A_v y^2 + Ly \in \GF(2^{128})[y]$
    \State $t \gets \AES_k(J_0) \oplus f_A(H)$
    \State \Return{$(\vv{IV}, A, t)$}
  \end{algorithmic}
\end{scheme}

\begin{prop}
  GMAC is secure.
\end{prop}
\begin{prf}[outline]
  Consider the simplified tag $t' = f_A(H)$.

  Then, the adversary can guess with probability $\frac{1}{2^{128}}$.

  She can also guess $t'$ by making a guess $H'$ and computing $f_A(H')$
  with success probability $\frac{v+1}{2^{128}}$.

  But if Eve sees a single valid message-tag pair $(A,t')$,
  she can solve $f_A(H) = t'$ for $H$.

  To avoid this attack, we add a one-time pad $\AES_k(J_0) \oplus t$.
\end{prf}

Finally, we can define AES-GCM.
The scheme will encrypt/authenticate a message
and also authenticate (but not encrypt) an \term{encryption context}.

\begin{scheme}[AES-GCM]
  Let $M = (M_1,\dotsc,M_u)$ for $u \leq 2^{32}-2$ be the message,
  $A = (A_1,\dotsc,A_v)$ be the encryption context,
  and $k \randin \bits{128}$ be a shared secret.

  To encrypt/sign:
  \begin{algorithmic}[1]
    \State $L \gets \abs{A} \concat \abs{M}$
    \State $J_0 \gets \vv{IV} \concat 0^{31} \concat 1$ for unique $\vv{IV} \in \bits{96}$
    \For{$i = 1,\dotsc,u$} \Comment{Encryption with AES-CTR}
      \State $J_i \gets J_{i-1} + 1$
      \State $C_i \gets \AES_k(J_i) \xor M_i$
    \EndFor
    \State $H \gets \AES_k(0^{128})$ \Comment{Authentication with GMAC}
    \State Define $f_{A,C}(x) = A_1x^{u+v+2} + \dotsb + A_v x^{u+2} + C_1 x^{u+1} + \dotsb + C_u x^2 + Lx$
    \State $t \gets \AES_k(J_0) \xor f_{A,C}(H)$
    \State \Return{$(\vv{IV},A,C,t)$}
  \end{algorithmic}
  To decrypt/authenticate:
  \begin{algorithmic}[1]
    \State
  \end{algorithmic}
\end{scheme}

AES-GCM does both authentication and encryption,
but it can be used to do just authentication by passing $M = \varepsilon$.

There are very fast hardware implementations of AES and multiplication under $\GF(2^{128})$.
If we use Horner's rule, we can evaluate an $n$-degree polynomial in $\GF(2^{128})[x]$
with $n$ PCLMUL instructions and $n-1$ XOR instructions.

We can also parallelize since we can assign blocks to other processors
and split up the polynomial $f_{A,C}$ into multiple parts for evaluation.

The scheme can be used in streaming mode, which is helpful.

This is also proven to be secure.

\pagebreak
\phantomsection\addcontentsline{toc}{chapter}{Back Matter}
\renewcommand{\listtheoremname}{List of Named Results}
\phantomsection\addcontentsline{toc}{section}{\listtheoremname}
\listoftheorems[ignoreall,numwidth=3em,onlynamed={theorem,lemma,corollary,prop}]
\renewcommand{\listtheoremname}{List of Cryptoschemes and Attacks}
\phantomsection\addcontentsline{toc}{section}{\listtheoremname}
\listoftheorems[ignoreall,onlynamed={scheme,attack}]
\printindex


\end{document}
