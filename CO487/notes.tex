\documentclass[class=co487,tikz,minted,notes]{agony}
\declaretheoremstyle[
	headfont=\bfseries\color{MidnightBlue},
	mdframed={style=mdround,linecolor=RoyalBlue,backgroundcolor=CornflowerBlue!5},
	headpunct={\\[3pt]},
	postheadspace={0pt}
]{thmroundblue}
\declaretheorem[name=Cryptoscheme,refname={scheme,schemes},style=thmroundblue,parent=chapter]{scheme}
\declaretheorem[name=Attack,refname={attack,attacks},style=thmroundpink,sibling=scheme]{attack}

\title{CO 487 Winter 2024: Lecture Notes}
\begin{document}
\renewcommand{\contentsname}{CO 487 Winter 2024:\\{\huge Lecture Notes}}
\thispagestyle{firstpage}
\tableofcontents

Lecture notes taken, unless otherwise specified,
by myself during the Winter 2024 offering of CO 487,
taught by Alfred Menezes.

\begin{multicols}{2}
  \listoflecture
\end{multicols}

\chapter{Introduction}
\lecture{Jan 8}

Cryptography is securing communications in the presence of malicious adversaries.
To simplify, consider Alice and Bob communicating with the eavesdropper Eve.
Communications should be:
\begin{itemize}[noitemsep]
  \item Confidential: Only authorized people can read it
  \item Integral: Ensured that it is unmodified
  \item Origin authenticated: Ensured that the source is in fact Alice
  \item Non-repudiated: Unable to gaslight the message existing
\end{itemize}
Examples: TLS for intenet browsing, GSM for cell phone communications,
Bluetooth for other wireless devices.

\paragraph{Overview: Transport Layer Security} The protocol used by browsers
to visit websites.
TLS assures an individual user (a \term{client})
of the authenticity of the website (a \term{server})
and to establish a secure communications \term{session}.

TLS uses \term{symmetric-key cryptography}.
Both the client and server have a shared secret $k$ called a \term{key}.
They can then use AES for encryption and HMAC for authentication.

To establish the shared secret, use \term{public-key cryptography}.
Alice can encrypt the session key $k$ can be encrypted with Bob's RSA public key.
Then, Bob can decrypt it with his private key.

To ensure Alice is getting an authentic copy of Bob's public key,
a \term{certification authority} (CA) signs it using the CA's private key.
The CA public key comes with Alice's device preinstalled.

Potential vulnerabilities when using TLS:
\begin{itemize}[noitemsep]
  \item Weak cryptography scheme or vulnerable to quantum computing
  \item Weak random number generation for the session key
  \item Fraudulent certificates
  \item Implementation bugs
  \item Phishing attacks
  \item Transmission is secured, but the endpoints are not
\end{itemize}
These are mostly the purview of cybersecurity,
of which cryptography is a part.
Cryptography is not typically the weakest link in the cybersecurity chain.

\chapter{Symmetric key encryption}
\lecture{Jan 10}

\section{Basic concepts}

\begin{defn}[symmetric-key encryption scheme]
  A \term*{symmetric-key encryption scheme} (SKES) consists of:
  \begin{itemize}[nosep]
    \item plaintext space $M$,
    \item ciphertext space $C$,
    \item key space $K$,
    \item family of encryption functions $E_k : M \to C$ for all keys $k \in K$, and
    \item family of decryption functions $D_k : C \to M$ for all keys $k \in K$
  \end{itemize}
  such that $D_k(E_k(m)) = m$ for all $m$ and $k$.
\end{defn}

For Alice to send a message to Bob:
\begin{enumerate}[1.,nosep]
  \item Alice and Bob agree on a secret key $k$ \emph{somehow} (assume a secured channel)
  \item Alice computes $c = E_k(m)$ and sends $c$ to Bob
  \item Bob recovers the plaintext by computing $m = D_k(c)$
\end{enumerate}

Examples include the Enigma and Lorenz machines.

\begin{scheme}[simple substitution cipher]
  Let:
  \begin{itemize}[nosep]
    \item $M$ be English messages
    \item $C$ be encrypted messages
    \item $K$ be permutations of the English alphabet
    \item $E_k(m)$ apply the permutation $k$ to $m$, one letter at a time
    \item $D_k(c)$ apply the inverse permutation $k^{-1}$ to $c$, one letter at a time
  \end{itemize}
\end{scheme}

We want a system to have:
\begin{enumerate}[nosep]
  \item Efficient algorithms should be known for computing (encryption and decryption)
  \item Small keys but large enough to render exhaustive key search infeasible
  \item Security
  \item Security against its designer
\end{enumerate}

To determine how secure the protocol is, we have to define security.

\begin{defn}[security model]
  Some parameters which define the strength of the adversary,
  specific interaction with the ``secure'' channel,
  and the goal of the adversary.
\end{defn}

Some options for strength:
\begin{itemize}[nosep]
  \item \term[security!information-theoretic]{Information-theoretic security}: Eve has infinite resources.
  \item \term[security!complexity-theoretic]{Complexity-theoretic security}: Eve is a polynomimal-time Turing machine.
  \item \term[security!computational-theoretic]{Computational-theoretic security}: Eve has a specific amount of computing power.
        In this course, Eve is \term{computationally bounded} by
        6,768 Intel E5-2683 V4 cores running at 2.1 GHz at her disposal.
\end{itemize}
For the interaction:
\begin{itemize}[nosep]
  \item \term[attack!ciphertext-only]{Ciphertext-only attack}: Eve only knows the ciphertext.
  \item \term[attack!known-plaintext]{Known-plaintext attack}: Eve knows some plaintext and the corresponding ciphertext.
  \item \term[attack!chosen-plaintext]{Chosen-plaintext attack}: Eve picks some plaintext and knows the corresponding ciphertext.
  \item \term[attack!clandestine]{Clanedestine attack}: Eve resorts to bribery, blackmail, etc.
  \item \term[attack!side-channel]{Side-channel attack}: Eve has physical access to hardware and has some monitoring data.
\end{itemize}
And for the goal:
\begin{itemize}[nosep]
  \item Recovering the secret key $k$
  \item Systematically decrypt arbitrary ciphertexts without knowing $k$ (\term[security!total]{total security})
  \item Learn partial information about the plaintext (other than the length) (\term[security!semantic]{semantic security})
\end{itemize}

\begin{defn*}[security]
  An SKES is \term[symmetric-key encryption scheme!security]{secure}
  if it is semantically secure against a chosen-plaintext attack
  by a computationally bounded adversary.
\end{defn*}

Equivalently, an SKES is \term{broken} if:
\begin{enumerate}[nosep]
  \item Given a challenge ciphertext $c$ for $m$ generated by Alice,
  \item \dots and access to an encryption oracle for Alice,
  \item \dots Eve can obtain some information about $m$ other than its length,
  \item \dots using only a feasible amount of computation.
\end{enumerate}
Note: this is IND--CPA from CO 485.

\begin{example}
  Is the simple substitution cipher secure?
  What about under a ciphertext-only attack?
\end{example}
\begin{sol}
  Under CPA, encrypt the entire alphabet.
  Then, the entire key $k$ is recovered.

  With a ciphertext-only attack, an exhaustive key search would take
  $26! \approx 2^{88}$ attempts.
  This would take over 1,000 years, which is pretty infeasible,
  so it is secure.
\end{sol}

Can we quantify how feasible something is?

\begin{defn}[security level]
  A scheme has a \term{security level} of $\ell$ bits if
  the fastest known attack on the scheme takes approximately $2^\ell$ operations.
\end{defn}

\begin{convention}
  In this course:
  \begin{itemize}[nosep]
    \item 40 bits is very easy to break
    \item 56 bits is easy to break
    \item 64 bits is feasible to break
    \item 80 bits is barely feasible to break
    \item 128 bits is infeasible to break
  \end{itemize}
\end{convention}

\lecture{Jan 12}
The simple substitution cipher can be attacked by frequency analysis,
since, for example, if ``e'' is the most common English letter,
we check the ciphertext for the most common letter and identify it with ``e''.

\begin{scheme}[Vigen√®re cipher]
  Let the key $K$ be an English word with no repeated letters, e.g., $K = \symrm{CRYPTO}$.

  To encrypt, add letter-wise the key modulo 26, where $k$ is $K$ repeated until
  it matches the length of the message:
  \begin{center}
    \begin{tabular}{rcccccccccccccc}
      $m =$        & t & h & i & s & i & s & a & m & e & s & s & a & g & e \\
      $+\quad k =$ & C & R & Y & P & T & O & C & R & Y & P & T & O & C & R \\ \hline
      $c =$        & V & Y & G & H & B & G & C & D & C & H & L & O & I & V
    \end{tabular}
  \end{center}
  To decrypt, just take $c - k$.
\end{scheme}

This solves our frequency analysis problem.
However, the Vigenere cipher is still totally insecure.

\begin{xca}
  Show that the Vigenere cipher is totally insecure under a chosen-plaintext attack
  and a ciphertext-only attack.
\end{xca}

\begin{scheme}[one-time pad]
  The key is a random string of letters with the same length as the message.

  Repeat the process for Vigenere.
  To encode, add each letter.
  To decode, subtract each letter.
\end{scheme}

\begin{example}
  We can encrypt as follows:
  \begin{center}
    \begin{tabular}{rcccccccccccccc}
      $m =$        & t & h & i & s & i & s & a & m & e & s & s & a & g & e \\
      $+\quad k =$ & Z & F & K & W & O & G & P & S & M & F & J & D & L & G \\ \hline
      $c =$        & S & M & S & P & W & Y & P & F & Q & X & C & D & R & K \\
    \end{tabular}
  \end{center}
\end{example}

This is semantically secure as long as the key is never reused.
Formally, there exist keys that can decrypt the ciphertext into \emph{anything},
so there is no way for an attacker to know the plaintext.
If it is reused, i.e., if $c_1 = m_1 + k$ and $c_2 = m_2 + k$,
then $c_1 - c_2 = (m_1 + k) - (m_2 + k) = m_1 - m_2$.
Since this is a function only of messages, it can leak frequency information etc.

Also, since the key is never reused, this is secure against a chosen plaintext attack,
since one would only recover the already used key.

\begin{convention}
  From now on, messages and keys are assumed to be binary strings.
\end{convention}

\begin{defn}[bitwise exclusive or]
  For two bitstrings $x,y \in \bits{n} \cong \Z/2\Z^n$,
  the bitwise XOR $x \xor y$ is just addition mod 2.
\end{defn}

Unfortunately, due to Shannon, we have this theorem:

\begin{theorem}
  A perfectly secure symmetric-key scheme must have
  at least as many keys as there are messages.
\end{theorem}

\section{Stream ciphers}

Instead of using a random key in the OTP, use a pseudorandom key.

\begin{defn}[pseudorandomness]
  A \term*{pseudorandom bit generator} (PBRG) is a deterministic algorithm
  that takes as input a \term{seed} and outputs a \term*{pseudorandom} sequence called
  the \term{keystream}.
\end{defn}

Then, we can construct a stream cipher by defining the key as the seed
and the ciphertext as the keystream XOR'd with the plaintext.
To decrypt, use the seed to generate the same keystream and XOR with the ciphertext.

For a stream cipher to be secure, we need:
\begin{itemize}
  \item Indistinguishability: the keystream is indistinguishable from a
        truly random sequence; and
  \item Unpredictability: given a partial keystream, it is infeasible to learn
        any information from the remainder of the keystream.
\end{itemize}

\begin{remark}
  Do not use built-in UNIX \texttt{rand} or \texttt{srand} for cryptography!
\end{remark}

Now, we introduce ChaCha20, a stream cipher actually uesd in the real world.
The algorithm works entirely on words (32-bit numbers).
It has no known flaws (other than people bungling the implementation).

\begin{scheme}[ChaCha20]\label{s:chacha20}
  First, define a helper function $QR(a,b,c,d)$ on 32-bit words:
  \begin{enumerate}[nosep]
    \item $a \gets a \modplus b$, $d \gets d \xor a$, $d \gets d \lll 16$
    \item $c \gets c \modplus d$, $b \gets b \xor c$, $b \gets b \lll 12$
    \item $a \gets a \modplus b$, $d \gets d \xor a$, $d \gets d \lll 8$
    \item $c \gets c \modplus d$, $b \gets b \xor c$, $b \gets b \lll 7$
  \end{enumerate}
  where $\xor$ is bitwise XOR, $\modplus$ is addition mod $2^{32}$, and $\lll$ is left bit-rotation.

  Given a 256-bit key $k = (k_1,\dotsc,k_8)$,
  a selected 96-bit nonce $n = (n_1,n_2,n_3)$,
  a 128-bit given constant $f = (f_1,\dotsc,f_4)$,
  and 32-bit counter $c \gets 0$, construct an initial state:
  \[
    S := \begin{bmatrix}
      f_1 & f_2 & f_3 & f_4 \\
      k_1 & k_2 & k_3 & k_4 \\
      k_5 & k_6 & k_7 & k_8 \\
      c   & n_1 & n_2 & n_3
    \end{bmatrix} = \begin{bmatrix}
      S_1    & S_2    & S_3    & S_4    \\
      S_5    & S_6    & S_7    & S_8    \\
      S_9    & S_{10} & S_{11} & S_{12} \\
      S_{13} & S_{14} & S_{15} & S_{16}
    \end{bmatrix}
  \]
  Keep a copy $S' \gets S$, then apply:
  \begin{gather*}
    QR(S_{1}, S_{5}, S_{9}, S_{13}),\quad QR(S_{2}, S_{6}, S_{10}, S_{14}),\quad QR(S_{3}, S_{7}, S_{11}, S_{15}),\quad QR(S_{4}, S_{8}, S_{12}, S_{16}) \\
    QR(S_{1}, S_{6}, S_{11}, S_{16}),\quad QR(S_{2}, S_{7}, S_{12}, S_{13}),\quad QR(S_{3}, S_{8}, S_{9}, S_{14}),\quad QR(S_{4}, S_{5}, S_{10}, S_{15})
  \end{gather*}
  ten times (for 80 total calls to $QR$) and output $S \xor S'$.
  This gives us 64 keystream bytes.

  Increment $c \gets c+1$ and repeat as necessary to generate more keystream bytes.

  To encrypt, XOR the keystream with the plaintext, then append the nonce.

  To decrypt, pop off the nonce, then XOR the keystream with the ciphertext.
\end{scheme}

\lecture{Jan 15}
One must be careful never to reuse nonces, since this results in the same keystream,
leading to recoverable messages.
In practice, this is hard (e.g., two devices with the same key).

Miscellaneous remarks:
\begin{itemize}[nosep]
  \item Why is ChaCha20 so good? The $QR$ function is very fast at the hardware level
        and there is wide adoption/standardization by experts.
  \item Why 10 rounds? If you do 1 or 2 rounds, there is a trivial attack.
        The latest theoretical attacks can attack 7 rounds
        (currently infeasible, but still better than exhaustive key search).
        So 8 rounds is secure and we do 10 to be safe.
  \item Is this secure forever (i.e., can we always just increase rounds)?
        No. Nothing in this course is.
        Someone could find a super crazy PMATH theorem that shows
        predictability of the $QR$ scramble.
\end{itemize}

\section{Block ciphers}

\begin{defn}[block cipher]
  Like a stream cipher, but instead of processing one character at a time,
  we break up the plaintext into \term[block]{blocks} of equal length
  and encrypt block-wise.
\end{defn}

\begin{example}
  The Data Encryption Standard (DES) is a standard
  56-bit key and 64-bit blocks.
\end{example}

\paragraph{Aside: History and the NSA doing ratfuckery}
In 1972, the National Institute of Standards and Technology (NIST)\footnote{of standardized peanut butter fame}
puts out an RfP for encryption algorithms.

IBM developed and proposed 64-bit DES, but then the NSA reduced it in 1975
to 56-bit so they can do some spying.
This made DES feasible to break by nation-states but not smaller organizations.

The National Security Agency (NSA)
is the US' signals intelligence (SIGINT; hacking foreign intelligence)
and information insurance (IA; defending domestic intelligence) agency.
They have a history of regulating how strong cryptoraphic products can be
by banning the export of strong cryptography.

Canada has an NSA equivalent: the Communications Security Establishment (CSE).
Along with the Kiwi CCSA, British GCHQ, and Australian ASD,
these are the Five Eyes who spy on just about everyone.


We only really know stuff about the NSA/Five Eyes due to the Snowden leaks.
For example, the SIGINT Enabling Project attempts to influence/blackmail
companies to weaken their security with backdoors.

Throughout the course, we will use the NSA to mean ``generic nation-state level adversary'',
since if you can defeat the NSA, you can defeat basically anyone.

Anyways, weakened DES was adopted by NIST in 1977 as FIPS 46 in 1977,
then as a banking standard as ANSI X3.92 in 1982 (replaced by Triple-DES in 1988).
From 1997--2001, a new contest developed the Advanced Encryption Standard (AES),
which is the current standard block cipher.

\paragraph{Desired properties of block ciphers} (Shannon, 1949):
\begin{enumerate}[nosep]
  \item Diffusion: Each ciphertext bit should depend on all plaintext bits.
  \item Confusion: The key--ciphertext relationship should be complicated.
  \item Key length: Keys should be small but not too small to be searchable.
  \item Simplicity: Ease of implementation and analysis.
  \item Speed: Runs quickly on all reasonable hardware.
  \item Platform: Can be implemented in hardware and software.
\end{enumerate}

\begin{scheme}[DES]
  The design principles of DES are still classified,
  so we just treat it as a black box for this course.
  We only need to know that there is a 56-bit key and 64-bit blocks.
\end{scheme}

The DES key space is not very big.
Exhaustive search on DES takes $2^{56}$ operations.
In 1997, this took three months.
In 2012, it takes 11.5 hours.

The blocks are also not very large.
By the birthday paradox, there is a collision every $2^{32}$ blocks.
This is an information leak, breaking semantic security.

These are the only (known) weaknesses in DES.

\begin{defn}[multiple encryption]
  Re-encrypt the ciphertext more times with different keys.
\end{defn}

This is not always more secure. For example, in the simple substitution cipher,
permutations can be composed and do not introduce more security.

\begin{scheme}[Double-DES]
  Pick a secret key $k = (k_1,k_2) \randin \bits{112}$.

  Then, encrypt $E_{k_2}(E_{k_1}(m))$ where $E$ is DES encryption

  Likewise, decrypt $E^{-1}_{k_2}(E^{-1}_{k_1}(m))$ where $E^{-1}$ is DES decryption.
\end{scheme}

We now have an exhaustive key search of $2^{112}$ operations, which is better.
However, there is an attack which reduces this to breaking DES.

\textrule{$\downarrow$ Lectures 5, 6, and 7 taken directly from slides $\downarrow$}
\lecture{Jan 17}
\begin{attack}[Meet-in-the-middle attack on Double-DES]\label{atk:mitm}
  The main idea is that $c = E_{k_2}(E_{k_1}(m))$ if and only if
  $E_{k_2}^{-1}(c) = E_{k_1}(m)$.

  Given three plaintext/ciphertext pairs $(m_1,c_1)$, $(m_2,c_2)$ and $(m_3,c_3)$:
  \begin{algorithmic}[1]
    \State Create a table $T$ of pairs sorted by first entry
    \For{$h_2 \in \bits{56}$} \Comment{$h_2$ is a guess for $k_2$}
      \State $T.\vv{insert}(E_{h_2}^{-1}(m_1),h_2)$
    \EndFor
    \For{$h_1 \in \bits{56}$} \Comment{$h_1$ is a guess for $k_1$}
      \State Compute $E_{h_1}(m_1)$
      \State Search for entries in $T$ matching $(E_{h_1}(m_1),-)$
      \For{each match $(-,h_2)$}
        \If{$E_{h_2}(E_{h_1}(m_2)) = c_2$}
          \If{$E_{h_2}(E_{h_1}(m_3)) = c_3$}
            \State \Return{$(h_1,h_2)$}
          \EndIf
        \EndIf
      \EndFor
    \EndFor
  \end{algorithmic}
\end{attack}

In \cref{atk:mitm}, we use three pairs. Why do we need that many?

Since the key space is smaller than the message space,
there will be multiple keys that encrypt a message to the same ciphertext.

\begin{lemma}[number of plaintext-ciphertext pairs needed]
  Let $E$ be a block cipher with $\ell$-bit key space
  and $L$-bit plaintext/ciphertext space.

  If $E$ is a random bijection, the expected number of
  \term{false keys} matching $t$ pairs is $\frac{2^\ell-1}{2^{Lt}}$.
\end{lemma}
\begin{prf}
  We assume that $E$ is a random bijection, so we can calculate probabilities.

  Fix the true key $k'$.
  Let $(m_i,c_i)$ for $i = 1,\dotsc,t$ be known plaintext-ciphertext pairs
  where each plaintext is distinct.

  For some $k \in K$, $k \neq k'$, the probability that $E_k(m_i) = c_i$
  for all $i$ is $\underbrace{\frac{1}{2^L}\cdot\frac{1}{2^L}\cdots\frac{1}{2^L}}_{\text{$t$ times}} = \frac{1}{2^{Lt}}$.

  Therefore, across all of $K \setminus \{k'\}$,
  the expected number of false keys is $\frac{2^\ell-1}{2^{Lt}}$.
\end{prf}

For Double-DES, $\ell = 112$ and $L = 64$:
\begin{itemize}
  \item For $t=1$, $FK \approx 2^{48}$.
        That is, given $(m,c)$ the number of Double-DES keys $(h_1,h_2)$
        for which $E_{h_2}(E_{h_1}(m)) = c$ is $\approx 2^{48}$.
  \item For $t=2$, $FK \approx 2^{-16}$.
        That is, the number of Double-DES keys $(h_1,h_2)$
        for which $E_{h_2}(E_{h_1}(m_1)) = c_1$ is $\approx 2^{48}$.
\end{itemize}

Therefore, we use three plaintext-ciphertext pairs in \cref{atk:mitm}.

The time requirement of the attack is $2^{56} + 2^{57} + 2 \cdot 2^{48} \approx 2^{57}$
DES encryptions/decryptions.
The size of the table is $2^{56}(64+56)$ bits or about 1 million TB.

\begin{xca}
  Modify \cref{atk:mitm} to decrease storage requirements at the expense of time.
  We can get down to $2^{56+s}$ operations and $2^{56-s}$ rows
  for $1 \leq s \leq 55$.
\end{xca}

We can now conclude that the security level of Double-DES is 57 bits,
not much better than normal DES' 56 bits.

\begin{scheme}[Triple-DES]
  Pick a secret key $k = (k_1,k_2,k_3) \randin \bits{168}$.

  Then, encrypt $E_{k_3}(E_{k_2}(E_{k_1}(m)))$ where $E$ is DES encryption

  Likewise, decrypt $E^{-1}_{k_3}(E^{-1}_{k_2}(E^{-1}_{k_1}(m)))$
  where $E^{-1}$ is DES decryption.
\end{scheme}

As for Double-DES, the 168-bit keys are infeasible to search.

\begin{xca}
  Show that \cref{atk:mitm} on Triple-DES takes $2^{112}$ operations.
\end{xca}

This means the security level of Triple-DES is 112 bits.
We cannot \emph{prove} Triple-DES is more secure than DES,
just that it empirically feels better.

\section{Substitution-permutation networks}
\lecture{Jan 19}

\begin{defn}[substitution-permutation network]
  A \term*{substitution-permutation network} (SPN) is an iterated block cipher
  where each iteration (\term{round}) is a substitution followed by a permutation.

  Formally, we have:
  \begin{itemize}[nosep]
    \item a block length $n$, key length $\ell$
    \item number of rounds $h$,
    \item \term{substitution} $S : \bits{b} \to \bits{b}$,
          an invertible function where $b \mid n$,
    \item \term{permutation} $P$,
          an invertible function $\{1,\dotsc,n\} \to \{1,\dotsc,n\}$, and
    \item \term{key scheduling algorithm} $k_i$ that determines a
          \term{round key} for each round $i = 1,\dotsc,h+1$ given a key $k$.
  \end{itemize}
  Note that $n$, $\ell$, $h$, $S$, $P$, and the key scheduling algorithm are public.

  Then, we can write encryption as
  \begin{algorithmic}
    \State $A \gets m$
    \For{$i = 1,\dotsc,h$}
      \State $A \gets A \xor k_i$
      \State $A \gets S(A[1 : b]) \concat S(A[b+1 : 2b]) \concat \dotsb \concat S(A[n-b+1 : n])$
      \Comment{Apply $S$ to each $b$ bits}
      \State $A \gets P(A)$
    \EndFor
    \State $A \gets A \xor k_{h+1}$
    \Return{$A$}
  \end{algorithmic}
  and decryption is the reverse (since $S$ and $P$ are invertible).
\end{defn}

The most notable SPN is the Advanced Encryption Standard (AES)
which was adopted in 2001 as FIPS 197, a U.S. government standard.
It uses 128-bit blocks and either 128, 192, or 256-bit keys.

As of 2024, there are no known AES attacks
that are significantly faster than exhaustive key search.

\begin{scheme}[AES]
  Given a key $k$ and block of plaintext,
  initialize a $4 \times 4$ byte array $\vv{State}$
  containing the plaintext.

  Depending on the key size (128, 192, 256), let $h$ be (10, 12, 14).
  Using the key schedule, generate $h+1$ round keys $k_0,\dotsc,k_h$.
  We will need three helper functions
  \Call{SubBytes}{}, \Call{ShiftRows}{}, and \Call{MixColumns}{}.

  Then, encryption is
  \begin{algorithmic}
    \State $\vv{State} \gets$ block of plaintext
    \State $(k_0,\dotsc,k_h) \gets$ round keys from the key schedule
    \State $\vv{State} \gets \vv{State} \xor k_0$
    \For{$i = 1,\dotsc,h-1$}
      \State $\vv{State} \gets \Call{SubBytes}{\vv{State}}$
      \State $\vv{State} \gets \Call{ShiftRows}{\vv{State}}$
      \State $\vv{State} \gets \Call{MixColumns}{\vv{State}}$
      \State $\vv{State} \gets \vv{State} \xor k_i$
    \EndFor
    \State $\vv{State} \gets \Call{SubBytes}{\vv{State}}$
    \Comment{Note: we skip \Call{MixColumns}{} in the last round}
    \State $\vv{State} \gets \Call{ShiftRows}{\vv{State}}$
    \State $\vv{State} \gets \vv{State} \xor k_h$
    \State \Return{$\vv{State}$}
  \end{algorithmic}
  To decrypt, do everything backwards
  (making calls to \Call{InvSubBytes}{}, \Call{InvShiftRows}{}, and \Call{InvMixColumns}{}).
\end{scheme}

AES does a lot of math over the Galois field $\GF(2^8)$.

\begin{defn*}[$\GF(2^8)$]\label{def:gf28}
  Consider the field $\Z/2\Z[y]$ of polynomials with coefficients in $\Z$ mod 2.

  The finite field $\GF(2^8) = (\Z/2\Z[y])/(y^8+y^4+y^3+y+1)$
  contains those polynomials with degree at most 7.

  Addition and multiplication are defined normally (mod $y^8+y^4+y^3+y+1$).
\end{defn*}

We notate elements $a(y) \in \GF(2^8)$ as the binary string of their coefficients.

\begin{example}
  The string $a = 11101100 = \x{ec}$
  is identified with $a(y) = y^7 + y^6 + y^5 + y^3 + y^2$.
\end{example}

Since polynomial addition is coefficient-wise and $\Z/2\Z$ is isomorphic with XOR,
we can treat $\GF(2^8)$ addition as binary string XOR.

\begin{example}
  Let $b = 00111011 = \x{3b}$.
  Then, $c(y) = a(y) + b(y) = y^7 + y^6 + y^4 + y^2 + y + 1$.
  We could have instead found $c = 11010111 = \x{d7}$
  by noticing that $a \xor b = \x{ec} \xor \x{3b} = \x{d7}$.
\end{example}

Multiplication requires a long division to find the answer mod $y^8 + y^4 + y^3 + y + 1$.

\begin{example}\label{exa:mult}
  Let $d(y) := a(y) \cdot b(y)$. Calculate:
  \begin{align*}
    d(y) & = (y^7 + y^6 + y^5 + y^3 + y^2)(y^5 + y^4 + y^3 + y + 1)                           \\
         & = y^{12} + 2y^{11} + 3y^{10} + 2y^9 + 3y^8 + 4y^7 + 4y^6 + 2y^5 + y^4 + 2y^3 + y^2 \\
         & = y^{12} + y^{10} + y^8 + y^4 + y^2
  \end{align*}
  Then, do polynomial long division:
  \[
    \setlength\arraycolsep{0pt}
    \setlength\extrarowheight{2pt}
    \newcolumntype{q}{>{{}}c<{{}}}
    \begin{array}[t]{rqrqrqrqrqrqrqrqrqrqrqrqrqrqr}
                                     &   &        &   &        &   &     &   &     &   &     &   &     &   & y^4 &   &     & + & y^2 \\
      \cline{2-19}
      y^8 + y^4 + y^3 + y + 1 \bigl) &   & y^{12} & + & y^{10} & + & y^8 & + &     &   &     &   &     &   & y^4 &   &     & + & y^2 \\
                                     & - & y^{12} &   &        & + & y^8 & + & y^7 &   &     & + & y^5 & + & y^4                     \\
      \cline{2-15}
                                     &   &        &   & y^{10} &   &     & + & y^7 &   &     & + & y^5                               \\
                                     &   &        & - & y^{10} &   &     &   &     & + & y^6 & + & y^5 &   &     & + & y^3 & + & y^2 \\
      \cline{4-19}
                                     &   &        &   &        &   &     &   & y^7 & + & y^6 &   &     &   &     & + & y^3
    \end{array}
  \]
  to conclude that the remainder is $y^7 + y^6 + y^3$.
  Therefore, $\x{ec} \cdot \x{3b} = 11001000 = \x{c8}$.
\end{example}

But by the XOR trick from addition, we can do this faster using XOR.

\begin{example}
  First, long multiply $11101100$ by $00111011$ using XOR to reduce:
  \[
    \setlength\arraycolsep{0pt}
    \setlength\extrarowheight{2pt}
    \begin{array}[t]{rrl}
               &       & 11101100 \\
               & 1     & 1101100  \\
               & 111   & 01100    \\
               & 1110  & 1100     \\
      {}\xor{} & 11101 & 100      \\ \hline
               & 10101 & 00010100
    \end{array}
  \]
  Then, do long division by $f = 100011011$ using XOR to subtract:
  \[
    \begin{array}[t]{rrrrr}
      \setlength\arraycolsep{0pt}
      \setlength\extrarowheight{2pt}
                       &    & 1       & 01 & 00 \\ \cline{2-5}
      100011011 \bigl) & 10 & 1010001 & 01 & 00 \\
                       & 10 & 0011011 &         \\ \cline{2-3}
                       & 00 & 1001010 &         \\
                       &    & 1000110 & 11      \\ \cline{3-4}
                       &    & 0001100 & 10 & 00
    \end{array}
  \]
  Therefore, $\x{ec} \cdot \x{3b} = 11001000 = \x{c8}$.
\end{example}

Now, we can define the helper functions.
The substitution in AES is based on the inverse in $\GF(2^8)$.

\begin{defn*}[AES S-box]
  Let $p \in \bits8$. We define $S : \bits8 \to \bits8$.

  Considering $p$ as an element of $\GF(2^8)$,
  let $q = p^{-1}$ (which always exists except if $p = 0$, in which case let $q=0$).
  Treating $q$ as a bit vector, compute
  \[
    S(p) = r = \begin{pmatrix}
      1 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\
      1 & 1 & 0 & 0 & 0 & 1 & 1 & 1 \\
      1 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\
      1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 \\
      1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 \\
      0 & 1 & 1 & 1 & 1 & 1 & 0 & 0 \\
      0 & 0 & 1 & 1 & 1 & 1 & 1 & 0 \\
      0 & 0 & 0 & 1 & 1 & 1 & 1 & 1
    \end{pmatrix}q
    + \mqty(1\\1\\0\\0\\0\\1\\1\\0)
  \]
  with scalar arithmetic in $\Z/2\Z$.
\end{defn*}

Then, \Call{SubBytes}{} just applies $S$ to each byte of $\vv{State}$.
The decryption call \Call{InvSubBytes}{} just multiplies by the inverse of the matrix.

The permutation takes two steps:
first, \Call{ShiftRows}{} shifts the $i$\xth row left by $i$ bits.
Then, \Call{MixColumns}{} treats each column as a polynomial in $\GF(2^8)[x]/(x^4-1)$
and multiplies it by $c(x) = \x{02} + \x{01}x + \x{01}x^2 + \x{03}x^3$.

\begin{example}\label{exa:mixcolumn}
  Let $a = \x{d0f112bb}$ be a column.
  Multiply
  \begin{align*}
    a(x) \cdot c(x)
     & = (\x{d0} + \x{f1}x + \x{12}x^2 + \x{bb}x^3)(\x{02} + \x{01}x + \x{01}x^2 + \x{03}x^3)                                                            \\
     & = (\x{d0}\cdot\x{02}) + (\x{d0}\cdot\x{01} + \x{f1}\cdot{02})x + (\x{d0}\cdot\x{01} + \x{f1}\cdot\x{01} + \x{12}\cdot\x{02})x^2                   \\
     & \quad\quad + (\x{d0}\cdot\x{03} + \x{f1}\cdot\x{01} + \x{12}\cdot\x{01} + \x{bb}\cdot\x{02})x^3                                                   \\
     & \quad\quad + (\x{f1}\cdot\x{03} + \x{12}\cdot\x{01} + \x{bb}\cdot\x{01})x^4 + (\x{12}\cdot\x{03} + \x{bb}\cdot\x{01})x^5 + (\x{bb}\cdot\x{03})x^6 \\
     & = \x{bb} + \x{29}x + \x{05}x^2 + \x{e5}x^3 + \x{a1}x^4 + \x{8d}x^5 + \x{d6}x^6
  \end{align*}
  where coefficient arithmetic is in $\GF(2^8)$.
  Find the remainder modulo $\x{01}x^4-\x{01}$
  by replacing $x^4 \mapsto 1$:
  \begin{align*}
    r(x) & = \x{bb} + \x{29}x + \x{05}x^2 + \x{e5}x^3 + \x{a1} + \x{8d}x^2 + \x{d6}x^3 \\
         & = \x{1a} + \x{a4}x + \x{d3}x^2 + \x{e5}x^3
  \end{align*}
  Therefore, $\Call{MixColumn}{\x{d0f112bb}} = \x{1aa4d3e5}$.
\end{example}

Naturally, \Call{InvShiftRows}{} shifts the $i$\xth row right by $i$ bits
and \Call{InvMixColumns}{} multiplies each column by $c^{-1} = \x{0e09d00b}$.

Finally, we can define the key schedule.
For 128-bit keys, we need 11 round keys.
The first round key $k_0 = (r_0,r_1,r_2,r_3)$ is the actual AES key.
Then, each subsequent round key
\begin{align*}
  k_i & = (r_{4i},r_{4i+1},r_{4i+2},r_{4i+3}) = (f(r_{4i-1})\xor r_{4i-4}, r_{4i} \xor r_{4i-3}, r_{4i+1} \xor r_{4i-2}, r_{4i+2} \xor r_{4i-1})
\end{align*}
where $f_i$ maps the four bytes $(a,b,c,d)$ to $(S(b) \xor \ell_i, S(c), S(d), s(a))$
for some round constants $\ell_i$.

\paragraph{Aside: Implementation}
This section is just me doing nerd shit trying to make Assignment 2 easier.
The finite fields used in AES can be replicated in Sage or Mathematica.
In Sage:
\begin{minted}[autogobble,mathescape]{python}
  aes.<y> = GF(2^8, modulus=x^8+x^4+x^3+x+1) # define AES field $(\Z/2\Z)[y]/(f(y))$
  aes_int = aes._cache.fetch_int             # byte to $\GF(2^8)$ element
  mcf.<x> = aes[]                            # MixColumns field $\GF(2^8)[x]$
  hex_string = lambda x: bytes(u.integer_representation() for u in x.list()).hex()

  # $\text{\Cref{exa:mult}}$: multiply ec * 3b
  a, b = aes_int(0xec), aes_int(0x3b)
  r = a * b
  print('r(x):', r)
  print('a*b:', hex(r.integer_representation()))
  # r(x): $y^7 + y^6 + y^3$
  # a*b:  0xc8

  # $\text{\Cref{exa:mixcolumn}}$: MixColumn(d0f112bb)
  a = [0xd0,0xf1,0x12,0xbb]
  ax = mcf([aes_int(u) for u in a])
  cx = (y + x + x^2 + (y+1) * x^3)
  bx = (ax * cx).mod(x^4+1)
  print('b(x):', bx)
  print('b:', hex_string(bx))
  # b(x): $(y^7 + y^6 + y^5 + y^2 + 1)x^3 + (y^7 + y^6 + y^4 + y + 1)x^2 + (y^7 + y^5 + y^2)x + y^4 + y^3 + y$
  # b:    1aa4d3e5
\end{minted}

In Mathematica (version 13.3 or later):
\begin{minted}[autogobble,mathescape]{mathematica}
  F = FiniteField[2, #^8 + #^4 + #^3 + # + 1 &];
  GF[hex_] := F[FromDigits[hex, 16]];
  poly[f_] := Expand@FromDigits[Reverse@f["Coefficients"], y];
  hex[f_] := IntegerString[f["Index"], 16, 2];

  (* $\text{\Cref{exa:mult}}$: multiply ec * 3b *)
  a = GF["ec"];
  b = GF["3b"];
  poly[a*b]      (* $y^3+y^6+y^7$ *)
  hex[a*b]       (* c8 *)

  (* $\text{\Cref{exa:mixcolumn}}$: MixColumn(d0f112bb) *)
  a = GF["d0"] + GF["f1"] x + GF["12"] x^2 + GF["bb"] x^3;
  c = F[2] + F[1] x + F[1] x^2 + F[3] x^3;
  b = PolynomialRemainder[a*c, x^4 - 1, x];
  StringJoin[hex /@ CoefficientList[b, x]] (* 1aa4d3e5 *)
\end{minted}


\section{Block cipher modes of operation}

TODO

\chapter{Hash functions}

\section{Definitions}
\lecture{Jan 22}

\begin{defn}[hash function]
  A mapping $H$ such that
  \begin{enumerate}
    \item $H : \bits{\leq L} \to \bits{n}$ maps binary messages
          of arbitrary lengths $\leq L$ to outputs of a fixed length $n$.
    \item $H(x)$ can be efficiently computed for all $x \in \bits{\leq L}$
  \end{enumerate}
  is an \term*{$n$-bit hash function}.
  We call $H(x)$ the \term{hash} or \term{digest} of $x$.

  We usually suppose that $L$ is large and just write $H : \hash{n}$.
\end{defn}

In a more general context, a hash function is an efficiently computable function.

\begin{example}
  Let $H : \bits{\leq4} \to \bits{2}$
  be a hash function mapping a bitstring to its last two digits.
  For example, $H(1101) = 01$.

  We call 1001 a \term{preimage} of 01.

  The pair $(01, 1001)$ is a \term{collision}
  where 01 is a \term{second preimage} of 1001.
\end{example}

Generically, we can create a hash function given a block cipher.

\begin{scheme}[Davies--Meyer hash function]
  Let $E_k$ be an $m$-bit block cipher with $n$-bit key $k$.
  Let $\vv{IV}$ be a fixed $m$-bit initializing value.

  Then, to compute $H(x)$,
  \begin{enumerate}[noitemsep]
    \item Break up $x \concat 1$ into $n$-bit blocks $\bar x = x_1,\dotsc,x_t$
          (padding $x_t$ with 0s if necessary)
    \item $H_0 \gets \vv{IV}$
    \item $H_i \gets E_{x_i}(H_{i-1}) \xor H_{i-1}$ for all $i=1,\dotsc,t$
    \item $H(x) \gets H_t$
  \end{enumerate}
\end{scheme}

Hash functions are used basically everywhere in cryptography,
mostly just because they are stupidly fast and introduce ``scrambling'' that,
given a good enough hash function, cannot be reversed.

\begin{defn}[preimage resistance]
  A hash function $H = \hash{n}$ is \term*{preimage resistant} (PR) if,
  given a hash value $y \randin \bits{n}$, it is computationally infeasible
  to find any $x \in \bits{*}$ with $H(x) = y$ with non-negligible success probability.
\end{defn}

Note that we include disclaimers like ``non-negligible success probability''
since otherwise we could just use an attack like ``guess! it might just work!''

This is helpful for implementing passwords.
If we store $(\vv{password}, H(\vv{password}))$ with a PR hash $H$,
then stealing the system password file does not actually reveal the passwords.

\begin{defn}[2\nd preimage resistance]
  A hash function $H = \hash{n}$ is \term*{2\nd preimage resistant} (2PR) if,
  given $x \randin \bits{*}$, it is computationally infeasible to find
  any $x' \in \bits{*}$ with $x' \neq x$ and $H(x') = H(x)$ with non-negligible success probability.
\end{defn}

This is helpful for ensuring that a message is unchanged
(Modification Detection Codes; MDCs).
To ensure a message $m$ is unmodified, publicize $H(m)$.
Then, as long as $H$ is 2PR and we can verify the hash,
we can safely assume $m$ is unmodified.

\begin{defn}[collision resistance]
  A hash function $H = \hash{n}$ is \term*{collision resistant} (CR)
  if it is computationally infeasible to find distinct $x,x' \in \bits{*}$
  where $H(x') = H(x)$.
\end{defn}

This allows us to optimize message signing.
Instead of signing a large file $x$, Alice can sign $H(x)$ instead.
Keeping all the desired properties of a signing scheme
requires PR, 2PR, and CR.

\textrule{$\uparrow$ Lectures 5, 6, and 7 taken directly from slides $\uparrow$}
\lecture{Jan 24}

\begin{prop}
  If $H$ is CR, then $H$ is 2PR.
\end{prop}
\begin{prf}
  Take the contrapositive: $H$ is not 2PR $\implies$ $H$ is not CR.

  Suppose $H$ is not 2PR, i.e., we have an efficient algorithm
  to find a collision $x'$ given $x$.

  Select a random $x$. Get the collision $x'$ from our algorithm.
  Then, we have a collision $(x,x')$ that we found efficiently,
  so $H$ is not CR.
\end{prf}

It will \emph{always} be easier to do the contrapositive in this course,
especially because most definitions use ``it is not possible''.

\begin{prop}
  CR does not guarantee PR.
\end{prop}
\begin{prf}
  We give a counterexample.

  Suppose that $H : \bits{*} \to \bits{n}$ is CR.

  Consider the hash function $\bar H = \bits{*} \to \bits{n+1}$ defined by
  \[
    \bar H(x) = \begin{cases}
      0 \concat H(x) & x \not\in \bits{n} \\
      1 \concat x    & x \in \bits{n}
    \end{cases}
  \]
  where $\concat$ denotes the concatenation operation. Then $\bar H$ is CR because $H$ is.

  However, $\bar H$ is not PR for at least half of all $y \in \bits{n+1}$
  we can efficiently find the preimage (i.e., for all the hash values beginning with 1,
  we can just lop off the 1 to get the original).
\end{prf}

Note: if we disallow pathological hash functions like this, i.e.,
we have some constraint on uniformity in the size of preimages,
CR does guarantee PR.

\begin{prop}
  Suppose $H$ is \term{somewhat uniform}, i.e., preimages are all around the same size.
  If $H$ is CR, then $H$ is PR.
\end{prop}
\begin{prf}
  Suppose that $H : \bits{*} \to \bits{n}$ is not PR.
  We must show $H$ is not CR.

  Select $x \randin \bits{*}$ and compute $y = H(x)$.
  Since $H$ is not PR, we can efficiently find $x' \in \bits{*}$ with $H(x') = y$.
  Since $H$ is somewhat uniform, we expect that $y$ has many preimages,
  so $x' \neq x$ with very high probability.

  Therefore, $(x,x')$ is a collision and $H$ is not CR.
\end{prf}

\begin{prop}
  PR does not guarantee 2PR.
\end{prop}
\begin{prf}
  Suppose that $H : \bits{*} \to \bits{n}$ is PR.

  Define $\bar H : \bits{*} \to \bits{n}$ by $\bar H(x_1x_2\dots x_t) = H(0x_2\dots x_t)$.

  Then, $\bar H$ is PR but not 2PR.
\end{prf}

\begin{prop}
  Suppose $H$ is somewhat uniform. If $H$ is 2PR, then $H$ is PR.
\end{prop}
\begin{prf}
  Suppose that $H : \bits{*} \to \bits{n}$ is not PR and show it is not 2PR.

  Suppose we are given $x \bits{*}$.
  Compute $y = H(x)$ and then find with our PR-breaking algorithm $x'$ with $H(x') = y$.
  Since $H$ is somewhat uniform, we expect $x \neq x'$.

  Then, we have a collision $x'$ for $x$ and that breaks PR.
\end{prf}

\begin{prop}
  2PR does not guarantee CR.
\end{prop}
\begin{prf}
  Suppose that $H : \bits{*} \to \bits{n}$ is 2PR.

  Consider $\bar H : \bits{*} \to \bits{n}$ defined by $\bar H(x) = H(x)$
  except $\bar H(1) = H(0)$.

  Then, $\bar H$ is not CR because $(0,1)$ is a collision.

  However, we can show $\bar H$ is 2PR. Suppose $\bar H$ is \emph{not} 2PR.
  We show $H$ would also not be 2PR.

  Suppose we are given some $x$.
  Since $\bar H$ is not 2PR, we can find $x' \neq x$ with $\bar H(x') = \bar H(x)$.
  With almost certain probability, we can assume $x \neq 0,1$.
  Then, $\bar H(x) = H(x)$.
  If $x' \neq 1$, we have $\bar H(x') = H(x') = H(x)$.
  Otherwise, $\bar H(x') = \bar H(1) = H(0) = H(x)$
  We found a second preimage $x'$ or 0 for $x$, so $H$ is not 2PR.

  Therefore, by contradiction, $\bar H$ is 2PR.
\end{prf}

\begin{theorem}[relation between PR, 2PR, CR]\label{thm:prcr}
  Summarize:
  \begin{center}
    \begin{tabular}{r|ccc}
      If $H$ is $\downarrow$, then it is $\rightarrow$ & PR        & 2PR         & CR          \\ \hline
      PR                                               & --        & $\not\Rarr$ & $\not\Rarr$ \\
      2PR                                              & $\Rarr^*$ & --          & $\not\Rarr$ \\
      CR                                               & $\Rarr$   & $\Rarr^*$   & --
    \end{tabular}
  \end{center}
  where $\Rarr^*$ means ``implies under somewhat uniformity''
\end{theorem}

\section{Attacks}
\lecture{Jan 26}

\begin{defn}[generic attack]
  An attack which does not exploit any specific properties of a hash function.
  That is, it works on any generic hash function $H : \bits{*} \to \bits{n}$.
\end{defn}

To analyze a generic attack, we assume $H$ is a random function
in the sense that $y = H(x)$ can be treated as $y \randin \bits{n}$.

\begin{attack}[generic attack for preimages]
  Given $y \randin\bits{n}$, repeatedly select arbitrary $x$ until $H(x) = y$.
\end{attack}

This will take $2^n$ attempts, so as long as $n \geq 128$ we are safe.

\begin{attack}[generic attack for collisions]
  Select arbitrary $x \in \bits{*}$ and store $(H(x),x)$ in a table
  sorted by the first entry. Repeat until a collision is found.
\end{attack}

By the birthday paradox, the expected number of hash operations is
$\sqrt{\pi 2^n/2} \approx \sqrt{2^n}$.
Therefore, the attack is infeasible for $n \geq 256$.

The space complexity is also $\order{\sqrt{2^n}}$.
This is important since, for example, $n=128$ has a feasible runtime $2^{64}$
but an infeasible space requirement of 500 million TB.

We can prove that this is the optimal generic collision attack, i.e.,
no faster generic attack exists.

However, we can improve the space complexity.

Let $N = 2^n$.
Define a sequence $(x_i)_{i\geq0}$ by $x_0 \randin \bits{n}$
and $x_i = H(x_{i-1})$.

Since $(x_i) \subseteq \bits{n}$, we will eventually get repetitions.
Therefore, $(x_i)$ is eventually periodic, i.e.,
we will eventually get $x_a = x_b$ for $a \neq b$.
Then, we found a collision $(x_{a-1}, x_{b-1})$.

More formally, let $j$ be the smallest index for which $x_j = x_i$
for some $i < j$, which must exist.
Then, $x_{j+\ell} = x_{i+\ell}$ for all $\ell \geq 1$.

By the birthday paradox, $\E[j] \approx \sqrt{\pi N/2} \approx \sqrt{N}$.
In fact, since $i$ is a random element from before $j$,
we can say $\E[i] \approx \frac12\sqrt{N}$ and $\E[j-i] \approx \frac12\sqrt{N}$.

We will store only some \term*{distinguished points},
for example, elements where the top 32 bits are all 0.
Let $\theta$ be the proportion of distinguished points.
Here, $\theta = 2^{-32}$.

We can still tell detect a cycle as long as there is a distinguished point in the cycle.
Once we detect a collision, we work through the sequence near it.

\begin{attack}[van Oorschot--Wiener parallel collision search]
  We write this attack as two stages:
  \begin{algorithmic}[1]
    \State Create a table $T$
    \Procedure{DetectCollision}{$H$}
      \State Select $x_0 \randin \bits{n}$
      \State $T[x_0] \gets (0,-)$ \Comment{store (index, last distinguished point)}
      \State $c \gets 0$ \Comment{last distinguished point}
      \For{$d = 1,2,3\dotsc$}
        \State $x_d \gets H(x_{d-1})$
        \If{$x_d$ is distinguished}
          \If{$T[x_d]$ exists as $(b,x_a)$}
            \State $(a,-) \gets T[x_a]$ \Comment{need the index of $x_a$}
            \State \Return{\Call{FindCollision}{$x_a,x_c,a,b,c,d$}}
          \EndIf
          \State $T[x_d] \gets (d,c)$
          \State $c \gets d$
        \EndIf
      \EndFor
    \EndProcedure
    \Procedure{FindCollision}{$x_a,x_c,a,b,c,d$}
      \State $\ell_1 \gets b-a$, $\ell_2 \gets d-c$
      \State Suppose $\ell_1 \geq \ell_2$ so $k \gets \ell_1 - \ell_2$
      \State Compute $x_{a+1},\dotsc,x_{a+k}$
      \State $m \gets 1$
      \Repeat
        \State Compute $x_{a+k+m}, x_{c+m}$
        \State $m \gets m + 1$
      \Until{$x_{a+k+m} = x_{c+m}$}
      \State \Return{$(x_{a+k+m-1}, x_{c+m-1})$} \Comment{the collision is $H(x_{a+k+m-1}) = H(x_{c+m-1})$}
    \EndProcedure
  \end{algorithmic}
\end{attack}

In \Call{DetectCollision}{}, we will call the hash function
$\sqrt{\pi N/2} + \frac{1}{\theta}$ times.

In \Call{FindCollision}{}, we perform at most $\frac{3}{\theta}$ hashes.

In total, we expect to take $\sqrt{N}+\frac{4}{\theta}$ time.
But this time we only need $3n\theta\sqrt{N}$ space.

So for our $n=128$ case with $\theta=2^{-32}$,
the expected runtime is $2^{64}$ hashes (feasible)
and the expected storage is 192 GB (negigible).

\lecture{Jan 29}
We can parallelize VW collision search by having each processor
start on a random point and report discovered distinguished points to a central server.

For $m$ processors, we get an expected time of $\frac{1}{m}\sqrt{N}+\frac{4}{\theta}$ hashes
and space of $3n\theta\sqrt{N}$ bits.
That is, we get a speedup of $m$ times.

This is also nice because there is no communication between processors
and only occasional communication with the central server
(reducing the chance of race conditions and other parallelism problems).

\section{Iterated hash functions}

\begin{scheme}[Merkle's meta method]
  Fix an initializing value $\vv{IV} \in \bits{n}$
  and pick a compression function $f : \bits{n+r} \to \bits{n}$.

  Given a $b$-bit message $x$, to compute $H(x)$:
  \begin{enumerate}[nosep]
    \item Break up $x$ into $r$-bit blocks $\bar x = x_1,\dotsc,x_t$ (padding the last block with 0s if necessary)
    \item Define $x_{t+1}$ to hold the binary representation of $b$ (left-padding with 0s as necessary)
    \item Define $H_0 = \vv{IV}$
    \item Compute $H_i = f(H_{i-1} \concat x_i)$ for $i = 1,\dotsc,t+1$
    \item Return $H(x) = H_{t+1}$
  \end{enumerate}
\end{scheme}

Merkle also proved that collision resistance depends on $f$.

\begin{theorem}[Merkle]
  If the compression function $f$ is collision resistant,
  then the iterated hash function $H$ is also collision resistant.
\end{theorem}

Note that by \cref{thm:prcr}, we get PR and 2PR as well.

This feels very circular, but it can be helpful to give a proof of security
given certain very precise definitions.
However, the assumptions in the definitions might not be realistic.

\begin{prf}
  Suppose that $H$ is not CR. We will show that $f$ is not CR.

  Since $H$ is ont CR, we can efficiently find messages $x,x' \in \bits*$
  with $x \neq x'$ and $H(x) = H(x')$.

  Define $\bar x = x_1,\dotsc,x_t$, $b = \abs{x}$, length block $x_{t+1}$,
  and $\bar x' = x'_1,\dotsc,x'_{t'}$, $b' = \abs{x'}$, length block $x'_{t'+1}$.

  Then, we can efficiently compute
  \begin{align*}
    H_0        & = \vv{IV}            & H_0             & = \vv{IV}                \\
    H_1        & = f(H_0,x_1)         & H'_1            & = f(H_0,x'_1)            \\
    H_2        & = f(H_1,x_2)         & H'_2            & = f(H'_1,x'_2)           \\
    H_3        & = f(H_2,x_1)         & H'_3            & = f(H'_2,x'_3)           \\
               & \vdotswithin{=}      &                 & \vdotswithin{=}          \\
    H_{t-1}    & = f(H_{t-2},x_{t-1}) & H'_{t'-1}       & = f(H'_{t'-2},x'_{t'-1}) \\
    H(x) = H_t & = f(H_{t-1},x_t)     & H(x') = H'_{t'} & = f(H'_{t'-1},x'_{t'})
  \end{align*}
  Since $H(x) = H(x')$, we have $H_t = H'_{t'}$.

  Now, if $b \neq b'$, then $x_{t+1} \neq x'_{t'+1}$.
  Then, $(H_t \concat x_{t+1}, H'_{t'} \concat x'_{t'+1})$ is a collision for $f$.

  Otherwise, if $b = b'$, then $t = t'$ and $x_{t+1} = x'_{t+1}$.
  Let $i$ be the largest index for which $(H_i \concat x_{i+1}) \neq (H'_i \concat x'_{i+1})$
  which must exist because $x \neq x'$.

  Then, $H_{i+1} = f(H_i,x_{i+1}) = f(H'_i,x'_{i+1}) = H'_{i+1}$
  and we have a collision $(H_i\concat x_{i+1}, H'_i \concat x'_{i+1})$.

  Since we found a collision, $f$ is not CR.
\end{prf}

\paragraph{Aside: MD5 is bad}
MDx is a family of iterated hash functions.
MD4 was designed by Rivest in 1990 with a security level against VW of 64 bits
but broken \emph{by hand} by Wang in 2004 with an attack reducing the security level to 4 bits.
MD4 preimages can also be found in $2^{102}$ operations, which is infeasible but also still bad.

In 1991, Rivest designed MD5, a strengthened version of MD4.
The Wang attack reduced the security level of 39 bits,
but modern attacks can find collisions in $2^{24}$ operations.

In summary, MD5 should not be used if collision resistance is required
but it's \emph{probably} preimage resistant.
In fact, the Flame malware used a forged MD5-based Microsoft certificate
created using an improved version of Wang's attack.

\lecture{Jan 31}
For another example of why this is a problem,
consider the fact that Crowdmark uses MD5 hashes to verify
that a submitted file is the same (in case of an upload error).
A student could change their answer and submit the ``same''
file after looking at the solutions after the deadline.

The SHA family of functions are designed by the NSA.
Wang (our recurring character) attacked SHA (1993) to 39 bits and SHA-1 (1994) to 63 bits.
The SHA-2 family, a variable-length output version of SHA-1, has no known weaknesses.
The 224-, 256-, 384-, and 512-bit lengths are chosen so that the security levels
(against VW collision finding) line up with the security levels of
Triple-DES, AES-128, AES-192, and AES-256.

\begin{scheme}[SHA-256]
  SHA-256 is an iterated hash function with block length $r=512$,
  hash length $n = 256$, and compression function $f : \bits{256+512}\to\bits{256}$.

  The design principles are classified, so we can treat it as a black box.

  Define $h_1,\dotsc,h_8$ as the fractional parts of the
  square roots of the first eight primes
  and $y_0,\dotsc,y_{63}$ as the fractional parts of the
  cube roots of the first 64 primes.
  Finally, let
  \begin{align*}
    f(A,B,C) & = AB \xor \overline{BC}                                                         & g(A,B,C) & = AB \xor AC \xor BC                                                            \\
    r_1(A)   & = (A \hookrightarrow 2) \xor (A \hookrightarrow 13) \xor (A \hookrightarrow 22) & r_2(A)   & = (A \hookrightarrow 6) \xor (A \hookrightarrow 11) \xor (A \hookrightarrow 25) \\
    r_3(A)   & = (A \hookrightarrow 7) \xor (A \hookrightarrow 18) \xor (A \gg 3)              & r_4(A)   & = (A \hookrightarrow 17) \xor (A \hookrightarrow 19) \xor (A \gg 10)
  \end{align*}
  To find the hash of a $b$-bit message $x$ made of 32-bit words $x_0,x_1\dotsc$:
  \begin{algorithmic}[1]
    \State pad $x$ with 1 followed by 0s until the bitlength is $-64 \pmod{512}$.
    \State append a 64-bit representation of $b \pmod{2^{64}}$
    \State initialize $(H_1,\dotsc,H_8) \gets (h_1,\dotsc,h_8)$
    \For{$i = 0,\dotsc,m-1$}
      \State $X_j \gets x_{16i+j}$ \textbf{for} $0 \leq j \leq 15$
      \Comment{copy $i$\xth 16-word block into temp storage}
      \State $X_j \gets r_4(X_{j-2}) + X_{j-7} + r_3(X_{j-15}) + X_{j-16}$ \textbf{for} $16 \leq j \leq 63$
      \Comment{expand $X$ to 64 words}
      \State $(A,B,\dotsc,G,H) \gets (H_1,H_2,\dotsc,H_7,H_8)$
      \Comment{initialize working variables}
      \For{$j = 0,\dotsc,63$}
        \Comment{weird random shuffling}
        \State $T_1 \gets H + r_2(E) + f(E,F,G) + y_j + X_j$
        \State $T_2 \gets r_1(A) + g(A,B,C)$
        \State $(H,G,F,E,D,C,B,A) \gets (F,G,E,D+T_1,C,B,A,T_1+T_2)$
      \EndFor
      \State $(H_1,\dotsc,H_8) \gets (H_1 + A,\dotsc,H_8 + H)$
      \Comment{update working variables}
    \EndFor
    \State \Return{$H_1 \concat H_2 \concat \dotsb \concat H_8$}
  \end{algorithmic}
\end{scheme}

It would be very profitable to crack SHA-256 since it is used to
verify proof of work for Bitcoin mining.
Finding messages with arbitrary numbers of 0s at the starts of hashes would print money.

Just to be sure Wang can't come back and break SHA-2 (since it is still a Merkle design).
SHA-3 (Keecak, based on ``sponge construction'') was selected in 2012,
but nobody really uses it because SHA-256 is still better.

\chapter{Message authentication codes}
\lecture{Feb 2}

\section{Definitions}

\begin{defn}[message authentication code]
  A family of functions $\MAC_k : \hash{n}$ paramaterized by an $\ell$-bit key $k$
  where each function $\MAC_k$ can be efficiently computed.

  The \term{MAC} or \term{tag} of a message $x$ is denoted $t = \MAC_k(x)$.
\end{defn}

We use MAC schemes to provide data integrity and origin verification.
To do this:
\begin{enumerate}[nosep]
  \item Alice and Bob establish a secret key $k \in \bits{\ell}$.
  \item Alice computes the tag $t = \MAC_k(x)$ of a message $x$ and sends $(x,t)$ to Bob.
  \item Bob verifies that $t = \MAC_k(x)$.
\end{enumerate}
To avoid a \term[attack!replay]{replay attack}
(Eve saves a copy of a message and resends it later),
add a timestamp or sequence number.

Like with encryption, we have to formulate a security definition.

\begin{defn*}[MAC security]
  A MAC scheme is \term[message authentication code!security]{secure}
  if it is \term{existentially unforgeable} under a chosen-message attack.

  That is, for chosen messages $x_i$ and their MACs $t_i$,
  it is computationally infeasible to find with non-negligible success probability
  a valid message-MAC pair $(x,t)$ for a new message $x$.
\end{defn*}

Realistically, the messages $x_i$ will have to be ``harmless'' messages
that Alice is ordinarily willing to tag
and the forgery $x$ is a ``harmful'' message that Alice would ordinarily
be unwilling to tag.

\begin{defn}
  An \term[message authentication code!ideal]{ideal} MAC scheme
  is one where for each key $k \in \bits{\ell}$,
  the function $\MAC_k : \hash{n}$ is a random function.
\end{defn}

The naive generic attack is to just guess.

\begin{attack}[generic attack for tags]
  Select $y \randin \bits{n}$ and guess that $\MAC_k(x) = y$.
  Keep guessing.
\end{attack}

Assuming an ideal scheme, the success probability is $\frac{1}{2^n}$.

\begin{attack}[generic attack for keys]
  Perform the same attack as on an SKES.
\end{attack}

Assuming an ideal scheme, the expected number of keys for which $r$ messages
verify is $1+\vv{FK} = 1+(2^\ell -1)/2^r$.
If $\vv{FK}$ is negligible, the expected number of operations is $2^{\ell-1}$.

\section{Specific MACs}

\begin{scheme}[CBC-MAC]
  Let $E$ be an $n$-bit block cipher with key space $\bits{\ell}$.
  We assume that plaintext messages all have lengths that are multiples of $n$.
  To compute $\CBCMAC_k(x)$:
  \begin{algorithmic}[1]
    \State Divide $x$ into $n$-bit blocks $x_1,\dotsc,x_r$.
    \State $H_1 \gets E_k(x_1)$
    \For{$i = 2,\dotsc,r$}
      \State $H_i \gets E_k(H_{i-1} \xor x_i)$
    \EndFor
    \State \Return{$H_r$}
  \end{algorithmic}
\end{scheme}

It was proven in 1994 that CBC-MAC with fixed-length messages is secure
if $E$ is ideal (i.e., $E_k : \bits{n} \to \bits{n}$ is random).

However, it is totally broken for variable-length messages.
\begin{prf}
  Select an arbitrary 3-block message $x = (x_1,x_2,x_3)$.
  Obtain $t_1 = \CBCMAC_k(x_1) = E_k(x_1)$.
  Obtain $t_2 = \CBCMAC_k((t_1 \xor t_2) \concat x_3) = E_k(E_k(t_1 \xor x_2) \xor x_3)$.
  Then, $(x,t_2)$ is a forgery.
\end{prf}

One way to fix this is to add one more encryption round.

\begin{scheme}[Encrypted CBC-MAC (EMAC)]
  Given a second key $s$ for $E$,
  let $\operatorname{EMAC}_{k,s}(x) = E_s(\CBCMAC_k(x))$.
\end{scheme}

Again, it has been proven that EMAC is secure if $E$ is ideal.

Now, consider creating a MAC based on a hash function.
Let $H$ be an iterated $n$-bit hash function
with compression function $f : \bits{n+r} \to \bits{n}$.
Pick $k \randin \bits{n}$ and let $K \in \bits{r}$ be $k$ padded with 0s.

We might propose $\MAC_k(x) = H(K \concat x)$.
However, this is insecure under a \term[attack!length extension]{length extension attack}
since if we know $(x,\MAC_k(x))$,
we can calculate $\MAC_k(x \concat y)$ for arbitrary $y$
by resuming the hash process with the last block of $x$
(i.e., computing $f(t \concat y_1)$, etc.).

\begin{xca}
  Show that this is also insecure if messages are of arbitrary length
  and a length block is added to $K \concat x$.
\end{xca}

Securing against this attack is as simple as adding the key to each step,
giving us a secure hash-based MAC (``HMAC'')

\begin{scheme}[HMAC]
  Let $H$ be a hash function.
  Define $r$-bit constants $\vv{opad} = \x{3636\cdots 36}$ and $\vv{ipad} = \x{5c5c\cdots 5c}$.

  Then, $\HMAC_k(x) = H(K \oplus \vv{opad}, H(K \oplus \vv{ipad}, x))$.
\end{scheme}

\lecture{Feb 5}
The security analysis of HMAC is hard, but we can prove a theorem.

\begin{theorem}
  Suppose that the $f$ compression function used in $H$ is a secure MAC
  with fixed-length messages and a secret IV as the key.
  Then, HMAC is a secure MAC scheme.
\end{theorem}

Usually, HMAC uses SHA-256. It is widely used for internet security.

HMAC is also widely used as a \term{key derivation function}.
If Alice has $k$ and needs multiple session keys $sk_i$,
she can compute $sk_1 = \HMAC_k(1)$, $sk_2 = \HMAC_k(2)$, etc.
Then, since HMAC is secure, Eve can intercept individual session keys
and still know nothing about either the actual key $k$ or the other session keys.

\section{GSM}

TODO

\chapter{Authenticated encryption}

\section{Generic schemes}
We have established ways to encrypt data (e.g., AES-CBC)
and ways to authenticate data (i.e., MACs)
but what if we need both?

\begin{scheme}[Encrypt-and-MAC]
  Alice sends $(c,t) = (E_{k_1}(m), \MAC_{k_2}(m))$ to Bob,
  where $m$ is the plaintext and $(k_1,k_2)$ is a secret key shared with Bob.

  Then, Bob first decrypts $c$ to obtain $m = E_{k_1}^{-1}(c)$
  and then verifies $t = \MAC_{k_2}(m)$.
\end{scheme}

This is insecure because $\MAC_{k_2}(m)$ might leak information about $m$
(MACs are generally not semantically secure).

\begin{scheme}[Encrypt-then-MAC]
  Alice sends $(c,t) = (E_{k_1}(m), \MAC_{k_2}(c))$ to Bob,
  where $m$ is the plaintext and $(k_1,k_2)$ is a secret key shared with Bob.

  Then, Bob first verifies that $t = \MAC_{k_2}(c)$ and decrypts $c$ to obtain
  $m = E_{k_1}^{-1}(c)$.
\end{scheme}

This has been proven to be secure given that $E_{k_1}$ and $\MAC_{k_2}$
are secure.

\begin{defn*}[authentication encryption security]
  An authentication encryption scheme is
  \term[authentication encryption scheme!security]{secure} if
  \begin{enumerate}[nosep]
    \item it is semantically secure against chosen-plaintext attacks, and
    \item has \term{ciphertext integrity}, i.e., given $(m_1,c_1,t_1),\dotsc,(m_\ell,c_\ell,t_\ell)$,
          an attacker cannot forge a valid $(m,c,t)$.
  \end{enumerate}
\end{defn*}

\lecture{Feb 7}
\section{AES-GCM}
The most popular AE scheme is AES-GCM which uses AES-CTR and GMAC.

Recall how ChaCha20 (\cref{s:chacha20})
used a key, nonce, and counter to generate a keystream.
AES-CTR uses AES in a similar stream cipher paradigm.

\begin{scheme}[AES-CTR]
  Let $k \randin \bits{128}$ be a shared secret
  and $M = (M_1,\dotsc,M_u)$ be a message of 128-bit blocks.
  To encrypt:
  \begin{algorithmic}[1]
    \State Select a nonce $\vv{IV} \in \bits{96}$,
    \State $J_0 \gets \vv{IV} \concat 0^{31} \concat 1$,
    \For{$i = 1,\dotsc,u$}
      \State $J_i \gets J_{i-1} + 1$
      \State $C_i \gets \AES_k(J_i) \xor M_i$
    \EndFor
    \State \Return{$(\vv{IV}, C_1, \dotsc, C_u)$}
  \end{algorithmic}
  To decrypt, generate the keystream and XOR.
\end{scheme}

Since it is a counter, CTR encryption is parallelizable.
We also require, as with other IV-containing schemes, that the IV be unique
and never reused.

To define GMAC, recall \cref{def:gf28}.

\begin{defn*}[$\GF(2^{128})$]
  The field $\Z/2\Z[x]$ modulo $f(x) = 1 + x + x^2 + x^7 + x^{128}$.

  We associate a 128-bit block $a = a_0\cdots a_{127}$
  with the polynomial $a(x) = a_0 + a_1x + \dotsb + a_{127}x^{127}$.

  As in $\GF(2^8)$, we define $a \vdot b = a(x)\cdot b(x) \pmod{f(x)}$.
\end{defn*}

Then, we can describe GMAC:

\begin{scheme}[Galois Message Authentication Code (GMAC)]
  Let $A = (A_1,\dotsc,A_v)$ be the message in 128-bit blocks,
  $L$ be the bitlength of $A$ as a 128-bit block,
  and $k$ be a secret key.
  \begin{algorithmic}[1]
    \State $J_0 \gets \vv{IV} \concat 0^{31} \concat 1$
    \State $H \gets \AES_k(0^{128})$
    \State Define $f_A(y) = A_1 y^{v+1} + A_2y^v + \dotsb + A_{v-1}y^3 + A_v y^2 + Ly \in \GF(2^{128})[y]$
    \State $t \gets \AES_k(J_0) \oplus f_A(H)$
    \State \Return{$(\vv{IV}, A, t)$}
  \end{algorithmic}
\end{scheme}

\begin{prop}
  GMAC is secure.
\end{prop}
\begin{prf}[outline]
  Consider the simplified tag $t' = f_A(H)$.

  Then, the adversary can guess with probability $\frac{1}{2^{128}}$.

  She can also guess $t'$ by making a guess $H'$ and computing $f_A(H')$
  with success probability $\frac{v+1}{2^{128}}$.

  But if Eve sees a single valid message-tag pair $(A,t')$,
  she can solve $f_A(H) = t'$ for $H$.

  To avoid this attack, we add a one-time pad $\AES_k(J_0) \oplus t$.
\end{prf}

Finally, we can define AES-GCM.
The scheme will encrypt/authenticate a message
and also authenticate (but not encrypt) an \term{encryption context}.

\begin{scheme}[AES-GCM]
  Let $M = (M_1,\dotsc,M_u)$ for $u \leq 2^{32}-2$ be the message,
  $A = (A_1,\dotsc,A_v)$ be the encryption context,
  and $k \randin \bits{128}$ be a shared secret.

  To encrypt/sign:
  \begin{algorithmic}[1]
    \State $L \gets \abs{A} \concat \abs{M}$
    \State $J_0 \gets \vv{IV} \concat 0^{31} \concat 1$ for unique $\vv{IV} \in \bits{96}$
    \For{$i = 1,\dotsc,u$} \Comment{Encryption with AES-CTR}
      \State $J_i \gets J_{i-1} + 1$
      \State $C_i \gets \AES_k(J_i) \xor M_i$
    \EndFor
    \State $H \gets \AES_k(0^{128})$ \Comment{Authentication with GMAC}
    \State Define $f_{A,C}(x) = A_1x^{u+v+2} + \dotsb + A_v x^{u+2} + C_1 x^{u+1} + \dotsb + C_u x^2 + Lx$
    \State $t \gets \AES_k(J_0) \xor f_{A,C}(H)$
    \State \Return{$(\vv{IV},A,C,t)$}
  \end{algorithmic}
  To decrypt/authenticate:
  \begin{algorithmic}[1]
    \State
  \end{algorithmic}
\end{scheme}

AES-GCM does both authentication and encryption,
but it can be used to do just authentication by passing $M = \varepsilon$.

There are very fast hardware implementations of AES and multiplication under $\GF(2^{128})$.
If we use Horner's rule, we can evaluate an $n$-degree polynomial in $\GF(2^{128})[x]$
with $n$ PCLMUL instructions and $n-1$ XOR instructions.

We can also parallelize since we can assign blocks to other processors
and split up the polynomial $f_{A,C}$ into multiple parts for evaluation.

The scheme can be used in streaming mode, which is helpful.

This is also proven to be secure.

\lecture{Feb 9}
In general, an AES-GCM IV should never be reused.

If we reuse $(\vv{IV}, A_1, C_1, t_1)$ and $(\vv{IV}, A_2, C_2, t_2)$,
then $t_1 \oplus t_2 = f_{A_1,C_1}(H) \oplus f_{A_2,C_2}(H)$.
This is a polynomial function of $H$ and can be solved.
The entire keystream can then be derived from $H$.

\section{Real-life example: AWS}
AWS requires security for data in transit, data in use, and data at rest.

\lecture{Feb 12}
Every data item is encrypted with a one-time \term{data encryption key} (DEK).

To upload: A client makes a request to a \term{key management service} (KMS) server,
which gets the one-time key from an HSM and passes it on.
Then, the client sends the ciphertext to an S3 server.

Alternatively, a client can send data directly to the S3 server,
which queries the KMS (querying the HSM) for a key an doing the encryption server-side.

To keep the DEKs secure, we use a \term{customer main key} (CMK).
The HSM generates a CMK $K$ and DEK $k$,
then sends $k$ and $wk = E_K(A,k)$ to the KMS,
which passes it to the client.
Then, the client uses $k$ and immediately deletes it,
storing only $wk$ and the ciphertext.

To decrypt: The client sends $wk$ to the HSM (via the KMS).
The HSM decrypts $wk$ and returns $k$.
The client uses $k$ and immediately deletes it.

The IVs used are guaranteed to be randomly generated
with collision probability better than $2^{-32}$.
However, by the birthday paradox, this means that for $B$ IVs,
there is a $B^2/(2\cdot2^{96})$ chance of collision.
Therefore, never encrypt more than $2^{32}$ chunks with the same DEK.

This also applies to random IVs for use of CMKs,
so a CMK should only be used for $2^{32}$ DEKs.
AWS only rotates CMKs once a year,
so a user can only encrypt 136 DEKs per second.

To solve this, derive key mode works by setting
$wk = \Enc_L(A,k)$ where $L = \HMAC_K(N)$.
It permits $2^{40}$ CMKs, each encrypting $2^{50}$ DEKs.

AWS charges \$1/mo for CMKs, and \$0.03/10,000 requests to the KMS API
(after a free tier of 20,000).
DynamoDB is a NoSQL databse with unique DEKs for each table entry,
which is just kinda stupid crazy.

\lecture{Feb 14}
CMKs are never stored (unencrypted) on disk and never transmitted off its HSM.
A CMK is encrypted with a \term{domain key} (shared across HSMs in a domain)
to create an \term{exported key token} (EKT) which actually put in storage.

The domain key is rotated daily.
When an HSM selects the new domain key, it is encrypted with a one-time AES-GCM key $\ell$.
Then, $\ell$ is encrypted with ECC to be exported to the other HSMs,
adding another layer to the long chain of
\[
  \text{ECC keys} \to \ell \to \text{domain key} \to \text{CMK} \to \text{DEK} \to \text{actual data}
\]
to create an \term{exported domain token} (EDT),
which is distributed to the domain and stored in storage.

\chapter{Public-key cryptography}
\section{Overview}
Symmetric-key cryptography is limited because it assumes that
communicating parties share the secret key beforehand.
This can be established in a few ways:
\begin{enumerate}
  \item Point-to-point distribution. Alice sends it to Bob over a secured channel,
        like a face-to-face meeting or a pre-secured SIM card.
  \item Through a \term{trusted third party} (TTP) which acts as a \term{key distribution centre} (KDC).
        The KDC picks keys and sends them to each party, encrypted by pre-established keys.
\end{enumerate}
These are inefficient.
In a network of $n$ users, each user needs to manage $n-1$ secret keys
for a total of $\order{n^2}$ keys in the network.

Symmetric-key encryption also cannot be used to achieve non-repudiation
(i.e., a scheme where nobody can deny they were the source of a message).
This is because secrets are shared.

\begin{defn}[public-key cryptography]
  Schemes in which communicating parties share some authenticated (but non-secret)
  information.
\end{defn}

Public-key schemes were invented by Merkle, Diffie, and Hellman in 1975.

\begin{scheme}[Merkle puzzle]
  Suppose Alice and Bob desire to establish a secret session key
  over an authenticated, non-secret channel.
  \begin{enumerate}[nosep]
    \item Alice creates $N = 10^9$ puzzles $P_i$ such that each takes $t=5$ hours to solve.
          The solution to each $P_i$ is a 128-bit session key $sk_i$ and 128-bit serial number $n_i$.
    \item Alice sends the puzzles to Bob
    \item Bob picks a random $j \randin [1,N]$ and solves $P_j$ to obtain $sk_j$ and $n_j$.
    \item Bob sends $n_j$ to Alice.
    \item The secret key is $sk_j$.
  \end{enumerate}
\end{scheme}

In this case, an eavesdropper would need to solve 500 million puzzles to find out $sk_j$.

For example, let $P_i = \AESCBC_{k_i}(sk_i,n_i,n_i)$ for $k_i = r_i \concat 0^{88}$
and $r_i \randin \bits{40}$.
Then, the expected time to solve $P_i$ by exhaustive key search is $2^{39}$ operations.

In general, for public key cryptography, each entity $A$
generates a \term{key pair} of a \term{public key} and \term{secret key} $(P_A, S_A)$.
It should be infeasible to recover $S_A$ from $P_A$.
For example, $S_A = (p,q)$ and $P_A = pq$ where $p$ and $q$ are random, large primes.
Then, the encryption/signing function takes $S_A$
but the decryption/verifying function only requires $P_A$.

Public-key cryptography is good for key management, verification, and non-repudiation.
However, it is usually slower than symmetric-key encryption.

To work around this, hybrid schemes exist,
where public-key encryption is used to share a key used for symmetric-key encryption.

\section{Algorithmic number theory}
\lecture{Feb 16}

\begin{theorem}
  Every integer $n \geq 2$ has a unique prime factorization (up to permutation of the factors).
\end{theorem}

Given an integer, it is hard to find its prime factorization.
However, it is easy to verify that a given list of primes is the factorization.

\begin{defn}[big-$\mathcal{O}$ notation]
  If $f, g : \N \toto \R_+$, then $f(n) = \order{g(n)}$ means that
  there exists a positive constant $c$ and integer $n_0$ such that
  $f(n) \leq cg(n)$ for all $n \geq n_0$.
\end{defn}

\begin{defn}
  An \term{algorithm} is a well-defined computational procedure
  that takes some input and halts with some output.

  The \term{running time} of an algorithm is an upper bound,
  as a function of the \term{input size},
  of the worst-case number of fixed-size operations.

  An algorithm is \term{polynomial-time} if its expected running time is
  $\order{k^c}$ for input size $k$ and a fixed positive integer $c$.
\end{defn}

Then, we can figure out the expected time in bit operations
for basic integer operations. Given two $k$-bit integers $a$ and $b$
(i.e., input size $\order{k}$ bits):

\begin{center}
  \begin{tabular}{r|cCl}
    Operation      & Algorithm           & \text{Running time} &                                               \\ \hline
    Addition       & Elementary school   & \order{k}                                                           \\
    Subtraction    & Elementary school   & \order{k}                                                           \\
    Multiplication & Elementary school   & \order{k^2}         & Fastest known algorithm is $\order{k\log k}$  \\
    Division       & Elementary school   & \order{k^2}                                                         \\
    GCD            & Euclidean algorithm & \order{k^2}         & Fastest known algorithm is $\order{k\log^2k}$
  \end{tabular}
\end{center}

Given integers $a,b,m \in \Z_n$ for a $k$-bit integer $n$:

\begin{center}
  \begin{tabular}{r|cC}
    Operation      & Algorithm                    & \text{Running time} \\ \hline
    Addition       & Elementary school            & \order{k}           \\
    Subtraction    & Elementary school            & \order{k}           \\
    Multiplication & Elementary school            & \order{k^2}         \\
    Inversion      & Extended Euclidean algorithm & \order{k^2}         \\
    Exponentiation & Square-and-multiply          & \order{k^3}
  \end{tabular}
\end{center}

\lecture{Feb 26}
The square-and-multiply algorithm reduces the time complexity
of exponentiation from $\order{2^kk}$ (calculating $a^m$ and modding)
to $\order{k^3}$.

Let $m = \sum m_i2^i$. Then,
\[
  a^m = a^{\sum m_i 2^i}
  = \prod a^{m_i 2^i}
  \equiv \prod_{\mathclap{0 \leq i \leq k-1, m_i = 1}} a^{2^i} \pmod{n}
\]
which gives us the algorithm
\begin{algorithm}[H]
  \caption{Repeated square-and-multiply}
  \begin{algorithmic}[1]
    \Procedure{SquareAndMultiply}{$a,m = (m_k\cdots m_0)$}
      \If{$m_0 = 1$}
        \State $B \gets a$
      \Else
        \State $B \gets 1$
      \EndIf
      \State $A \gets a$
      \For{$i = 1,\dotsc,k-1$}
        \State $A \gets A^2 \bmod n$
        \If{$m_i = 1$}
          \State $B \gets B \times A \bmod n$
        \EndIf
      \EndFor
      \State \Return{$B$}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
which takes only at most $k$ squarings and $k$ multiplications,
for a total time of $\order{k^3}$.

\chapter{RSA}

RSA (Rivest--Shamir--Adleman) was invented in 1977.

\section{RSA encryption}

\begin{scheme}[RSA encryption]\label{s:rsa}
  Each entity $A$ generates a key pair:
  \begin{algorithmic}[1]
    \State Randomly select two large, distinct primes $p$ and $q$ of the same bitlength
    \State Compute the \term{RSA modulus} $n = pq$ and $\phi = \phi(n) = (p-1)(q-1)$
    \State Select an arbitrary \term{encryption exponent} $1 < e < \phi$ with $\gcd(e,\phi) = 1$
    \State Compute the \term{decryption exponent} $1 < d < \phi$ with $ed \equiv 1 \pmod{\phi}$
    \State \Return{$(P_A,S_A) = ((n,e), d)$}
  \end{algorithmic}
  To encrypt a message for $A$:
  \begin{algorithmic}[1]
    \State Obtain an authenticated copy of $P_A = (n,e)$
    \State Compute the ciphertext $c = m^e \bmod n$
    \State \Return{$c$}
  \end{algorithmic}
  To decrypt a message as $A$:
  \begin{algorithmic}[1]
    \State Compute $m = c^d \pmod n$
    \State \Return{$m$}
  \end{algorithmic}
\end{scheme}

For example, suppose Alice selects $p = 23$ and $q = 37$.
Then, $n = 851$ and $\phi(n) = 792$.
She can pick $e = 631$ since it is coprime to $\phi = 792$.
Then, solving $631d \equiv 1 \bmod 792$ yields $d = 487$.

To encrypt $m = 13$, for Alice, Bob obtains the public key $(n=851,e=631)$.
Then, he computes $c = 13^{631} \bmod 851$ using square-and-multiply to get 616,
and sends 616 to Alice.

To decrypt $c = 616$, Alice computes $m = 616^{487} \bmod 851$ which recovers 13.

\begin{theorem}[RSA works]
  For all $n$ and $m \in [0,n-1]$, if $c = m^e \bmod n$, then $m = c^d \bmod n$.
\end{theorem}
\begin{prf}
  We will prove that $m^{ed} \equiv m \pmod n$ for all $m \in [0, n-1]$.

  Since $ed \equiv 1 \pmod{\phi}$, we can write
  \[ ed = 1 + k(p-1)(q-1) \]
  for some $k \in \Z$.
  Since $ed > 1$ and $(p-1)(q-1) \geq 1$, we have $k \geq 1$.
  Claim that $m^{ed} \equiv m \pmod p$.

  Suppose $p \mid m$. Then, $m \equiv 0 \pmod p$,
  so $m^{ed} \equiv 0^{ed} \equiv 0 \equiv m \pmod p$.

  Otherwise, $p \nmid m$. Then, $\gcd(m,p) = 1$,
  so by Fermat's Little Theorem:
  \begin{align*}
    m^{p-1}                 & \equiv 1                  & \pmod p \\
    m \cdot m^{k(p-1)(q-1)} & \equiv m \cdot 1^{k(q-1)} & \pmod p \\
    m^{1 + k(p-1)(q-1)}     & \equiv m                  & \pmod p \\
    m^{ed}                  & \equiv m                  & \pmod p
  \end{align*}

  Symmetrically, $m^{ed} \equiv m \pmod q$.
  Since $p$ and $q$ are distinct primes, we have $m^{ed} \equiv m \pmod n$,
  completing the proof.
\end{prf}

\lecture{Feb 28}
RSA is only secure if Eve cannot factor $n$.
This is the only way Eve can break RSA;
the RSA problem of computing $d$ from $(n,e)$ reduces to factoring $n$.

\begin{attack}[dictionary attack]
  Suppose that $m \in \mathscr{M}$ where $\abs{\mathscr{M}}$ is small.

  Then, an adversary can just encrypt all of $\mathscr{M}$ and find the one that matches.
\end{attack}

To prevent this, add a bunch of random garage (\term{salt}) $s \randin \bits{128}$ to messages.
That is, instead of encrypting $m$, encrypt $m' = s \concat m$.
Since the salt is a fixed length, Alice knows to discard it.

\begin{attack}[chosen-ciphertext attack on Basic RSA]
  Suppose Eve has access to a decryption oracle
  (i.e., Alice will decrypt any message except $c$).
  Then, Eve can decrypt $c$ by:
  \begin{algorithmic}[1]
    \State Select $x \in [2,n-1]$ coprime to $n$
    \State Compute $\hat c = cx^e \bmod n$. We will have $\hat c \neq c$ as long as $\gcd(c,n) = 1$
    \State Obtain $\hat m = \hat c^d = (cx^e)^d = c^dx^{ed} = mx$ from the oracle.
    \State Recover $m = \hat m x^{-1} \bmod n$
  \end{algorithmic}
\end{attack}

To prevent this, ensure that $m$ follows prescribed format before encryption.
Then, $\hat m$ is unlikely to follow the right format,
so it cannot be decrypted.

Therefore, a secure implementation of RSA must include salting and formatting.

\lecture{Mar 1}
\begin{defn*}[security of a public-key encryption scheme]
  A public-key encryption scheme is \term[security!public-key]{secure}
  if it is semantically secure against a chosen-ciphertext attack
  by a computationally bounded adversary.
\end{defn*}

That is, to break a public-key scheme,
the adversary must be able to divine some information about the plaintext
given feasible computation and access to a decryption oracle.

\begin{scheme}[RSA Optimal Asymmetric Encryption Padding (RSA-OAEP)]
  Let $k$ be the bitlength of $n$ and $\ell = k-256-1$.
  Pick hash functions $G_1 : \bits{256} \to \bits{\ell}$ and $G_2 : \bits{\ell} \to \bits{256}$.

  To encrypt $M \in \bits{\ell-256}$, pick a salt $r \randin \bits{256}$.
  Then, use RSA to encrypt $m = 0 \concat s \concat t$
  where $s = (0^{256} \concat M) \oplus G_1(r) \in \bits{\ell}$
  and $t = r \oplus G_2(s) \in \bits{256}$.

  To decrypt $c$, first use RSA to get $m$ and parse as $0 \concat s \concat t$.
  Compute $r = G_2(s) \oplus t$ and check the first 256 bits of $G_1(r) \oplus s$.
  If they are all zeroes, output $M = $ the rest, otherwise reject.
\end{scheme}

\begin{theorem}[Bellare \& Rogaway]
  Suppose that the RSA problem is intractable and that $G_1,G_2$ are random functions.
  Then, RSA-OAEP is secure.
\end{theorem}

\section{Integer factorization}

Recall from CS 136/246/240/341:

\begin{defn}
  We write $f(n) = \order{g(n)}$ if there exists $c > 0$ and $n_0$
  such that $n \geq n_0$ implies $f(n) \leq cg(n)$.

  We write $f(n) = o(g(n))$ if $\lim\limits_{n\to\infty}\frac{f(n)}{g(n)} = 0$.

  An algorithm is \term[running time!polynomial]{polynomial-time} if its worst-case running time
  is $\order{n^c}$ for input size $n$ and constant $c$.

  An algorithm is \term[running time!exponential]{exponential-time} if its worst-case running time
  is \emph{not} of the form $\order{n^c}$ for \emph{any} constant $c$.
  In particular, if it is of the form $\order{2^{cn}}$ for a constant $c$,
  it is \term[running time!fully exponential]{fully exponential-time}.
  If it is of the form $\order{2^{o(n)}}$, it is \term[running time!subexponential]{subexponential-time}.
\end{defn}

Consider some attacks on RSA via factoring.

\begin{attack}[trial division]
  Trial divide $n$ by all the primes $2,3,5,\dotsc,\floor{\sqrt{n}}$.
  If any divide $n$, then stop.
\end{attack}

The running time of this attack is $\order{\sqrt{n}}$, which is not great
($2^{64}$ divisions for 128-bit keys).
In fact, since the size is $\log n$, this is fully exponential-time
($\order{2^{(\log n)/2}}$).

For classifying subexponential times, we use $L_n[\alpha,c]$ notation.

\begin{defn}
  If the running time is of the form
  \[ L_n[\alpha,c] := \order{\exp[(c+o(1))(\log_e n)^\alpha(\log_e\log_e n)^{1-\alpha}]} \]
  where $c > 0$ and $0 < \alpha < 1$, then the running time is subexponential.
\end{defn}

Notice that if $\alpha = 0$, we have $L_n[0,c] = \order{(\log n)^{c+o(1)}}$,
which is polynomial-time.
On the other edge, if $\alpha = 1$, then $L_n[1,c] = \order{n^{c+o(1)}}$,
which is fully exponential-time.

There are a number of \term*{special-purpose factoring algorithms},
like Pollard's $p-1$, Pollard's $\rho$, elliptic curve, etc.\ that work when
$n$ has special, known properties.
For example, $n$ has a small prime factor $p$ or $p-1$ only has small prime factors.

If $p$ and $q$ are of the same bitlength and randomly generated,
these won't get very far.

The algorithms that really matter are the \term*{general-purpose factoring algorithms}
which care only about the size of the number being factored.
The two major advancements are
\begin{enumerate}[nosep]
  \item the quadratic sieve factoring algorithm (QS; 1982), with running time $L_n[1/2,1]$, and
  \item the number field sieve factoring algorithm (NFS; 1990), with running time $L_n[1/3,1.923]$.
\end{enumerate}

\lecture{Mar 4}
The largest RSA challenge factored was the 2020 factorization of RSA-250
(250 decimal digits, 829 bits).

The security levels of RSA are approximately equivalent to
\begin{center}
  \begin{tabular}{c|ccc}
    Security & Block cipher & Hash function & RSA   \\ \hline
    80       & SKIPJACK     & SHA-1         & 1024  \\
    112      & Triple-DES   & SHA-224       & 2048  \\
    128      & AES small    & SHA-256       & 3072  \\
    192      & AES medium   & SHA-384       & 7680  \\
    256      & AES large    & SHA-512       & 15360
  \end{tabular}
\end{center}
under the current best known algorithms.

However, we have no actual proof that factoring is hard.
Shor's algorithm can factor in $\order{(\log n)^2}$,
but nobody has built a large enough quantum computer yet.

Today, 512-bit RSA is considered insecure, 1024-bit risky, and 2048-/3072-bit secure.

\section{Real-life example: QQ browser}

\section{RSA signatures}

The building blocks of RSA encryption can be used to create a signing scheme.

\begin{scheme}[RSA signing]
  Pick a hash function $H$.
  Generate keys as in \nameref{s:rsa}.

  To sign a message $m \in \bits{*}$ as $A$,
  \begin{algorithmic}[1]
    \State Compute $M = H(m)$
    \State Compute the signature $s = M^d \bmod n$
    \State \Return{$(m,s)$}
  \end{algorithmic}

  To verify that message $(m,s)$,
  \begin{algorithmic}[1]
    \State Obtain an authenticated copy of $P_A = (n,e)$
    \State Compute $M = H(m)$
    \State Compute $M' = s^e \bmod n$
    \State \Return{$M = M'$}
  \end{algorithmic}
\end{scheme}

This only works if the RSA problem is intractable.
If it is not, Eve can forge Alice's signature by solving
$s^e = H(m) \bmod n$ for any $m$.

Recall that an authentication scheme is secure if it is existentially unforgeable
by a computationally bounded adversary under a chosen-mesage attack.
That is, Eve gets a signing oracle.

The hash function must also be PR, 2PR, and CR.

If $H$ is not PR, then Eve can pick a random $s$ and get $M = s^e \bmod n$.
Then, use the PR oracle to get $m$ such that $H(m) = M$.

If $H$ is not 2PR, then given a signed message $(m,s)$,
the oracle can find another $m' \neq m$ such that $s$
is also the signature for $m'$.

If $H$ is not CR, then given a collision $(m,m')$,
Eve can use the signing oracle to get $s = H(m)^d \bmod n$.
But $s$ is also the signature for $m'$.

The basic RSA signature scheme is not secure if $H$ is SHA-256.
However, if we use a full-domain hash function, i.e.,
$H : \bits{*} \to [0,n-1]$, then we get a secure signature scheme.
Usually, to implement RSA-FDH, we use
\[
  H(m) = \operatorname{SHA-256}(1,m) \concat \operatorname{SHA-256}(2,m) \concat \dotsb \concat \operatorname{SHA-256}(t-1,m) \concat \operatorname{truncate}(\operatorname{SHA-256}(t,m))
\]
where we repeat until it has the same bitlength as $n$.

\begin{scheme}[PKCS \#1 v1.5 RSA]
  To sign $m \in \bits{*}$,
  \begin{algorithmic}[1]
    \State Compute $H(m)$, where $H$ is a hash function from a fixed list
    \State Format $M = \x{0001FF} \dotsb \x{FF00} \concat \text{hash name} \concat h$
    to have the same byte length as $n$.
    \State Compute $s = M^d \bmod n$
    \State \Return{$(m,s)$}
  \end{algorithmic}

  To verify $(m,s)$,
  \begin{algorithmic}[1]
    \State Obtain an authentic copy of $(n,e)$
    \State Compute $M = s^e \bmod n$
    \State Check the formatting: starts with $\x{0001}$, consecutive $\x{FF}$, then $\x{00}$
    \State Read the next 15 bytes to determine the hash function $H$
    \State \Return{$h = H(m)$}
  \end{algorithmic}
\end{scheme}

\section{Bleichenbacher's attack}
\lecture{Mar 6}
\begin{attack}[Bleichenbacher]
  Suppose that $e = 3$ and the verifier does not check that there are no bytes
  to the right of $h$.
  \WLOG, let $n$ be 384 bytes (3072 bits) and $H$ be SHA-1.
  Then, the attacker can
  \begin{algorithmic}[1]
    \State Select $m \in \bits{*}$
    \State Compute $h \gets H(m) \in \bits{160}$
    \State $D \gets \x{00} \concat \text{hash name} \concat h \in \bits{288}$
    \State $N \gets 2^{288} - D$
    \State Assert that $3 \mid N$. If $3 \nmid N$, modify $m$ sightly and try again.
    \State $s \gets 2^{1019} - 2^{34}\frac{N}{3}$
    \State \Return{$(m,s)$}
  \end{algorithmic}
  Then, the verifier will accept $(m,s)$.
\end{attack}
\begin{prf}
  The verifier will compute
  \begin{align*}
    M & = s^e                                                                                                                                                             & \pmod n                                                \\
      & = \qty(2^{1019} - 2^{34}\frac{N}{3})^3                                                                                                                            & \pmod n                                                \\
      & = 2^{3057} - 2^{2072}N + \underbrace{2^{1087}\frac{N^2}{3} - \qty(2^{34}\frac{N}{3})^3}_{\text{garbage}}                                                          & \pmod n                                                \\
      & = 2^{3057} - 2^{2072}(2^{288} - D) + \text{garbage}                                                                                                               & \pmod n                                                \\
      & = 2^{3057} - 2^{2360} + 2^{2072}D + \text{garbage}                                                                                                                & \text{(no mod since this is $\ll 2^{3072} \approx n$)} \\
      & = 2^{2360}(2^{697}-1) + 2^{2072}D + \text{garbage}                                                                                                                                                                         \\
      & = \underbrace{\x{0001FF}\cdots\x{FF}}_{3071-2360} \concat \underbrace{\x{00} \concat \text{hash name} \concat h}_{D} \concat \underbrace{\text{garbage}}_{2071-0}
  \end{align*}
  and extract $h = H(m)$, ignoring the garbage.
\end{prf}
The attack fails if $s^e \gg n$, so the mod cannot be ignored.
In practice, $e = 2^{16}+1 = 65537$ is used.

\chapter{Elliptic curve cryptography}
\section{Elliptic curves}

\begin{defn}
  Let $F$ be a field (i.e., $\R$ or $\Z_p$ for prime $p \geq 5$).
  An \term*{elliptic curve $E$ over $F$} is given by an equation
  \[ E/F : Y^2 = X^3 + aX + b \]
  where $a$ and $b$ are in $F$ such that $4a^3 + 27b^2 \neq 0$.
\end{defn}

\begin{defn}
  Let $E/F$ be an ellpitic curve.
  The set of \term*{$F$-rational points on $E$} is
  \[ E(F) = \{(x,y) \in F^2 : y^2 = x^3 + ax + b\} \cup \{\infty\} \]
  where $\infty$ is a special value called the \term*{point at infinity}.
\end{defn}

\lecture{Mar 8}
\begin{example}\label{ex:ec1}
  The set of $\Z_{11}$-rational points on $E/\Z_{11} : Y^2 = X^3 + X + 6$
  is $E(\Z_{11}) = \{\infty, (2,4), (2,7), (3,5), (3,6), (5,2), (5,9),
    (7,2), (7,9), (8,3), (8,8), (10,2), (10,9)\}$.

  The size of the set $\#E(\Z_{11}) = 13$.
\end{example}

\begin{theorem}[Hasse]
  Let $E/\Z_p : Y^2 = X^3 + aX + b$.
  Then, $(\sqrt{p}-1)^2 \leq \#E(\Z_p) \leq (\sqrt{p}+1)^2$.
\end{theorem}

This allows us to approximate $\#E(\Z_p) \approx p$.
There is a polynomial-time algorithm to find $\#E(\Z_p)$.

To define addition on $E(F)$, we can imagine connecting a line together.

\begin{defn}[geometric rule]
  Let $P$ and $Q$ be points on $E(\R)$ and let $\ell$ be the line through $P$ and $Q$.
  In the case where $P = Q$, consider the tangent line.

  Let $T \in E(\R)$ be the third point of intersection of $\ell$ with $E$.
  If it does not exist, let $T = \infty$, since we treat $\infty$
  as a point ``at infinity'' on every line.

  Then, we define $P + Q$ as the reflection $-T$ in the $x$-axis.
\end{defn}

Formally:

\begin{defn}[addition rule]
  Let $E$ be an elliptic curve over $F$.
  Let $P = (x_1,y_1)$ and $Q = (x_2,y_2) \in E(F)$.
  Then:
  \begin{enumerate}[nosep]
    \item $P + \infty = \infty + P = P$ for all $P \in E(F)$.
    \item $-P = (x_1,-y_1)$. Also, $-\infty = \infty$.
          Furthermore, $P + (-P) = (-P) + P = \infty$ for all $P$.
    \item Suppose $P \neq \pm Q$.
          Then, $P + Q = (x_3,y_3)$ where $x_3 = \lambda^2 - x_1 - x_2$
          and $y_3 = -1 + \lambda(x_1-x_3)$ for $\lambda = \frac{y_2-y_1}{x_2-x_1}$.
    \item Suppose $P \neq -P$. Then, $P + P = (x_3,y_3)$ where
          $x_3 = \lambda^2 - 2x_1$, $y_3 = -y_1 + \lambda(x_1-x_3)$ for $\lambda = \frac{3x_1^2 + a}{2y_1}$.
  \end{enumerate}
\end{defn}

\begin{example}
  In $E/\Z_11 : Y^2 = X^3 + X + 6$ (from \cref{ex:ec1}),
  $(2,4) + (2,7) = \infty$, $(2,4) + (8,3) = (5,2)$,
  and $(2,4) + (2,4) = (5,9)$.
\end{example}

\begin{prop}
  $(E(F), +)$ is an abelian group with identity element $\infty$. That is,
  \begin{enumerate}[nosep]
    \item $P + \infty = P$ for all $P$;
    \item for each $P$, there exists $Q$ such that $P + Q = \infty$;
    \item $P + Q = Q + P$ for all $P$ and $Q$; and
    \item $(P + Q) + R = P + (Q + R)$ for all $P$, $Q$, and $R$.
  \end{enumerate}
\end{prop}

Associativity is non-trivial, but we do not prove it.

We write $P - Q := P + (-Q)$. This is \term{point subtraction}.

\section{Elliptic curve discrete log}
\lecture{Mar 11}

\begin{defn}[point multiplication]
  Let $P \in E(\Z_p)$ and $k \in N$. Then,
  \[ kP := \underbrace{P + P + \dotsb + P}_{k} \]
  Define also $0P := \infty$ and $(-k)P := -(kP)$.
\end{defn}

\begin{theorem}
  Suppose $n = \#E(\Z_p)$ is prime and $\infty \neq P \in E(\Z_p)$. Then:
  \begin{enumerate}[nosep]
    \item $nP = \infty$; and
    \item the points $\infty,P,2P,3P,\dotsc,(n-1)P$ are distinct,
          so $E(\Z_p) = \{\infty,P,2P,3P,\dotsc,(n-1)P\}$.
  \end{enumerate}
  That is, $P$ is a \term{generator} of $E(\Z_p)$.
\end{theorem}

\begin{example}\label{ex:ec2}
  For the curve $E/\Z_{23} : Y^2 = X^3 + X + 4$, we have $\#E(\Z_{23}) = 29$
  and generator $P = (0,2)$ since:
  \[
    \begin{smallmatrix}
      1P = (0,2)  & 2P = (13,12) & 3P = (11,9) & 4P = (1,12)  & 5P = (7,20)  & 6P = (9,11)  \\
      7P = (15,6) & 8P = (14,5)  & 9P = (4,7)  & 10P = (22,5) & 11P = (10,5) & 12P = (17,9) \\
      13P =(8,15) & 14P=(18,9)   & 15P=(18,14) & 16P=(8,8)    & 17P=(17,14)  & 18P=(10,18)  \\
      19P=(22,18) & 20P=(4,16)   & 21P=(14,18) & 22P=(15,17)  & 23P=(9,12)   & 24P=(7,3)    \\
      25P=(1,11)  & 26P=(11,14)  & 27P=(13,11) & 28P=(0,21)   & 29P=\infty
    \end{smallmatrix}
  \]
\end{example}

\begin{problem}[elliptic curve discrete logarithm problem (ECDLP)]
  Given $E$, $p$, $n$, $\infty \neq P \in E(\Z_p)$, and $Q \in E(\Z_p)$,
  find the \term{discrete logarithm} $\ell \in [0,n-1]$ such that $Q = \ell P$.

  We write $\ell =: \log_P Q$.
\end{problem}

\begin{example}
  In \cref{ex:ec2}, we can see that $\log_{(0,2)}((10,18)) = 18$.
\end{example}

A brute force approach will take $\order{n} = \order{p}$ point additions,
which is fully exponential-time as the input size is $\log p$.

Shank's algorithm uses the division algorithm.
Let $m = \ceil{\sqrt{n}}$. By the division algorithm there exist unique
$q$ and $r$ such that $\ell = qm + r$ where $0 \leq r < m$ and $0 \leq q < m$.
Then, $\ell = qm - r$ and in particular $\ell P - qm P = rP$.

If $M = mP$, we can say $Q - qM = rP$. This leads us to the algorithm:

\begin{algorithm}[H]
  \caption{Shank's algorithm for ECDLP}
  \begin{algorithmic}[1]
    \For{$r \in [0,m-1]$}
      \State Compute $rP$ and store $(rP, r)$ in a sorted table
    \EndFor
    \State $M \gets mP$
    \For{$q \in [0,m-1]$}
      \State $R \gets Q - qM$
      \If{$(R,r)$ in the table}
        \State \Return{$\ell = qm + r$}
      \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}

This will require $\order{m} = \order{\sqrt{p}}$ point additions and storage of $\order{\sqrt{p}}$ points.

Pollard's algorithm (no details given) also works in $\order{\sqrt{p}}$ time
but with negligible space complexity.
VW parallel collision search can also parallelize Pollard's algorithm.

Some other results: If $n = p$, there exists a polynomial-time algorithm.
Also, if $n \mid (p^c-1)$ for small $c \lessapprox 100$, there is a subexponential-time algorithm.
Shor's algorithm solves the ECDLP in $\order{(\log p)^2}$ time.

\section{Elliptic curve cryptography}

ECC is better than RSA because the fastest attacks are fully exponential instead of subexponential.
This means smaller parameters are needed for the same security level:

\begin{center}
  \begin{tabular}{c|cc}
    Security level & RSA bitlength & ECC bitlength \\ \hline
    80             & 1,024         & 160           \\
    112            & 2,048         & 224           \\
    128            & 3,072         & 256           \\
    192            & 7,680         & 384           \\
    256            & 15,360        & 512
  \end{tabular}
\end{center}

The NSA has specific curves -- P-256, P-384, and P-521 -- that they use
for achieving 128-, 192-, and 256-bit security levels.
They all use curves of the form $E/\Z_p : Y^2 = X^3 - 3X + b$.

The $b$-values for these curves were supposedly chosen at random,
but there is gossip that they were picked because of attacks only known to the NSA.

\lecture{Mar 13}
Curve25519 was selected by Bernstein and Lange in 2005,
and provides a 128-bit security level.
It is the curve $E/\Z_{2^{255}-19} : Y^2 = X^3 + 48662X^2 + X$.

SM2 is a 256-bit elliptic curve chosen by the Chinese State Cryptography Administration,
which is similar to P-256 but with a slightly different prime and $b$.

The NSA's Commercial National Security Algorithm Suite (CNSA)
uses AES-256 (encryption), SHA-384 (hashing), ECDSA with P-384/RSA-PSS with $\geq3072$-bit $n$ (signature),
and ECDH with P-384/RSA with $\geq3072$-bit $n$ (key establishment).

The primes $p = 2^{256} - 2^{224} + 2^{192} + 2^{96} - 1$
and $2^{384} - 2^{128} - 2^{96} + 2^{32} - 1$ in P-256/384 were chosen
because reduction modulo $p$ can be implemented on a 32-bit machine without division.

Consider $p = 2^{192} - 2^{64} - 1$.
Suppose that $a,b \in [0,p-1]$ and we want to compute $c = a \times b \bmod p$.
We can write $a = a_2 2^{128} + a_1 2^{64} + a_0$ as 64-bit integers and similarly for $b$.
Then,
\[ d = a \times b = d_5 2^{230} + d_4 2^{256} + d_3 2^{192} + d_2 2^{128} + d_1 2^{64} + d_0 \]
is a 384-bit integer of six 64-bit words.
We have that
\begin{align*}
  2^{192} & \equiv 2^{64} + 1                                    & \pmod p \\
  2^{256} & \equiv 2^{128} + 1                                   & \pmod p \\
  2^{320} & \equiv 2^{192} + 2^{128} \equiv 2^{128} + 2^{64} + 1 & \pmod p
\end{align*}
so we can write that
\begin{align*}
  c & \equiv d                                                                                                                         & \pmod p \\
    & \equiv d_5 2^{230} + d_4 2^{256} + d_3 2^{192} + d_2 2^{128} + d_1 2^{64} + d_0                                                  & \pmod p \\
    & \equiv d_5(2^{128} + 2^{64} + 1) + d_4(2^{128} + 2^{64}) + d_3(2^{64} + 1) + d_2 2^{128} + d_1 2^{64} + d_0                      & \pmod p \\
    & \equiv (d_5 \concat d_5 \concat d_5) + (d_4 \concat d_4 \concat 0) + (0 \concat d_3 \concat d_3) + (d_2 \concat d_1 \concat d_0) & \pmod p
\end{align*}
which leads us to a nice and clean algorithm:

\begin{algorithm}[H]
  \caption{Reduction modulo $p = 2^{192} - 2^{64} - 1$}
  \begin{algorithmic}[1]
    \State $(d_5,d_4,d_3,d_2,d_1,d_0) \gets a \times b$
    \State $t_1 \gets (d_2,d_1,d_0)$
    \State $t_2 \gets (0,d_3,d_3)$
    \State $t_3 \gets (d_4,d_4,0)$
    \State $t_4 \gets (d_5,d_5,d_5)$
    \State $c \gets t_1 + t_2 + t_3 + t_4$
    \Repeat
      \State $c \gets c - p$
    \Until{$c < p$}
    \State \Return{$c$}
  \end{algorithmic}
\end{algorithm}

Notice that we never perform a full long division.
We also have $0 \leq c < 4p$, so at most three subtractions are required on line 8.

We now have enough defined to actually do some cryptography.

Suppose two parties want to agree on a shared secret $k$
so they can use AES-GCM or something, but over an unsecured connection.
Elliptic Curve Diffie--Hellman builds on the Diffie--Hellman key agreement protocol.

\begin{scheme}[Elliptic Curve Diffie--Hellman (ECDH)]
  Let $E$ be an elliptic curve defined over $\Z_p$ with $n = \#E(\Z_p)$ prime.
  Let $P \in E(\Z_p)$ be an arbitrary base point $P \neq \infty$.
  Also select a key derivation function $KDF : E(\Z_p) \to \bits{*}$.
  Then:
  \begin{algorithmic}[1]
    \State Alice selects $x \randin [1,n-1]$ and computes $X \gets xP$
    \State Alice sends $X$ to Bob
    \State Bob selects $y \randin [1,n-1]$ and computes $Y \gets yP$
    \State Bob sends $Y$ to Alice
    \State Bob computes $K \gets yX$ and $k = KDF(K)$
    \State Alice computes $K \gets xY$ and $k = KDF(K)$
  \end{algorithmic}
\end{scheme}

Then, Alice and Bob both have the same $k$ since
$xY = x(yP) = (xy)P = (yx)P = y(xP) = yX$.
The fastest way an attacker can determine $K = xY = yX$ from only $X$ and $Y$
is to solve an instance of the ECDLP.

However, since ECDH does not ask for an authenticated channel,
it is vulnerable to a man-in-the-middle (MITM) attack.
An attacker Eve can sit between Alice and Bob and just pass along her own
secrets to establish a key with Alice and a key with Bob.
Then, Eve can decrypt/re-encrypt messages and pass them along
without Alice or Bob knowing.

\lecture{Mar 15}
To solve this, instead of sending just $X$,
Alice sends $X$, an ECDSA signature on $X$, and a certificate for her public key.
Then, Bob verifies the certificate and signature to know that $X$ is legitimate.

\begin{scheme}[Elliptic Curve Digital Signature Algorithm (ECDSA)]
  Let $E$ be an elliptic curve defined over $\Z_p$ with $n = \#E(\Z_p)$ prime.
  Let $P \in E(\Z_p)$ be a generator point $P \neq \infty$.
  Also select a collision-resistant hash function $H : \bits{*} \to \bits{n}$.

  Alice selects $a \randin [1,n-1]$ and computes $A \gets aP$.
  Her public key is $A$; her private key is $a$.
  To sign a message $M \in \bits{*}$:
  \begin{algorithmic}[1]
    \State $m \gets H(M)$
    \State Select a per-mesage secret $k \randin [1,n-1]$
    \State $R \gets kP$, $r \gets x(R) \bmod n$. Assert $r \neq 0$.
    \State $s \gets k^{-1}(m+ar) \bmod n$. Assert $s \neq 0$.
    \State \Return{$(M,(r,s))$}
  \end{algorithmic}
  Note that $k$ should be random and only used once.
  To validate a signature $(r,s)$ on $M$:
  \begin{algorithmic}[1]
    \State Obtain a copy of $A$
    \State Assert that $r,s \in [1,n-1]$
    \State $m \gets H(M)$
    \State $u_1 \gets ms^{-1} \bmod n$, $u_2 \gets rs^{-1} \bmod n$
    \State $V \gets u_1P + u_2A$. Assert $V \neq \infty$.
    \State $v = x(V) \bmod n$
    \State \Return{$v = r$}
  \end{algorithmic}
\end{scheme}

ECDSA is secure assuming the ECDLP is intractable and $H$ is secure.

We can compare 256-bit ECDSA with 3072-bit RSA:
\begin{itemize}
  \item Usually we take $e = 3$ or $e = 2^{16}+1$ (small),
        so RSA verification $s^e \bmod n$
        is faster than ECDSA verification $u_1P + u_2A$
  \item RSA generation $H(M)^d \bmod n$ is slower than ECDSA generation $kP$
  \item RSA signatures are much longer (3072 bits) than ECDSA signatures (512 bits)
\end{itemize}

\pagebreak
\phantomsection\addcontentsline{toc}{chapter}{Back Matter}
\renewcommand{\listtheoremname}{List of Named Results}
\phantomsection\addcontentsline{toc}{section}{\listtheoremname}
\listoftheorems[ignoreall,numwidth=3em,onlynamed={theorem,lemma,corollary,prop}]
\renewcommand{\listtheoremname}{List of Cryptoschemes and Attacks}
\phantomsection\addcontentsline{toc}{section}{\listtheoremname}
\listoftheorems[ignoreall,onlynamed={scheme,attack}]
\printindex


\end{document}
